[{
  "comments": [{
    "author": {
      "login": "keypointt"
    },
    "body": "minor, is it better to make test case name like line-88 with expected result? like `fail if by name not position`",
    "commit": "9864d42501feff1acd01e11aedd2a7dc84a88bd5",
    "createdAt": "2019-08-21T14:42:22Z",
    "diffHunk": "@@ -0,0 +1,583 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import scala.collection.JavaConverters._\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row}\n+import org.apache.spark.sql.catalog.v2.{CatalogPlugin, Identifier}\n+import org.apache.spark.sql.catalog.v2.expressions.{BucketTransform, DaysTransform, FieldReference, HoursTransform, IdentityTransform, LiteralValue, MonthsTransform, YearsTransform}\n+import org.apache.spark.sql.catalyst.analysis.{CannotReplaceMissingTableException, NoSuchTableException, TableAlreadyExistsException}\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructType}\n+\n+class DataFrameWriterV2Suite extends QueryTest with SharedSparkSession with BeforeAndAfter {\n+  import org.apache.spark.sql.functions._\n+  import testImplicits._\n+\n+  private def catalog(name: String): CatalogPlugin = {\n+    spark.sessionState.catalogManager.catalog(name)\n+  }\n+\n+  before {\n+    spark.conf.set(\"spark.sql.catalog.testcat\", classOf[TestInMemoryTableCatalog].getName)\n+\n+    val df = spark.createDataFrame(Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\"))).toDF(\"id\", \"data\")\n+    df.createOrReplaceTempView(\"source\")\n+    val df2 = spark.createDataFrame(Seq((4L, \"d\"), (5L, \"e\"), (6L, \"f\"))).toDF(\"id\", \"data\")\n+    df2.createOrReplaceTempView(\"source2\")\n+  }\n+\n+  after {\n+    spark.sessionState.catalogManager.reset()\n+    spark.sessionState.conf.clear()\n+  }\n+\n+  test(\"Append: basic append\") {\n+    spark.sql(\"CREATE TABLE testcat.table_name (id bigint, data string) USING foo\")\n+\n+    checkAnswer(spark.table(\"testcat.table_name\"), Seq.empty)\n+\n+    spark.table(\"source\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\")))\n+\n+    spark.table(\"source2\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\"), Row(4L, \"d\"), Row(5L, \"e\"), Row(6L, \"f\")))\n+  }\n+\n+  test(\"Append: by name not position\") {",
    "line": 73
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "This tests that the validation is by name and not by position, so failing if by name is incorrect. The failure tests that a name violation (can't find \"data\") is generated, even though the number columns and column types match by position.",
    "commit": "9864d42501feff1acd01e11aedd2a7dc84a88bd5",
    "createdAt": "2019-08-21T17:03:52Z",
    "diffHunk": "@@ -0,0 +1,583 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import scala.collection.JavaConverters._\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row}\n+import org.apache.spark.sql.catalog.v2.{CatalogPlugin, Identifier}\n+import org.apache.spark.sql.catalog.v2.expressions.{BucketTransform, DaysTransform, FieldReference, HoursTransform, IdentityTransform, LiteralValue, MonthsTransform, YearsTransform}\n+import org.apache.spark.sql.catalyst.analysis.{CannotReplaceMissingTableException, NoSuchTableException, TableAlreadyExistsException}\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructType}\n+\n+class DataFrameWriterV2Suite extends QueryTest with SharedSparkSession with BeforeAndAfter {\n+  import org.apache.spark.sql.functions._\n+  import testImplicits._\n+\n+  private def catalog(name: String): CatalogPlugin = {\n+    spark.sessionState.catalogManager.catalog(name)\n+  }\n+\n+  before {\n+    spark.conf.set(\"spark.sql.catalog.testcat\", classOf[TestInMemoryTableCatalog].getName)\n+\n+    val df = spark.createDataFrame(Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\"))).toDF(\"id\", \"data\")\n+    df.createOrReplaceTempView(\"source\")\n+    val df2 = spark.createDataFrame(Seq((4L, \"d\"), (5L, \"e\"), (6L, \"f\"))).toDF(\"id\", \"data\")\n+    df2.createOrReplaceTempView(\"source2\")\n+  }\n+\n+  after {\n+    spark.sessionState.catalogManager.reset()\n+    spark.sessionState.conf.clear()\n+  }\n+\n+  test(\"Append: basic append\") {\n+    spark.sql(\"CREATE TABLE testcat.table_name (id bigint, data string) USING foo\")\n+\n+    checkAnswer(spark.table(\"testcat.table_name\"), Seq.empty)\n+\n+    spark.table(\"source\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\")))\n+\n+    spark.table(\"source2\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\"), Row(4L, \"d\"), Row(5L, \"e\"), Row(6L, \"f\")))\n+  }\n+\n+  test(\"Append: by name not position\") {",
    "line": 73
  }, {
    "author": {
      "login": "keypointt"
    },
    "body": "oh I see. no issues then. thanks!",
    "commit": "9864d42501feff1acd01e11aedd2a7dc84a88bd5",
    "createdAt": "2019-08-21T17:19:32Z",
    "diffHunk": "@@ -0,0 +1,583 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import scala.collection.JavaConverters._\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row}\n+import org.apache.spark.sql.catalog.v2.{CatalogPlugin, Identifier}\n+import org.apache.spark.sql.catalog.v2.expressions.{BucketTransform, DaysTransform, FieldReference, HoursTransform, IdentityTransform, LiteralValue, MonthsTransform, YearsTransform}\n+import org.apache.spark.sql.catalyst.analysis.{CannotReplaceMissingTableException, NoSuchTableException, TableAlreadyExistsException}\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructType}\n+\n+class DataFrameWriterV2Suite extends QueryTest with SharedSparkSession with BeforeAndAfter {\n+  import org.apache.spark.sql.functions._\n+  import testImplicits._\n+\n+  private def catalog(name: String): CatalogPlugin = {\n+    spark.sessionState.catalogManager.catalog(name)\n+  }\n+\n+  before {\n+    spark.conf.set(\"spark.sql.catalog.testcat\", classOf[TestInMemoryTableCatalog].getName)\n+\n+    val df = spark.createDataFrame(Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\"))).toDF(\"id\", \"data\")\n+    df.createOrReplaceTempView(\"source\")\n+    val df2 = spark.createDataFrame(Seq((4L, \"d\"), (5L, \"e\"), (6L, \"f\"))).toDF(\"id\", \"data\")\n+    df2.createOrReplaceTempView(\"source2\")\n+  }\n+\n+  after {\n+    spark.sessionState.catalogManager.reset()\n+    spark.sessionState.conf.clear()\n+  }\n+\n+  test(\"Append: basic append\") {\n+    spark.sql(\"CREATE TABLE testcat.table_name (id bigint, data string) USING foo\")\n+\n+    checkAnswer(spark.table(\"testcat.table_name\"), Seq.empty)\n+\n+    spark.table(\"source\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\")))\n+\n+    spark.table(\"source2\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\"), Row(4L, \"d\"), Row(5L, \"e\"), Row(6L, \"f\")))\n+  }\n+\n+  test(\"Append: by name not position\") {",
    "line": 73
  }],
  "prId": 25354
}, {
  "comments": [{
    "author": {
      "login": "keypointt"
    },
    "body": "same with `fail by name not position`",
    "commit": "9864d42501feff1acd01e11aedd2a7dc84a88bd5",
    "createdAt": "2019-08-21T14:45:15Z",
    "diffHunk": "@@ -0,0 +1,583 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import scala.collection.JavaConverters._\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.{AnalysisException, QueryTest, Row}\n+import org.apache.spark.sql.catalog.v2.{CatalogPlugin, Identifier}\n+import org.apache.spark.sql.catalog.v2.expressions.{BucketTransform, DaysTransform, FieldReference, HoursTransform, IdentityTransform, LiteralValue, MonthsTransform, YearsTransform}\n+import org.apache.spark.sql.catalyst.analysis.{CannotReplaceMissingTableException, NoSuchTableException, TableAlreadyExistsException}\n+import org.apache.spark.sql.test.SharedSparkSession\n+import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructType}\n+\n+class DataFrameWriterV2Suite extends QueryTest with SharedSparkSession with BeforeAndAfter {\n+  import org.apache.spark.sql.functions._\n+  import testImplicits._\n+\n+  private def catalog(name: String): CatalogPlugin = {\n+    spark.sessionState.catalogManager.catalog(name)\n+  }\n+\n+  before {\n+    spark.conf.set(\"spark.sql.catalog.testcat\", classOf[TestInMemoryTableCatalog].getName)\n+\n+    val df = spark.createDataFrame(Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\"))).toDF(\"id\", \"data\")\n+    df.createOrReplaceTempView(\"source\")\n+    val df2 = spark.createDataFrame(Seq((4L, \"d\"), (5L, \"e\"), (6L, \"f\"))).toDF(\"id\", \"data\")\n+    df2.createOrReplaceTempView(\"source2\")\n+  }\n+\n+  after {\n+    spark.sessionState.catalogManager.reset()\n+    spark.sessionState.conf.clear()\n+  }\n+\n+  test(\"Append: basic append\") {\n+    spark.sql(\"CREATE TABLE testcat.table_name (id bigint, data string) USING foo\")\n+\n+    checkAnswer(spark.table(\"testcat.table_name\"), Seq.empty)\n+\n+    spark.table(\"source\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\")))\n+\n+    spark.table(\"source2\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\"), Row(4L, \"d\"), Row(5L, \"e\"), Row(6L, \"f\")))\n+  }\n+\n+  test(\"Append: by name not position\") {\n+    spark.sql(\"CREATE TABLE testcat.table_name (id bigint, data string) USING foo\")\n+\n+    checkAnswer(spark.table(\"testcat.table_name\"), Seq.empty)\n+\n+    val exc = intercept[AnalysisException] {\n+      spark.table(\"source\").withColumnRenamed(\"data\", \"d\").writeTo(\"testcat.table_name\").append()\n+    }\n+\n+    assert(exc.getMessage.contains(\"Cannot find data for output column\"))\n+    assert(exc.getMessage.contains(\"'data'\"))\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq())\n+  }\n+\n+  test(\"Append: fail if table does not exist\") {\n+    val exc = intercept[NoSuchTableException] {\n+      spark.table(\"source\").writeTo(\"testcat.table_name\").append()\n+    }\n+\n+    assert(exc.getMessage.contains(\"table_name\"))\n+  }\n+\n+  test(\"Overwrite: overwrite by expression: true\") {\n+    spark.sql(\n+      \"CREATE TABLE testcat.table_name (id bigint, data string) USING foo PARTITIONED BY (id)\")\n+\n+    checkAnswer(spark.table(\"testcat.table_name\"), Seq.empty)\n+\n+    spark.table(\"source\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\")))\n+\n+    spark.table(\"source2\").writeTo(\"testcat.table_name\").overwrite(lit(true))\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(4L, \"d\"), Row(5L, \"e\"), Row(6L, \"f\")))\n+  }\n+\n+  test(\"Overwrite: overwrite by expression: id = 3\") {\n+    spark.sql(\n+      \"CREATE TABLE testcat.table_name (id bigint, data string) USING foo PARTITIONED BY (id)\")\n+\n+    checkAnswer(spark.table(\"testcat.table_name\"), Seq.empty)\n+\n+    spark.table(\"source\").writeTo(\"testcat.table_name\").append()\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(3L, \"c\")))\n+\n+    spark.table(\"source2\").writeTo(\"testcat.table_name\").overwrite($\"id\" === 3)\n+\n+    checkAnswer(\n+      spark.table(\"testcat.table_name\"),\n+      Seq(Row(1L, \"a\"), Row(2L, \"b\"), Row(4L, \"d\"), Row(5L, \"e\"), Row(6L, \"f\")))\n+  }\n+\n+  test(\"Overwrite: by name not position\") {",
    "line": 136
  }],
  "prId": 25354
}]