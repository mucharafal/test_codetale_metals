[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Can you also add tests that use the session catalog? This should work with both SQL and other paths like `saveAsTable`.",
    "commit": "27598ce9b5ef7bc8224e37df6f14907e766ddd54",
    "createdAt": "2019-08-20T00:00:22Z",
    "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row, SparkSession}\n+import org.apache.spark.sql.catalog.v2.expressions.{FieldReference, IdentityTransform, Transform}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter, InsertableRelation}\n+import org.apache.spark.sql.sources.v2.writer.{SupportsOverwrite, SupportsTruncate, V1WriteBuilder, WriteBuilder}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class V1WriteFallbackSuite extends QueryTest with SharedSQLContext with BeforeAndAfter {\n+\n+  import testImplicits._\n+\n+  private val format = classOf[InMemoryV1Provider].getName\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    InMemoryV1Provider.clear()\n+  }\n+\n+  override def afterEach(): Unit = {\n+    super.afterEach()\n+    InMemoryV1Provider.clear()\n+  }\n+\n+  test(\"append fallback\") {",
    "line": 52
  }, {
    "author": {
      "login": "brkyvz"
    },
    "body": "Done",
    "commit": "27598ce9b5ef7bc8224e37df6f14907e766ddd54",
    "createdAt": "2019-08-20T21:24:01Z",
    "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources.v2\n+\n+import java.util\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row, SparkSession}\n+import org.apache.spark.sql.catalog.v2.expressions.{FieldReference, IdentityTransform, Transform}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter, InsertableRelation}\n+import org.apache.spark.sql.sources.v2.writer.{SupportsOverwrite, SupportsTruncate, V1WriteBuilder, WriteBuilder}\n+import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class V1WriteFallbackSuite extends QueryTest with SharedSQLContext with BeforeAndAfter {\n+\n+  import testImplicits._\n+\n+  private val format = classOf[InMemoryV1Provider].getName\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    InMemoryV1Provider.clear()\n+  }\n+\n+  override def afterEach(): Unit = {\n+    super.afterEach()\n+    InMemoryV1Provider.clear()\n+  }\n+\n+  test(\"append fallback\") {",
    "line": 52
  }],
  "prId": 25348
}]