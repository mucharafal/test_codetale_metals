[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "what does this mean?\n",
    "commit": "24acaababce99695c56c49f9ac81e350dc01109d",
    "createdAt": "2016-06-02T05:16:06Z",
    "diffHunk": "@@ -0,0 +1,285 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark for performance with very wide and nested DataFrames.\n+ * To run this:\n+ *  build/sbt \"sql/test-only *WideSchemaBenchmark\"\n+ */\n+class WideSchemaBenchmark extends SparkFunSuite {\n+  private val scaleFactor = 100000\n+  private val widthsToTest = Seq(1, 10, 100, 1000, 2500)\n+  private val depthsToTest = Seq(1, 10, 100, 250)\n+  assert(scaleFactor > widthsToTest.max)\n+\n+  private lazy val sparkSession = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"microbenchmark\")\n+    .getOrCreate()\n+\n+  import sparkSession.implicits._\n+\n+  ignore(\"parsing large select expressions\") {\n+    val benchmark = new Benchmark(\"parsing large select\", 1)\n+    for (width <- widthsToTest) {\n+      val selectExpr = (1 to width).map(i => s\"id as a_$i\")\n+      benchmark.addCase(s\"$width select expressions\") { iter =>\n+        sparkSession.range(1).toDF.selectExpr(selectExpr: _*)\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    parsing large select                   Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 select expressions                          15 /   19          0.0    15464923.0       1.0X\n+    10 select expressions                         26 /   30          0.0    26131438.0       0.6X\n+    100 select expressions                        46 /   58          0.0    45719540.0       0.3X\n+    1000 select expressions                      285 /  328          0.0   285407972.0       0.1X\n+    5000 select expressions                     1482 / 1645          0.0  1482298845.0       0.0X\n+    */\n+  }\n+\n+  ignore(\"many column field read and write\") {\n+    val benchmark = new Benchmark(\"many column field r/w\", scaleFactor)\n+    for (width <- widthsToTest) {\n+      // normalize by width to keep constant data size\n+      val numRows = scaleFactor / width\n+      val selectExpr = (1 to width).map(i => s\"id as a_$i\")\n+      val df = sparkSession.range(numRows).toDF.selectExpr(selectExpr: _*).cache()\n+      df.count()  // force caching\n+      benchmark.addCase(s\"$width cols x $numRows rows (read)\") { iter =>\n+        df.selectExpr(\"sum(a_1)\").collect()\n+      }\n+      benchmark.addCase(s\"$width cols x $numRows rows (write)\") { iter =>\n+        df.selectExpr(\"*\", \"hash(a_1) as f\").selectExpr(\"sum(a_1)\", \"sum(f)\").collect()\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    many column field r/w:                 Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 cols x 100000 rows (read)                   41 /   51          2.4         414.5       1.0X\n+    1 cols x 100000 rows (write)                  52 /   68          1.9         520.1       0.8X\n+    10 cols x 10000 rows (read)                   43 /   49          2.3         426.7       1.0X\n+    10 cols x 10000 rows (write)                  48 /   65          2.1         478.3       0.9X\n+    100 cols x 1000 rows (read)                   84 /   91          1.2         840.4       0.5X\n+    100 cols x 1000 rows (write)                 103 /  122          1.0        1032.1       0.4X\n+    1000 cols x 100 rows (read)                  458 /  468          0.2        4575.5       0.1X\n+    1000 cols x 100 rows (write)                 849 /  875          0.1        8494.0       0.0X\n+    2500 cols x 40 rows (read)                  1077 / 1135          0.1       10770.3       0.0X\n+    2500 cols x 40 rows (write)                 2251 / 2351          0.0       22510.8       0.0X\n+    */\n+  }\n+\n+  ignore(\"wide struct field read and write\") {\n+    val benchmark = new Benchmark(\"wide struct field r/w\", scaleFactor)\n+    for (width <- widthsToTest) {\n+      val numRows = scaleFactor / width\n+      var datum: String = \"{\"\n+      for (i <- 1 to width) {\n+        if (i == 1) {\n+          datum += s\"\"\"\"value_$i\": 1\"\"\"\n+        } else {\n+          datum += s\"\"\", \"value_$i\": 1\"\"\"\n+        }\n+      }\n+      datum += \"}\"\n+      val df = sparkSession.read.json(sparkSession.range(numRows).map(_ => datum).rdd).cache()\n+      df.count()  // force caching\n+      benchmark.addCase(s\"$width wide x $numRows rows (read)\") { iter =>\n+        df.selectExpr(\"sum(value_1)\").collect()\n+      }\n+      benchmark.addCase(s\"$width wide x $numRows rows (write)\") { iter =>\n+        df.selectExpr(\"*\", \"hash(value_1) as f\").selectExpr(s\"sum(value_1)\", \"sum(f)\").collect()\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    wide struct field r/w:                 Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 wide x 100000 rows (read)                   31 /   38          3.2         309.3       1.0X\n+    1 wide x 100000 rows (write)                  55 /   61          1.8         551.6       0.6X\n+    10 wide x 10000 rows (read)                   51 /   53          2.0         506.6       0.6X\n+    10 wide x 10000 rows (write)                  60 /   71          1.7         596.5       0.5X\n+    100 wide x 1000 rows (read)                   74 /   86          1.4         737.4       0.4X\n+    100 wide x 1000 rows (write)                 133 /  154          0.8        1332.1       0.2X\n+    1000 wide x 100 rows (read)                  377 /  441          0.3        3767.3       0.1X\n+    1000 wide x 100 rows (write)                 688 /  828          0.1        6880.8       0.0X\n+    2500 wide x 40 rows (read)                   974 / 1044          0.1        9738.1       0.0X\n+    2500 wide x 40 rows (write)                 2081 / 2256          0.0       20805.8       0.0X\n+    */\n+  }\n+\n+  ignore(\"deeply nested struct field read and write\") {\n+    val benchmark = new Benchmark(\"deeply nested struct field r/w\", scaleFactor)\n+    for (depth <- depthsToTest) {\n+      val numRows = scaleFactor / depth\n+      var datum: String = \"{\\\"value\\\": 1}\"\n+      var selector: String = \"value\"\n+      for (i <- 1 to depth) {\n+        datum = \"{\\\"value\\\": \" + datum + \"}\"\n+        selector = selector + \".value\"\n+      }\n+      val df = sparkSession.read.json(sparkSession.range(numRows).map(_ => datum).rdd).cache()\n+      df.count()  // force caching\n+      benchmark.addCase(s\"$depth deep x $numRows rows (read)\") { iter =>\n+        df.selectExpr(s\"sum($selector)\").collect()\n+      }\n+      benchmark.addCase(s\"$depth deep x $numRows rows (write)\") { iter =>\n+        df.selectExpr(\"*\", s\"$selector as f\").selectExpr(s\"sum($selector)\", \"sum(f)\").collect()\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    deeply nested struct field r/w:        Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 deep x 100000 rows (read)                   38 /   49          2.6         382.2       1.0X\n+    1 deep x 100000 rows (write)                  38 /   45          2.6         382.1       1.0X\n+    10 deep x 10000 rows (read)                   48 /   61          2.1         483.8       0.8X\n+    10 deep x 10000 rows (write)                  93 /  104          1.1         931.2       0.4X\n+    100 deep x 1000 rows (read)                  151 /  162          0.7        1513.2       0.3X\n+    100 deep x 1000 rows (write)                 903 / 1004          0.1        9030.5       0.0X\n+    250 deep x 400 rows (read)                   653 /  735          0.2        6526.8       0.1X\n+    250 deep x 400 rows (write)                 8749 / 9217          0.0       87490.4       0.0X\n+    */\n+  }\n+\n+  ignore(\"bushy struct field read and write\") {\n+    val benchmark = new Benchmark(\"bushy struct field r/w\", scaleFactor)\n+    for (width <- Seq(1, 10, 100, 500)) {\n+      val numRows = scaleFactor / width\n+      var numNodes = 1\n+      var datum: String = \"{\\\"value\\\": 1}\"\n+      var selector: String = \"value\"\n+      var depth = 1\n+      while (numNodes < width) {\n+        numNodes *= 2\n+        datum = s\"\"\"{\"left_$depth\": $datum, \"right_$depth\": $datum}\"\"\"\n+        selector = s\"left_$depth.\" + selector\n+        depth += 1\n+      }\n+      // TODO(ekl) seems like the json parsing is actually the majority of the time, perhaps\n+      // we should benchmark that too separately.\n+      val df = sparkSession.read.json(sparkSession.range(numRows).map(_ => datum).rdd).cache()\n+      df.count()  // force caching\n+      benchmark.addCase(s\"$numNodes nodes x $depth deep x $numRows rows (read)\") { iter =>\n+        df.selectExpr(s\"sum($selector)\").collect()\n+      }\n+      benchmark.addCase(s\"$numNodes nodes x $depth deep x $numRows rows (write)\") { iter =>\n+        df.selectExpr(\"*\", s\"$selector as f\").selectExpr(s\"sum($selector)\", \"sum(f)\").collect()\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    bushy struct field r/w:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 nodes x 1 deep x 100000 rows (read)         57 /   69          1.7         571.7       1.0X\n+    1 nodes x 1 deep x 100000 rows (write)        61 /   75          1.6         614.4       0.9X\n+    16 nodes x 5 deep x 10000 rows (read)         44 /   55          2.3         439.0       1.3X\n+    16 nodes x 5 deep x 10000 rows (write)       100 /  126          1.0         999.5       0.6X\n+    128 nodes x 8 deep x 1000 rows (read)         53 /   61          1.9         532.9       1.1X\n+    128 nodes x 8 deep x 1000 rows (write)       530 /  656          0.2        5295.6       0.1X\n+    512 nodes x 10 deep x 200 rows (read)         98 /  134          1.0         979.2       0.6X\n+    512 nodes x 10 deep x 200 rows (write)      6698 / 7124          0.0       66977.0       0.0X\n+    */\n+  }\n+\n+  //\n+  // The following benchmarks are for reference: the schema is not actually wide for them."
  }, {
    "author": {
      "login": "ericl"
    },
    "body": "Removed the comment, it's confusing.\n",
    "commit": "24acaababce99695c56c49f9ac81e350dc01109d",
    "createdAt": "2016-06-02T20:34:49Z",
    "diffHunk": "@@ -0,0 +1,285 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark for performance with very wide and nested DataFrames.\n+ * To run this:\n+ *  build/sbt \"sql/test-only *WideSchemaBenchmark\"\n+ */\n+class WideSchemaBenchmark extends SparkFunSuite {\n+  private val scaleFactor = 100000\n+  private val widthsToTest = Seq(1, 10, 100, 1000, 2500)\n+  private val depthsToTest = Seq(1, 10, 100, 250)\n+  assert(scaleFactor > widthsToTest.max)\n+\n+  private lazy val sparkSession = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"microbenchmark\")\n+    .getOrCreate()\n+\n+  import sparkSession.implicits._\n+\n+  ignore(\"parsing large select expressions\") {\n+    val benchmark = new Benchmark(\"parsing large select\", 1)\n+    for (width <- widthsToTest) {\n+      val selectExpr = (1 to width).map(i => s\"id as a_$i\")\n+      benchmark.addCase(s\"$width select expressions\") { iter =>\n+        sparkSession.range(1).toDF.selectExpr(selectExpr: _*)\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    parsing large select                   Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 select expressions                          15 /   19          0.0    15464923.0       1.0X\n+    10 select expressions                         26 /   30          0.0    26131438.0       0.6X\n+    100 select expressions                        46 /   58          0.0    45719540.0       0.3X\n+    1000 select expressions                      285 /  328          0.0   285407972.0       0.1X\n+    5000 select expressions                     1482 / 1645          0.0  1482298845.0       0.0X\n+    */\n+  }\n+\n+  ignore(\"many column field read and write\") {\n+    val benchmark = new Benchmark(\"many column field r/w\", scaleFactor)\n+    for (width <- widthsToTest) {\n+      // normalize by width to keep constant data size\n+      val numRows = scaleFactor / width\n+      val selectExpr = (1 to width).map(i => s\"id as a_$i\")\n+      val df = sparkSession.range(numRows).toDF.selectExpr(selectExpr: _*).cache()\n+      df.count()  // force caching\n+      benchmark.addCase(s\"$width cols x $numRows rows (read)\") { iter =>\n+        df.selectExpr(\"sum(a_1)\").collect()\n+      }\n+      benchmark.addCase(s\"$width cols x $numRows rows (write)\") { iter =>\n+        df.selectExpr(\"*\", \"hash(a_1) as f\").selectExpr(\"sum(a_1)\", \"sum(f)\").collect()\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    many column field r/w:                 Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 cols x 100000 rows (read)                   41 /   51          2.4         414.5       1.0X\n+    1 cols x 100000 rows (write)                  52 /   68          1.9         520.1       0.8X\n+    10 cols x 10000 rows (read)                   43 /   49          2.3         426.7       1.0X\n+    10 cols x 10000 rows (write)                  48 /   65          2.1         478.3       0.9X\n+    100 cols x 1000 rows (read)                   84 /   91          1.2         840.4       0.5X\n+    100 cols x 1000 rows (write)                 103 /  122          1.0        1032.1       0.4X\n+    1000 cols x 100 rows (read)                  458 /  468          0.2        4575.5       0.1X\n+    1000 cols x 100 rows (write)                 849 /  875          0.1        8494.0       0.0X\n+    2500 cols x 40 rows (read)                  1077 / 1135          0.1       10770.3       0.0X\n+    2500 cols x 40 rows (write)                 2251 / 2351          0.0       22510.8       0.0X\n+    */\n+  }\n+\n+  ignore(\"wide struct field read and write\") {\n+    val benchmark = new Benchmark(\"wide struct field r/w\", scaleFactor)\n+    for (width <- widthsToTest) {\n+      val numRows = scaleFactor / width\n+      var datum: String = \"{\"\n+      for (i <- 1 to width) {\n+        if (i == 1) {\n+          datum += s\"\"\"\"value_$i\": 1\"\"\"\n+        } else {\n+          datum += s\"\"\", \"value_$i\": 1\"\"\"\n+        }\n+      }\n+      datum += \"}\"\n+      val df = sparkSession.read.json(sparkSession.range(numRows).map(_ => datum).rdd).cache()\n+      df.count()  // force caching\n+      benchmark.addCase(s\"$width wide x $numRows rows (read)\") { iter =>\n+        df.selectExpr(\"sum(value_1)\").collect()\n+      }\n+      benchmark.addCase(s\"$width wide x $numRows rows (write)\") { iter =>\n+        df.selectExpr(\"*\", \"hash(value_1) as f\").selectExpr(s\"sum(value_1)\", \"sum(f)\").collect()\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    wide struct field r/w:                 Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 wide x 100000 rows (read)                   31 /   38          3.2         309.3       1.0X\n+    1 wide x 100000 rows (write)                  55 /   61          1.8         551.6       0.6X\n+    10 wide x 10000 rows (read)                   51 /   53          2.0         506.6       0.6X\n+    10 wide x 10000 rows (write)                  60 /   71          1.7         596.5       0.5X\n+    100 wide x 1000 rows (read)                   74 /   86          1.4         737.4       0.4X\n+    100 wide x 1000 rows (write)                 133 /  154          0.8        1332.1       0.2X\n+    1000 wide x 100 rows (read)                  377 /  441          0.3        3767.3       0.1X\n+    1000 wide x 100 rows (write)                 688 /  828          0.1        6880.8       0.0X\n+    2500 wide x 40 rows (read)                   974 / 1044          0.1        9738.1       0.0X\n+    2500 wide x 40 rows (write)                 2081 / 2256          0.0       20805.8       0.0X\n+    */\n+  }\n+\n+  ignore(\"deeply nested struct field read and write\") {\n+    val benchmark = new Benchmark(\"deeply nested struct field r/w\", scaleFactor)\n+    for (depth <- depthsToTest) {\n+      val numRows = scaleFactor / depth\n+      var datum: String = \"{\\\"value\\\": 1}\"\n+      var selector: String = \"value\"\n+      for (i <- 1 to depth) {\n+        datum = \"{\\\"value\\\": \" + datum + \"}\"\n+        selector = selector + \".value\"\n+      }\n+      val df = sparkSession.read.json(sparkSession.range(numRows).map(_ => datum).rdd).cache()\n+      df.count()  // force caching\n+      benchmark.addCase(s\"$depth deep x $numRows rows (read)\") { iter =>\n+        df.selectExpr(s\"sum($selector)\").collect()\n+      }\n+      benchmark.addCase(s\"$depth deep x $numRows rows (write)\") { iter =>\n+        df.selectExpr(\"*\", s\"$selector as f\").selectExpr(s\"sum($selector)\", \"sum(f)\").collect()\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    deeply nested struct field r/w:        Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 deep x 100000 rows (read)                   38 /   49          2.6         382.2       1.0X\n+    1 deep x 100000 rows (write)                  38 /   45          2.6         382.1       1.0X\n+    10 deep x 10000 rows (read)                   48 /   61          2.1         483.8       0.8X\n+    10 deep x 10000 rows (write)                  93 /  104          1.1         931.2       0.4X\n+    100 deep x 1000 rows (read)                  151 /  162          0.7        1513.2       0.3X\n+    100 deep x 1000 rows (write)                 903 / 1004          0.1        9030.5       0.0X\n+    250 deep x 400 rows (read)                   653 /  735          0.2        6526.8       0.1X\n+    250 deep x 400 rows (write)                 8749 / 9217          0.0       87490.4       0.0X\n+    */\n+  }\n+\n+  ignore(\"bushy struct field read and write\") {\n+    val benchmark = new Benchmark(\"bushy struct field r/w\", scaleFactor)\n+    for (width <- Seq(1, 10, 100, 500)) {\n+      val numRows = scaleFactor / width\n+      var numNodes = 1\n+      var datum: String = \"{\\\"value\\\": 1}\"\n+      var selector: String = \"value\"\n+      var depth = 1\n+      while (numNodes < width) {\n+        numNodes *= 2\n+        datum = s\"\"\"{\"left_$depth\": $datum, \"right_$depth\": $datum}\"\"\"\n+        selector = s\"left_$depth.\" + selector\n+        depth += 1\n+      }\n+      // TODO(ekl) seems like the json parsing is actually the majority of the time, perhaps\n+      // we should benchmark that too separately.\n+      val df = sparkSession.read.json(sparkSession.range(numRows).map(_ => datum).rdd).cache()\n+      df.count()  // force caching\n+      benchmark.addCase(s\"$numNodes nodes x $depth deep x $numRows rows (read)\") { iter =>\n+        df.selectExpr(s\"sum($selector)\").collect()\n+      }\n+      benchmark.addCase(s\"$numNodes nodes x $depth deep x $numRows rows (write)\") { iter =>\n+        df.selectExpr(\"*\", s\"$selector as f\").selectExpr(s\"sum($selector)\", \"sum(f)\").collect()\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic\n+    Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz\n+    bushy struct field r/w:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n+    ----------------------------------------------------------------------------------------------\n+    1 nodes x 1 deep x 100000 rows (read)         57 /   69          1.7         571.7       1.0X\n+    1 nodes x 1 deep x 100000 rows (write)        61 /   75          1.6         614.4       0.9X\n+    16 nodes x 5 deep x 10000 rows (read)         44 /   55          2.3         439.0       1.3X\n+    16 nodes x 5 deep x 10000 rows (write)       100 /  126          1.0         999.5       0.6X\n+    128 nodes x 8 deep x 1000 rows (read)         53 /   61          1.9         532.9       1.1X\n+    128 nodes x 8 deep x 1000 rows (write)       530 /  656          0.2        5295.6       0.1X\n+    512 nodes x 10 deep x 200 rows (read)         98 /  134          1.0         979.2       0.6X\n+    512 nodes x 10 deep x 200 rows (write)      6698 / 7124          0.0       66977.0       0.0X\n+    */\n+  }\n+\n+  //\n+  // The following benchmarks are for reference: the schema is not actually wide for them."
  }],
  "prId": 13456
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "These benckmarks are quite small In my experience C2 JIT compilation will only kick in after a few seconds. So I don't think these are running a full speed.\n",
    "commit": "24acaababce99695c56c49f9ac81e350dc01109d",
    "createdAt": "2016-06-02T16:21:13Z",
    "diffHunk": "@@ -0,0 +1,285 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark for performance with very wide and nested DataFrames.\n+ * To run this:\n+ *  build/sbt \"sql/test-only *WideSchemaBenchmark\"\n+ */\n+class WideSchemaBenchmark extends SparkFunSuite {\n+  private val scaleFactor = 100000\n+  private val widthsToTest = Seq(1, 10, 100, 1000, 2500)\n+  private val depthsToTest = Seq(1, 10, 100, 250)\n+  assert(scaleFactor > widthsToTest.max)\n+\n+  private lazy val sparkSession = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"microbenchmark\")\n+    .getOrCreate()\n+\n+  import sparkSession.implicits._\n+\n+  ignore(\"parsing large select expressions\") {\n+    val benchmark = new Benchmark(\"parsing large select\", 1)\n+    for (width <- widthsToTest) {\n+      val selectExpr = (1 to width).map(i => s\"id as a_$i\")\n+      benchmark.addCase(s\"$width select expressions\") { iter =>\n+        sparkSession.range(1).toDF.selectExpr(selectExpr: _*)\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic"
  }, {
    "author": {
      "login": "ericl"
    },
    "body": "I addressed this in https://github.com/apache/spark/pull/13472: in practice the times didn't seem to change by that much (maybe 1.5x faster for the smaller cases).\n",
    "commit": "24acaababce99695c56c49f9ac81e350dc01109d",
    "createdAt": "2016-06-02T19:25:00Z",
    "diffHunk": "@@ -0,0 +1,285 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.Benchmark\n+\n+/**\n+ * Benchmark for performance with very wide and nested DataFrames.\n+ * To run this:\n+ *  build/sbt \"sql/test-only *WideSchemaBenchmark\"\n+ */\n+class WideSchemaBenchmark extends SparkFunSuite {\n+  private val scaleFactor = 100000\n+  private val widthsToTest = Seq(1, 10, 100, 1000, 2500)\n+  private val depthsToTest = Seq(1, 10, 100, 250)\n+  assert(scaleFactor > widthsToTest.max)\n+\n+  private lazy val sparkSession = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"microbenchmark\")\n+    .getOrCreate()\n+\n+  import sparkSession.implicits._\n+\n+  ignore(\"parsing large select expressions\") {\n+    val benchmark = new Benchmark(\"parsing large select\", 1)\n+    for (width <- widthsToTest) {\n+      val selectExpr = (1 to width).map(i => s\"id as a_$i\")\n+      benchmark.addCase(s\"$width select expressions\") { iter =>\n+        sparkSession.range(1).toDF.selectExpr(selectExpr: _*)\n+      }\n+    }\n+    benchmark.run()\n+\n+    /*\n+    OpenJDK 64-Bit Server VM 1.8.0_66-internal-b17 on Linux 4.2.0-36-generic"
  }],
  "prId": 13456
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Nit: call `super.afterEach()` in a finally block.\n",
    "commit": "24acaababce99695c56c49f9ac81e350dc01109d",
    "createdAt": "2016-06-02T22:28:51Z",
    "diffHunk": "@@ -0,0 +1,372 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.io.File\n+\n+import org.scalatest.BeforeAndAfterEach\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{Benchmark, Utils}\n+\n+/**\n+ * Benchmark for performance with very wide and nested DataFrames.\n+ * To run this:\n+ *  build/sbt \"sql/test-only *WideSchemaBenchmark\"\n+ */\n+class WideSchemaBenchmark extends SparkFunSuite with BeforeAndAfterEach {\n+  private val scaleFactor = 100000\n+  private val widthsToTest = Seq(1, 10, 100, 1000, 2500)\n+  private val depthsToTest = Seq(1, 10, 100, 250)\n+  assert(scaleFactor > widthsToTest.max)\n+\n+  private lazy val sparkSession = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"microbenchmark\")\n+    .getOrCreate()\n+\n+  import sparkSession.implicits._\n+\n+  private var tmpFiles: List[File] = Nil\n+\n+  override def afterEach() {",
    "line": 48
  }, {
    "author": {
      "login": "ericl"
    },
    "body": "Done\n",
    "commit": "24acaababce99695c56c49f9ac81e350dc01109d",
    "createdAt": "2016-06-02T22:51:21Z",
    "diffHunk": "@@ -0,0 +1,372 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import java.io.File\n+\n+import org.scalatest.BeforeAndAfterEach\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.util.{Benchmark, Utils}\n+\n+/**\n+ * Benchmark for performance with very wide and nested DataFrames.\n+ * To run this:\n+ *  build/sbt \"sql/test-only *WideSchemaBenchmark\"\n+ */\n+class WideSchemaBenchmark extends SparkFunSuite with BeforeAndAfterEach {\n+  private val scaleFactor = 100000\n+  private val widthsToTest = Seq(1, 10, 100, 1000, 2500)\n+  private val depthsToTest = Seq(1, 10, 100, 250)\n+  assert(scaleFactor > widthsToTest.max)\n+\n+  private lazy val sparkSession = SparkSession.builder\n+    .master(\"local[1]\")\n+    .appName(\"microbenchmark\")\n+    .getOrCreate()\n+\n+  import sparkSession.implicits._\n+\n+  private var tmpFiles: List[File] = Nil\n+\n+  override def afterEach() {",
    "line": 48
  }],
  "prId": 13456
}]