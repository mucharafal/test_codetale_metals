[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why was this change needed? Is this a breaking change for existing code?",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-18T23:16:29Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "I'm glad you asked. Consider the signature of the new register overloading:\r\n\r\n```scala\r\n  def register[IN: TypeTag, BUF, OUT](\r\n      name: String,\r\n      agg: Aggregator[IN, BUF, OUT]): UserDefinedAggregator[IN, BUF, OUT] = {\r\n```\r\n\r\nIf you don't tell scala you are sending it something function-like, it will now be confused in the Function2[] case",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-18T23:19:31Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "So, in that particular Function2[] case, I guess it is a breaking change. I felt that was acceptable since we're looking at spark 3.0",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-18T23:21:14Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Not sure I follow why it is confusing Scala. Why would adding a new method that accepts Aggregator cause another overload to stop working? Aggregator isn't a Function2",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-18T23:30:39Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "Good question. Scala type inference is a wonderland.  I'm pretty sure it isn't a coincidence that it was the 3-type-parameter overloading that got confused, tho.",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-18T23:46:24Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "So is the problem that this has 3 type parameters, so the method can't be automatically converted to `Function2[R, A1, A2]`? That doesn't make much sense to me since a method clearly isn't an `Aggregator[IN, BUF, OUT]`",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-18T23:52:26Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "My bet is some interaction of the default lifting of `myadd` into `(Int,Int)=>Int`, combined with type erasure.",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-19T00:02:24Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "to make our life easier, how about we use `registerAggregator` as the new function name?  ",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-21T08:05:46Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }, {
    "author": {
      "login": "erikerlandson"
    },
    "body": "I'm ok with a new method name if everyone else is",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-21T12:57:55Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "It would be nice to know what's going on here with Scala, but I don't think this is worth breaking existing code (even if we _can_ for 3.0). Unless we find out what the underlying problem is, I think we should use `registerAggregator`.",
    "commit": "6d0833d42196d4ecf80f914a6458ff2805aa9ee8",
    "createdAt": "2019-10-21T20:41:54Z",
    "diffHunk": "@@ -51,7 +51,7 @@ class SQLContextSuite extends SparkFunSuite with SharedSparkContext {\n \n     // UDF should not be shared\n     def myadd(a: Int, b: Int): Int = a + b\n-    session1.udf.register[Int, Int, Int](\"myadd\", myadd)\n+    session1.udf.register[Int, Int, Int](\"myadd\", myadd(_, _))"
  }],
  "prId": 25024
}]