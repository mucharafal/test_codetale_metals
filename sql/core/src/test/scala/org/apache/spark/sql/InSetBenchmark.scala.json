[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`values mkString \",\"` -> `values.mkString(\",\")`",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T19:35:06Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def longBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def floatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def doubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def smallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def largeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def stringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def timestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def dateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def arrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def structBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def benchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Benchmark = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values mkString \",\"})\")"
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "Yep, will update that.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T19:39:22Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def longBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def floatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def doubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def smallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def largeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def stringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def timestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def dateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def arrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def structBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def benchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Benchmark = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values mkString \",\"})\")"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Is `smallNumRows` used simply in order to reduce the benchmark time?",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T19:35:57Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def longBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def floatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def doubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def smallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def largeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def stringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def timestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def dateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def arrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def structBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def benchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Benchmark = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values mkString \",\"})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000"
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "Yes, exactly, `smallNumRows` is used with data types that take longer to process.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T19:39:10Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def longBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def floatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def doubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def smallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def largeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def stringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def timestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def dateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def arrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def structBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def benchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Benchmark = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values mkString \",\"})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Ur, can we use a consistent set of `numItems` if there is no other reason?\r\nHere, we already used three different sets; `byte` -> (10,50), `short` -> (10, 100), `int` -> (10, 50, 250).",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T19:40:06Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def longBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def floatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def doubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def smallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def largeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def stringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def timestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def dateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def arrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def structBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def benchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Benchmark = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values mkString \",\"})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"InSet Expression Benchmark\") {\n+      byteBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      byteBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+\n+      shortBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      shortBenchmark(numItems = 100, largeNumRows, minNumIters).run()\n+\n+      intBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 250, largeNumRows, minNumIters).run()"
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "What about (10,50) for bytes/shorts and (10, 50, 250) for ints/longs?",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T19:48:56Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def longBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def floatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def doubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def smallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def largeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def stringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def timestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def dateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def arrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def structBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def benchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Benchmark = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values mkString \",\"})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"InSet Expression Benchmark\") {\n+      byteBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      byteBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+\n+      shortBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      shortBenchmark(numItems = 100, largeNumRows, minNumIters).run()\n+\n+      intBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 250, largeNumRows, minNumIters).run()"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Since the default value of `OPTIMIZER_INSET_CONVERSION_THRESHOLD` is 10, what about `(10, 25, 50, 100, 200)` for all types? We had better collect more data points (if possible) because this is a benchmark and not a part of UTs. ",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T19:59:29Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def longBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def floatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def doubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def smallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def largeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def stringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def timestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def dateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def arrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def structBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def benchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Benchmark = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values mkString \",\"})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"InSet Expression Benchmark\") {\n+      byteBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      byteBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+\n+      shortBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      shortBenchmark(numItems = 100, largeNumRows, minNumIters).run()\n+\n+      intBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 250, largeNumRows, minNumIters).run()"
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "If there are no objections w.r.t. to increasing the execution time, it would make sense to have more data points, I agree.\r\n\r\nWhat if we follow the pattern (10, 25, 50, 100, 200) but stop as soon as `InSet` starts to outperform `In`? For example, `InSet` starts to match the performance of `In` on 50 bytes. If we run the test on 100 bytes, `InSet` will be faster than `In`. So, I propose to drop the case with 200 elements as the outcome is clear and it will just increase the benchmarking time.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T21:38:21Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def longBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def floatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def doubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def smallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def largeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def stringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def timestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def dateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def arrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def structBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def benchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Benchmark = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values mkString \",\"})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"InSet Expression Benchmark\") {\n+      byteBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      byteBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+\n+      shortBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      shortBenchmark(numItems = 100, largeNumRows, minNumIters).run()\n+\n+      intBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 250, largeNumRows, minNumIters).run()"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Non-determinism is not good for benchmark because everything(Spark/JVM/HW) will change. BTW, at the final step, I will run this benchmark on Amazon EC2 machine, too.\r\n> stop as soon as `InSet` starts to outperform `In`",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T21:50:54Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def longBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems longs\"\n+    val values = (1 to numItems).map(v => s\"${v}L\")\n+    val df = spark.range(1, numRows).toDF(\"id\")\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def floatBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems floats\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS float)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(FloatType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def doubleBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems doubles\"\n+    val values = 1.0 to numItems by 1.0\n+    val df = spark.range(1, numRows).select($\"id\".cast(DoubleType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def smallDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems small decimals\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS decimal(12, 1))\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(12, 1)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def largeDecimalBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems large decimals\"\n+    val values = (1 to numItems).map(v => s\"9223372036854775812.10539$v\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(DecimalType(30, 7)))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def stringBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems strings\"\n+    val values = (1 to numItems).map(n => s\"'$n'\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(StringType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def timestampBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems timestamps\"\n+    val values = (1 to numItems).map(m => s\"CAST('1970-01-01 01:00:$m' AS timestamp)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def dateBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems dates\"\n+    val values = (1 to numItems).map(n => 1970 + n).map(y => s\"CAST('$y-01-01' AS date)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(TimestampType).cast(DateType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def arrayBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems arrays\"\n+    val values = (1 to numItems).map(i => s\"array($i)\")\n+    val df = spark.range(1, numRows).select(array($\"id\").as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def structBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems structs\"\n+    val values = (1 to numItems).map(i => s\"struct($i)\")\n+    val df = spark.range(1, numRows).select(struct($\"id\".as(\"col1\")).as(\"id\"))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def benchmark(\n+      name: String,\n+      df: DataFrame,\n+      values: Seq[Any],\n+      numRows: Long,\n+      minNumIters: Int): Benchmark = {\n+\n+    val benchmark = new Benchmark(name, numRows, minNumIters, output = output)\n+\n+    df.createOrReplaceTempView(\"t\")\n+\n+    def testClosure(): Unit = {\n+      val df = spark.sql(s\"SELECT * FROM t WHERE id IN (${values mkString \",\"})\")\n+      df.queryExecution.toRdd.foreach(_ => Unit)\n+    }\n+\n+    benchmark.addCase(\"In expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> values.size.toString) {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark.addCase(\"InSet expression\") { _ =>\n+      withSQLConf(SQLConf.OPTIMIZER_INSET_CONVERSION_THRESHOLD.key -> \"1\") {\n+        testClosure()\n+      }\n+    }\n+\n+    benchmark\n+  }\n+\n+  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\n+    val largeNumRows = 10000000\n+    val smallNumRows = 1000000\n+    val minNumIters = 5\n+\n+    runBenchmark(\"InSet Expression Benchmark\") {\n+      byteBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      byteBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+\n+      shortBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      shortBenchmark(numItems = 100, largeNumRows, minNumIters).run()\n+\n+      intBenchmark(numItems = 10, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 50, largeNumRows, minNumIters).run()\n+      intBenchmark(numItems = 250, largeNumRows, minNumIters).run()"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "nit. Shall we invoke `run` here intead of `intBenchmark(...).run()`? It seems to be repeated multiple times.\r\n```\r\n- benchmark(name, df, values, numRows, minNumIters)\r\n+ benchmark(name, df, values, numRows, minNumIters).run()\r\n```",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T19:49:46Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)"
  }, {
    "author": {
      "login": "aokolnychyi"
    },
    "body": "We can do this. Shall we rename `intBenchmark` to `runIntBenchmark` then? There is no consistency in existing benchmarks, unfortunately. ",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T21:22:14Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "+1 for renaming. Yep. We overlooked the naming consistency in the previous benchmarks.",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T21:25:45Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems bytes\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS tinyint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ByteType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def shortBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems shorts\"\n+    val values = (1 to numItems).map(v => s\"CAST($v AS smallint)\")\n+    val df = spark.range(1, numRows).select($\"id\".cast(ShortType))\n+    benchmark(name, df, values, numRows, minNumIters)\n+  }\n+\n+  def intBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {\n+    val name = s\"$numItems ints\"\n+    val values = 1 to numItems\n+    val df = spark.range(1, numRows).select($\"id\".cast(IntegerType))\n+    benchmark(name, df, values, numRows, minNumIters)"
  }],
  "prId": 23291
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`def` -> `private def`",
    "commit": "713f3dbcfafbdb52d05c611259f8d7bee41f7393",
    "createdAt": "2018-12-11T19:50:28Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql\n+\n+import org.apache.spark.benchmark.Benchmark\n+import org.apache.spark.sql.catalyst.expressions.In\n+import org.apache.spark.sql.catalyst.expressions.InSet\n+import org.apache.spark.sql.execution.benchmark.SqlBasedBenchmark\n+import org.apache.spark.sql.functions.{array, struct}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * A benchmark that compares the performance of [[In]] and [[InSet]] expressions.\n+ *\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class> <spark sql test jar>\n+ *   2. build/sbt \"sql/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/InSetBenchmark-results.txt\".\n+ * }}}\n+ */\n+object InSetBenchmark extends SqlBasedBenchmark {\n+\n+  import spark.implicits._\n+\n+  def byteBenchmark(numItems: Int, numRows: Long, minNumIters: Int): Benchmark = {"
  }],
  "prId": 23291
}]