[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "You need to add the Apache License header at the beginning of a file.",
    "commit": "576600e2dcf9a58161a566e181f7fbc3ac94ae14",
    "createdAt": "2018-08-29T23:19:00Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+package test.org.apache.spark.sql;",
    "line": 18
  }],
  "prId": 22253
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Can we shorten this method name? E.g., `checkExceptionMessage`?",
    "commit": "576600e2dcf9a58161a566e181f7fbc3ac94ae14",
    "createdAt": "2018-08-29T23:23:00Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+package test.org.apache.spark.sql;\n+\n+import org.apache.spark.api.java.function.FilterFunction;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.*;\n+\n+import static org.apache.spark.sql.types.DataTypes.*;\n+\n+public class JavaColumnExpressionSuite {\n+\n+    private transient TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void isInCollectionWorksCorrectlyOnJava() {\n+        List<Row> rows = Arrays.asList(\n+                RowFactory.create(1, \"x\"),\n+                RowFactory.create(2, \"y\"),\n+                RowFactory.create(3, \"z\")\n+        );\n+        StructType schema = createStructType(Arrays.asList(\n+                createStructField(\"a\", IntegerType, false),\n+                createStructField(\"b\", StringType, false)\n+        ));\n+        Dataset<Row> df = spark.createDataFrame(rows, schema);\n+        // Test with different types of collections\n+        Assert.assertTrue(Arrays.equals(\n+                (Row[]) df.filter(df.col(\"a\").isInCollection(Arrays.asList(1, 2))).collect(),\n+                (Row[]) df.filter((FilterFunction<Row>) r -> r.getInt(0) == 1 || r.getInt(0) == 2).collect()\n+        ));\n+        Assert.assertTrue(Arrays.equals(\n+                (Row[]) df.filter(df.col(\"a\").isInCollection(new HashSet<>(Arrays.asList(1, 2)))).collect(),\n+                (Row[]) df.filter((FilterFunction<Row>) r -> r.getInt(0) == 1 || r.getInt(0) == 2).collect()\n+        ));\n+        Assert.assertTrue(Arrays.equals(\n+                (Row[]) df.filter(df.col(\"a\").isInCollection(new ArrayList<>(Arrays.asList(3, 1)))).collect(),\n+                (Row[]) df.filter((FilterFunction<Row>) r -> r.getInt(0) == 3 || r.getInt(0) == 1).collect()\n+        ));\n+    }\n+\n+    @Test\n+    public void isInCollectionThrowsExceptionWithCorrectMessageOnJava() {"
  }],
  "prId": 22253
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Java file also should be two space indentation",
    "commit": "576600e2dcf9a58161a566e181f7fbc3ac94ae14",
    "createdAt": "2018-08-30T01:34:29Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+package test.org.apache.spark.sql;\n+\n+import org.apache.spark.api.java.function.FilterFunction;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.*;\n+\n+import static org.apache.spark.sql.types.DataTypes.*;\n+\n+public class JavaColumnExpressionSuite {\n+\n+    private transient TestSparkSession spark;"
  }],
  "prId": 22253
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Shall we avoid wlidcard import if they are less then 5 imports",
    "commit": "576600e2dcf9a58161a566e181f7fbc3ac94ae14",
    "createdAt": "2018-08-30T01:35:20Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+package test.org.apache.spark.sql;\n+\n+import org.apache.spark.api.java.function.FilterFunction;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.*;\n+\n+import static org.apache.spark.sql.types.DataTypes.*;",
    "line": 34
  }, {
    "author": {
      "login": "aai95"
    },
    "body": "Hello @HyukjinKwon, I tried to remove the wildcard import and got the following result:\r\n\r\n`import java.util.ArrayList;`\r\n`import java.util.Arrays;`\r\n`import java.util.HashSet;`\r\n`import java.util.List;`\r\n`import java.util.Locale;`\r\n\r\n`import static org.apache.spark.sql.types.DataTypes.IntegerType;`\r\n`import static org.apache.spark.sql.types.DataTypes.StringType;`\r\n`import static org.apache.spark.sql.types.DataTypes.createArrayType;`\r\n`import static org.apache.spark.sql.types.DataTypes.createStructField;`\r\n`import static org.apache.spark.sql.types.DataTypes.createStructType;`\r\n\r\nIs it OK to use wildcard here if exactly 5 imports were used in every case? Thanks.",
    "commit": "576600e2dcf9a58161a566e181f7fbc3ac94ae14",
    "createdAt": "2018-08-30T08:30:40Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+package test.org.apache.spark.sql;\n+\n+import org.apache.spark.api.java.function.FilterFunction;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.*;\n+\n+import static org.apache.spark.sql.types.DataTypes.*;",
    "line": 34
  }],
  "prId": 22253
}]