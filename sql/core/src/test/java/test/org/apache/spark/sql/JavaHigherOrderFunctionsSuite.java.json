[{
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: style. one more indent?",
    "commit": "64c0f87a8005a27458394a14648c0e75ee514678",
    "createdAt": "2019-10-01T21:26:14Z",
    "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import scala.collection.Seq;\n+import static scala.collection.JavaConverters.mapAsScalaMap;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.*;\n+import static org.apache.spark.sql.types.DataTypes.*;\n+import static org.apache.spark.sql.functions.*;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import static test.org.apache.spark.sql.JavaTestUtils.*;\n+import test.org.apache.spark.sql.JavaTestUtils;\n+\n+public class JavaHigherOrderFunctionsSuite {\n+    private transient TestSparkSession spark;\n+    private Dataset<Row> arrDf;\n+    private Dataset<Row> mapDf;\n+\n+    private void setUpArrDf() {\n+        List<Row> data = toRows(\n+            makeArray(1, 9, 8, 7),\n+            makeArray(5, 8, 9, 7, 2),\n+            JavaTestUtils.<Integer>makeArray(),\n+            null\n+        );\n+        StructType schema =  new StructType()\n+            .add(\"x\", new ArrayType(IntegerType, true), true);\n+        arrDf = spark.createDataFrame(data, schema);\n+    }\n+\n+    private void setUpMapDf() {\n+        List<Row> data = toRows(\n+            new HashMap<Integer, Integer>() {{\n+                put(1, 1);\n+                put(2, 2);\n+            }},\n+            null\n+        );\n+        StructType schema = new StructType()\n+            .add(\"x\", new MapType(IntegerType, IntegerType, true));\n+        mapDf = spark.createDataFrame(data, schema);\n+    }\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+        setUpArrDf();\n+        setUpMapDf();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testTransform() {\n+        checkAnswer(\n+            arrDf.select(transform(col(\"x\"), x -> x.plus(1))),\n+            toRows(\n+                makeArray(2, 10, 9, 8),\n+                makeArray(6, 9, 10, 8, 3),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+        checkAnswer(\n+            arrDf.select(transform(col(\"x\"), (x, i) -> x.plus(i))),\n+            toRows(\n+                makeArray(1, 10, 10, 10),\n+                makeArray(5, 9, 11, 10, 6),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testFilter() {\n+        checkAnswer(\n+            arrDf.select(filter(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                makeArray(9),\n+                makeArray(9),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testExists() {\n+        checkAnswer(\n+            arrDf.select(exists(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                true,\n+                true,\n+                false,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testForall() {\n+        checkAnswer(\n+            arrDf.select(forall(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                false,\n+                false,\n+                true,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testAggregate() {\n+        checkAnswer(\n+            arrDf.select(aggregate(col(\"x\"), lit(0), (acc, x) -> acc.plus(x))),\n+            toRows(\n+                25,\n+                31,\n+                0,\n+                null\n+            ));\n+        checkAnswer(\n+            arrDf.select(aggregate(col(\"x\"), lit(0), (acc, x) -> acc.plus(x), x -> x)),\n+            toRows(\n+                25,\n+                31,\n+                0,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testZipWith() {\n+        checkAnswer(\n+            arrDf.select(zip_with(col(\"x\"), col(\"x\"), (a, b) -> lit(42))),\n+            toRows(\n+                makeArray(42, 42, 42, 42),\n+                makeArray(42, 42, 42, 42, 42),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testTransformKeys() {\n+        checkAnswer(\n+            mapDf.select(transform_keys(col(\"x\"), (k, v) -> k.plus(v))),\n+            toRows(\n+                mapAsScalaMap(\n+                new HashMap<Integer, Integer>() {{\n+                    put(2, 1);\n+                    put(4, 2);\n+                }}),\n+                null",
    "line": 185
  }],
  "prId": 24232
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "ditto.",
    "commit": "64c0f87a8005a27458394a14648c0e75ee514678",
    "createdAt": "2019-10-01T21:26:21Z",
    "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import scala.collection.Seq;\n+import static scala.collection.JavaConverters.mapAsScalaMap;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.*;\n+import static org.apache.spark.sql.types.DataTypes.*;\n+import static org.apache.spark.sql.functions.*;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import static test.org.apache.spark.sql.JavaTestUtils.*;\n+import test.org.apache.spark.sql.JavaTestUtils;\n+\n+public class JavaHigherOrderFunctionsSuite {\n+    private transient TestSparkSession spark;\n+    private Dataset<Row> arrDf;\n+    private Dataset<Row> mapDf;\n+\n+    private void setUpArrDf() {\n+        List<Row> data = toRows(\n+            makeArray(1, 9, 8, 7),\n+            makeArray(5, 8, 9, 7, 2),\n+            JavaTestUtils.<Integer>makeArray(),\n+            null\n+        );\n+        StructType schema =  new StructType()\n+            .add(\"x\", new ArrayType(IntegerType, true), true);\n+        arrDf = spark.createDataFrame(data, schema);\n+    }\n+\n+    private void setUpMapDf() {\n+        List<Row> data = toRows(\n+            new HashMap<Integer, Integer>() {{\n+                put(1, 1);\n+                put(2, 2);\n+            }},\n+            null\n+        );\n+        StructType schema = new StructType()\n+            .add(\"x\", new MapType(IntegerType, IntegerType, true));\n+        mapDf = spark.createDataFrame(data, schema);\n+    }\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+        setUpArrDf();\n+        setUpMapDf();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testTransform() {\n+        checkAnswer(\n+            arrDf.select(transform(col(\"x\"), x -> x.plus(1))),\n+            toRows(\n+                makeArray(2, 10, 9, 8),\n+                makeArray(6, 9, 10, 8, 3),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+        checkAnswer(\n+            arrDf.select(transform(col(\"x\"), (x, i) -> x.plus(i))),\n+            toRows(\n+                makeArray(1, 10, 10, 10),\n+                makeArray(5, 9, 11, 10, 6),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testFilter() {\n+        checkAnswer(\n+            arrDf.select(filter(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                makeArray(9),\n+                makeArray(9),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testExists() {\n+        checkAnswer(\n+            arrDf.select(exists(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                true,\n+                true,\n+                false,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testForall() {\n+        checkAnswer(\n+            arrDf.select(forall(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                false,\n+                false,\n+                true,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testAggregate() {\n+        checkAnswer(\n+            arrDf.select(aggregate(col(\"x\"), lit(0), (acc, x) -> acc.plus(x))),\n+            toRows(\n+                25,\n+                31,\n+                0,\n+                null\n+            ));\n+        checkAnswer(\n+            arrDf.select(aggregate(col(\"x\"), lit(0), (acc, x) -> acc.plus(x), x -> x)),\n+            toRows(\n+                25,\n+                31,\n+                0,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testZipWith() {\n+        checkAnswer(\n+            arrDf.select(zip_with(col(\"x\"), col(\"x\"), (a, b) -> lit(42))),\n+            toRows(\n+                makeArray(42, 42, 42, 42),\n+                makeArray(42, 42, 42, 42, 42),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testTransformKeys() {\n+        checkAnswer(\n+            mapDf.select(transform_keys(col(\"x\"), (k, v) -> k.plus(v))),\n+            toRows(\n+                mapAsScalaMap(\n+                new HashMap<Integer, Integer>() {{\n+                    put(2, 1);\n+                    put(4, 2);\n+                }}),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testTransformValues() {\n+        checkAnswer(\n+            mapDf.select(transform_values(col(\"x\"), (k, v) -> k.plus(v))),\n+            toRows(\n+                mapAsScalaMap(\n+                new HashMap<Integer, Integer>() {{\n+                    put(1, 2);\n+                    put(2, 4);\n+                }}),\n+                null",
    "line": 199
  }, {
    "author": {
      "login": "nvander1"
    },
    "body": "```java\r\n    @Test\r\n    public void testTransformValues() {\r\n        checkAnswer(\r\n            mapDf.select(transform_values(col(\"x\"), (k, v) -> k.plus(v))),\r\n            toRows(\r\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\r\n                    put(1, 2);\r\n                    put(2, 4);\r\n                }}),\r\n                null\r\n            )\r\n        );\r\n    }\r\n```\r\n\r\nDoes this work as well? I've moved the new HashMap up a line. @ueshin \r\n\r\nAlso, what is the general preference in the codebase, each paren and brace on a new line? \r\n\r\nOr the more \"lispy\" style of every close on the same line:\r\n\r\n```java\r\n    @Test\r\n    public void testTransformValues() {\r\n        checkAnswer(\r\n            mapDf.select(transform_values(col(\"x\"), (k, v) -> k.plus(v))),\r\n            toRows(\r\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\r\n                    put(1, 2);\r\n                    put(2, 4);}}),\r\n                null));\r\n    }\r\n```\r\n\r\nI've seen a mixture of the two to various degrees in the code, I edited this file to at least be consistent with itself (the exception here being the mapAsScalaMap / hashmap since it really is its own entity just being converted to a scala equivalent.",
    "commit": "64c0f87a8005a27458394a14648c0e75ee514678",
    "createdAt": "2019-10-02T02:36:25Z",
    "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import scala.collection.Seq;\n+import static scala.collection.JavaConverters.mapAsScalaMap;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.*;\n+import static org.apache.spark.sql.types.DataTypes.*;\n+import static org.apache.spark.sql.functions.*;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import static test.org.apache.spark.sql.JavaTestUtils.*;\n+import test.org.apache.spark.sql.JavaTestUtils;\n+\n+public class JavaHigherOrderFunctionsSuite {\n+    private transient TestSparkSession spark;\n+    private Dataset<Row> arrDf;\n+    private Dataset<Row> mapDf;\n+\n+    private void setUpArrDf() {\n+        List<Row> data = toRows(\n+            makeArray(1, 9, 8, 7),\n+            makeArray(5, 8, 9, 7, 2),\n+            JavaTestUtils.<Integer>makeArray(),\n+            null\n+        );\n+        StructType schema =  new StructType()\n+            .add(\"x\", new ArrayType(IntegerType, true), true);\n+        arrDf = spark.createDataFrame(data, schema);\n+    }\n+\n+    private void setUpMapDf() {\n+        List<Row> data = toRows(\n+            new HashMap<Integer, Integer>() {{\n+                put(1, 1);\n+                put(2, 2);\n+            }},\n+            null\n+        );\n+        StructType schema = new StructType()\n+            .add(\"x\", new MapType(IntegerType, IntegerType, true));\n+        mapDf = spark.createDataFrame(data, schema);\n+    }\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+        setUpArrDf();\n+        setUpMapDf();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testTransform() {\n+        checkAnswer(\n+            arrDf.select(transform(col(\"x\"), x -> x.plus(1))),\n+            toRows(\n+                makeArray(2, 10, 9, 8),\n+                makeArray(6, 9, 10, 8, 3),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+        checkAnswer(\n+            arrDf.select(transform(col(\"x\"), (x, i) -> x.plus(i))),\n+            toRows(\n+                makeArray(1, 10, 10, 10),\n+                makeArray(5, 9, 11, 10, 6),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testFilter() {\n+        checkAnswer(\n+            arrDf.select(filter(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                makeArray(9),\n+                makeArray(9),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testExists() {\n+        checkAnswer(\n+            arrDf.select(exists(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                true,\n+                true,\n+                false,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testForall() {\n+        checkAnswer(\n+            arrDf.select(forall(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                false,\n+                false,\n+                true,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testAggregate() {\n+        checkAnswer(\n+            arrDf.select(aggregate(col(\"x\"), lit(0), (acc, x) -> acc.plus(x))),\n+            toRows(\n+                25,\n+                31,\n+                0,\n+                null\n+            ));\n+        checkAnswer(\n+            arrDf.select(aggregate(col(\"x\"), lit(0), (acc, x) -> acc.plus(x), x -> x)),\n+            toRows(\n+                25,\n+                31,\n+                0,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testZipWith() {\n+        checkAnswer(\n+            arrDf.select(zip_with(col(\"x\"), col(\"x\"), (a, b) -> lit(42))),\n+            toRows(\n+                makeArray(42, 42, 42, 42),\n+                makeArray(42, 42, 42, 42, 42),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testTransformKeys() {\n+        checkAnswer(\n+            mapDf.select(transform_keys(col(\"x\"), (k, v) -> k.plus(v))),\n+            toRows(\n+                mapAsScalaMap(\n+                new HashMap<Integer, Integer>() {{\n+                    put(2, 1);\n+                    put(4, 2);\n+                }}),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testTransformValues() {\n+        checkAnswer(\n+            mapDf.select(transform_values(col(\"x\"), (k, v) -> k.plus(v))),\n+            toRows(\n+                mapAsScalaMap(\n+                new HashMap<Integer, Integer>() {{\n+                    put(1, 2);\n+                    put(2, 4);\n+                }}),\n+                null",
    "line": 199
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "Maybe the first one is more preferred.\r\nThe second one needs a line break at the end of `HashMap` since it's a block:\r\n\r\n```java\r\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\r\n                    put(1, 2);\r\n                    put(2, 4);\r\n                }}),\r\n                null));\r\n```\r\n\r\nI'm not quite sure about the parentheses after `null`. Maybe we need a line break as well.\r\n\r\nAs for my comment, sorry, maybe my pointer was wrong.\r\nI meant `new HashMap ...` should be on one more indent.\r\n\r\n```java\r\n                mapAsScalaMap(\r\n                    new HashMap<Integer, Integer>() {{\t\r\n                        put(1, 2);\r\n                        put(2, 4);\r\n                    }}\r\n                ),\r\n                null\r\n```\r\n",
    "commit": "64c0f87a8005a27458394a14648c0e75ee514678",
    "createdAt": "2019-10-02T04:06:39Z",
    "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import scala.collection.Seq;\n+import static scala.collection.JavaConverters.mapAsScalaMap;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.*;\n+import static org.apache.spark.sql.types.DataTypes.*;\n+import static org.apache.spark.sql.functions.*;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import static test.org.apache.spark.sql.JavaTestUtils.*;\n+import test.org.apache.spark.sql.JavaTestUtils;\n+\n+public class JavaHigherOrderFunctionsSuite {\n+    private transient TestSparkSession spark;\n+    private Dataset<Row> arrDf;\n+    private Dataset<Row> mapDf;\n+\n+    private void setUpArrDf() {\n+        List<Row> data = toRows(\n+            makeArray(1, 9, 8, 7),\n+            makeArray(5, 8, 9, 7, 2),\n+            JavaTestUtils.<Integer>makeArray(),\n+            null\n+        );\n+        StructType schema =  new StructType()\n+            .add(\"x\", new ArrayType(IntegerType, true), true);\n+        arrDf = spark.createDataFrame(data, schema);\n+    }\n+\n+    private void setUpMapDf() {\n+        List<Row> data = toRows(\n+            new HashMap<Integer, Integer>() {{\n+                put(1, 1);\n+                put(2, 2);\n+            }},\n+            null\n+        );\n+        StructType schema = new StructType()\n+            .add(\"x\", new MapType(IntegerType, IntegerType, true));\n+        mapDf = spark.createDataFrame(data, schema);\n+    }\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+        setUpArrDf();\n+        setUpMapDf();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testTransform() {\n+        checkAnswer(\n+            arrDf.select(transform(col(\"x\"), x -> x.plus(1))),\n+            toRows(\n+                makeArray(2, 10, 9, 8),\n+                makeArray(6, 9, 10, 8, 3),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+        checkAnswer(\n+            arrDf.select(transform(col(\"x\"), (x, i) -> x.plus(i))),\n+            toRows(\n+                makeArray(1, 10, 10, 10),\n+                makeArray(5, 9, 11, 10, 6),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testFilter() {\n+        checkAnswer(\n+            arrDf.select(filter(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                makeArray(9),\n+                makeArray(9),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testExists() {\n+        checkAnswer(\n+            arrDf.select(exists(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                true,\n+                true,\n+                false,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testForall() {\n+        checkAnswer(\n+            arrDf.select(forall(col(\"x\"), x -> x.plus(1).equalTo(10))),\n+            toRows(\n+                false,\n+                false,\n+                true,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testAggregate() {\n+        checkAnswer(\n+            arrDf.select(aggregate(col(\"x\"), lit(0), (acc, x) -> acc.plus(x))),\n+            toRows(\n+                25,\n+                31,\n+                0,\n+                null\n+            ));\n+        checkAnswer(\n+            arrDf.select(aggregate(col(\"x\"), lit(0), (acc, x) -> acc.plus(x), x -> x)),\n+            toRows(\n+                25,\n+                31,\n+                0,\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testZipWith() {\n+        checkAnswer(\n+            arrDf.select(zip_with(col(\"x\"), col(\"x\"), (a, b) -> lit(42))),\n+            toRows(\n+                makeArray(42, 42, 42, 42),\n+                makeArray(42, 42, 42, 42, 42),\n+                JavaTestUtils.<Integer>makeArray(),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testTransformKeys() {\n+        checkAnswer(\n+            mapDf.select(transform_keys(col(\"x\"), (k, v) -> k.plus(v))),\n+            toRows(\n+                mapAsScalaMap(\n+                new HashMap<Integer, Integer>() {{\n+                    put(2, 1);\n+                    put(4, 2);\n+                }}),\n+                null\n+            ));\n+    }\n+\n+    @Test\n+    public void testTransformValues() {\n+        checkAnswer(\n+            mapDf.select(transform_values(col(\"x\"), (k, v) -> k.plus(v))),\n+            toRows(\n+                mapAsScalaMap(\n+                new HashMap<Integer, Integer>() {{\n+                    put(1, 2);\n+                    put(2, 4);\n+                }}),\n+                null",
    "line": 199
  }],
  "prId": 24232
}]