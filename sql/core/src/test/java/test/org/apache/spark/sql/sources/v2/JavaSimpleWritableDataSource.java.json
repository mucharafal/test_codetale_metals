[{
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "Why the translation?",
    "commit": "7eeb3b0bd15644d3facddefcdd2a218316573953",
    "createdAt": "2017-09-30T12:10:02Z",
    "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql.sources.v2;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.sources.v2.DataSourceV2;\n+import org.apache.spark.sql.sources.v2.DataSourceV2Options;\n+import org.apache.spark.sql.sources.v2.ReadSupport;\n+import org.apache.spark.sql.sources.v2.WriteSupport;\n+import org.apache.spark.sql.sources.v2.reader.DataReader;\n+import org.apache.spark.sql.sources.v2.reader.DataSourceV2Reader;\n+import org.apache.spark.sql.sources.v2.reader.ReadTask;\n+import org.apache.spark.sql.sources.v2.writer.*;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaSimpleWritableDataSource implements DataSourceV2, ReadSupport, WriteSupport {\n+  private StructType schema = new StructType().add(\"i\", \"long\").add(\"j\", \"long\");\n+\n+  class Reader implements DataSourceV2Reader {\n+    private String path;\n+\n+    Reader(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public StructType readSchema() {\n+      return schema;\n+    }\n+\n+    @Override\n+    public List<ReadTask<Row>> createReadTasks() {\n+      return java.util.Arrays.asList(new JavaSimpleCSVReadTask(path));\n+    }\n+  }\n+\n+  static class JavaSimpleCSVReadTask implements ReadTask<Row>, DataReader<Row> {\n+    private String path;\n+    private volatile Iterator<String> lines;\n+    private volatile String currentLine;\n+\n+    JavaSimpleCSVReadTask(Iterator<String> lines) {\n+      this.lines = lines;\n+    }\n+\n+    JavaSimpleCSVReadTask(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataReader<Row> createReader() {\n+      assert path != null;\n+      try {\n+        if (Files.exists(Paths.get(path))) {\n+          return new JavaSimpleCSVReadTask(Files.readAllLines(Paths.get(path)).iterator());\n+        } else {\n+          return new JavaSimpleCSVReadTask(Collections.emptyIterator());\n+        }\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Because `IOException` is a checked exception and needs to be declared in method signature, this is just a test so wanna make it as simple as possible.",
    "commit": "7eeb3b0bd15644d3facddefcdd2a218316573953",
    "createdAt": "2017-10-09T16:14:31Z",
    "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql.sources.v2;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.sources.v2.DataSourceV2;\n+import org.apache.spark.sql.sources.v2.DataSourceV2Options;\n+import org.apache.spark.sql.sources.v2.ReadSupport;\n+import org.apache.spark.sql.sources.v2.WriteSupport;\n+import org.apache.spark.sql.sources.v2.reader.DataReader;\n+import org.apache.spark.sql.sources.v2.reader.DataSourceV2Reader;\n+import org.apache.spark.sql.sources.v2.reader.ReadTask;\n+import org.apache.spark.sql.sources.v2.writer.*;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaSimpleWritableDataSource implements DataSourceV2, ReadSupport, WriteSupport {\n+  private StructType schema = new StructType().add(\"i\", \"long\").add(\"j\", \"long\");\n+\n+  class Reader implements DataSourceV2Reader {\n+    private String path;\n+\n+    Reader(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public StructType readSchema() {\n+      return schema;\n+    }\n+\n+    @Override\n+    public List<ReadTask<Row>> createReadTasks() {\n+      return java.util.Arrays.asList(new JavaSimpleCSVReadTask(path));\n+    }\n+  }\n+\n+  static class JavaSimpleCSVReadTask implements ReadTask<Row>, DataReader<Row> {\n+    private String path;\n+    private volatile Iterator<String> lines;\n+    private volatile String currentLine;\n+\n+    JavaSimpleCSVReadTask(Iterator<String> lines) {\n+      this.lines = lines;\n+    }\n+\n+    JavaSimpleCSVReadTask(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataReader<Row> createReader() {\n+      assert path != null;\n+      try {\n+        if (Files.exists(Paths.get(path))) {\n+          return new JavaSimpleCSVReadTask(Files.readAllLines(Paths.get(path)).iterator());\n+        } else {\n+          return new JavaSimpleCSVReadTask(Collections.emptyIterator());\n+        }\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Does Spark have a `RuntimeIOException`? I typically like to use those so that the IOException can still be caught and handled by code that chooses to.",
    "commit": "7eeb3b0bd15644d3facddefcdd2a218316573953",
    "createdAt": "2017-10-09T16:56:24Z",
    "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql.sources.v2;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.sources.v2.DataSourceV2;\n+import org.apache.spark.sql.sources.v2.DataSourceV2Options;\n+import org.apache.spark.sql.sources.v2.ReadSupport;\n+import org.apache.spark.sql.sources.v2.WriteSupport;\n+import org.apache.spark.sql.sources.v2.reader.DataReader;\n+import org.apache.spark.sql.sources.v2.reader.DataSourceV2Reader;\n+import org.apache.spark.sql.sources.v2.reader.ReadTask;\n+import org.apache.spark.sql.sources.v2.writer.*;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaSimpleWritableDataSource implements DataSourceV2, ReadSupport, WriteSupport {\n+  private StructType schema = new StructType().add(\"i\", \"long\").add(\"j\", \"long\");\n+\n+  class Reader implements DataSourceV2Reader {\n+    private String path;\n+\n+    Reader(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public StructType readSchema() {\n+      return schema;\n+    }\n+\n+    @Override\n+    public List<ReadTask<Row>> createReadTasks() {\n+      return java.util.Arrays.asList(new JavaSimpleCSVReadTask(path));\n+    }\n+  }\n+\n+  static class JavaSimpleCSVReadTask implements ReadTask<Row>, DataReader<Row> {\n+    private String path;\n+    private volatile Iterator<String> lines;\n+    private volatile String currentLine;\n+\n+    JavaSimpleCSVReadTask(Iterator<String> lines) {\n+      this.lines = lines;\n+    }\n+\n+    JavaSimpleCSVReadTask(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataReader<Row> createReader() {\n+      assert path != null;\n+      try {\n+        if (Files.exists(Paths.get(path))) {\n+          return new JavaSimpleCSVReadTask(Files.readAllLines(Paths.get(path)).iterator());\n+        } else {\n+          return new JavaSimpleCSVReadTask(Collections.emptyIterator());\n+        }\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I see. I'd just declare `testXYZ throws Throwable` and not worry about what gets raised internally: saves on try/catch translation logic",
    "commit": "7eeb3b0bd15644d3facddefcdd2a218316573953",
    "createdAt": "2017-10-09T17:27:44Z",
    "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql.sources.v2;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.sources.v2.DataSourceV2;\n+import org.apache.spark.sql.sources.v2.DataSourceV2Options;\n+import org.apache.spark.sql.sources.v2.ReadSupport;\n+import org.apache.spark.sql.sources.v2.WriteSupport;\n+import org.apache.spark.sql.sources.v2.reader.DataReader;\n+import org.apache.spark.sql.sources.v2.reader.DataSourceV2Reader;\n+import org.apache.spark.sql.sources.v2.reader.ReadTask;\n+import org.apache.spark.sql.sources.v2.writer.*;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaSimpleWritableDataSource implements DataSourceV2, ReadSupport, WriteSupport {\n+  private StructType schema = new StructType().add(\"i\", \"long\").add(\"j\", \"long\");\n+\n+  class Reader implements DataSourceV2Reader {\n+    private String path;\n+\n+    Reader(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public StructType readSchema() {\n+      return schema;\n+    }\n+\n+    @Override\n+    public List<ReadTask<Row>> createReadTasks() {\n+      return java.util.Arrays.asList(new JavaSimpleCSVReadTask(path));\n+    }\n+  }\n+\n+  static class JavaSimpleCSVReadTask implements ReadTask<Row>, DataReader<Row> {\n+    private String path;\n+    private volatile Iterator<String> lines;\n+    private volatile String currentLine;\n+\n+    JavaSimpleCSVReadTask(Iterator<String> lines) {\n+      this.lines = lines;\n+    }\n+\n+    JavaSimpleCSVReadTask(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataReader<Row> createReader() {\n+      assert path != null;\n+      try {\n+        if (Files.exists(Paths.get(path))) {\n+          return new JavaSimpleCSVReadTask(Files.readAllLines(Paths.get(path)).iterator());\n+        } else {\n+          return new JavaSimpleCSVReadTask(Collections.emptyIterator());\n+        }\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);"
  }],
  "prId": 19269
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "if you are catching and wrapping exceptions, this should be included in the try/catch clause. Also handle `fileWriter==null`",
    "commit": "7eeb3b0bd15644d3facddefcdd2a218316573953",
    "createdAt": "2017-09-30T12:10:42Z",
    "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql.sources.v2;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.sources.v2.DataSourceV2;\n+import org.apache.spark.sql.sources.v2.DataSourceV2Options;\n+import org.apache.spark.sql.sources.v2.ReadSupport;\n+import org.apache.spark.sql.sources.v2.WriteSupport;\n+import org.apache.spark.sql.sources.v2.reader.DataReader;\n+import org.apache.spark.sql.sources.v2.reader.DataSourceV2Reader;\n+import org.apache.spark.sql.sources.v2.reader.ReadTask;\n+import org.apache.spark.sql.sources.v2.writer.*;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaSimpleWritableDataSource implements DataSourceV2, ReadSupport, WriteSupport {\n+  private StructType schema = new StructType().add(\"i\", \"long\").add(\"j\", \"long\");\n+\n+  class Reader implements DataSourceV2Reader {\n+    private String path;\n+\n+    Reader(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public StructType readSchema() {\n+      return schema;\n+    }\n+\n+    @Override\n+    public List<ReadTask<Row>> createReadTasks() {\n+      return java.util.Arrays.asList(new JavaSimpleCSVReadTask(path));\n+    }\n+  }\n+\n+  static class JavaSimpleCSVReadTask implements ReadTask<Row>, DataReader<Row> {\n+    private String path;\n+    private volatile Iterator<String> lines;\n+    private volatile String currentLine;\n+\n+    JavaSimpleCSVReadTask(Iterator<String> lines) {\n+      this.lines = lines;\n+    }\n+\n+    JavaSimpleCSVReadTask(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataReader<Row> createReader() {\n+      assert path != null;\n+      try {\n+        if (Files.exists(Paths.get(path))) {\n+          return new JavaSimpleCSVReadTask(Files.readAllLines(Paths.get(path)).iterator());\n+        } else {\n+          return new JavaSimpleCSVReadTask(Collections.emptyIterator());\n+        }\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public boolean next() {\n+      if (lines.hasNext()) {\n+        currentLine = lines.next();\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+\n+    @Override\n+    public Row get() {\n+      String[] values = currentLine.split(\",\");\n+      assert values.length == 2;\n+      long l1 = Long.valueOf(values[0]);\n+      long l2 = Long.valueOf(values[1]);\n+      return new GenericRow(new Object[] {l1, l2});\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+\n+    }\n+  }\n+\n+  @Override\n+  public DataSourceV2Reader createReader(DataSourceV2Options options) {\n+    return new Reader(options.get(\"path\").get());\n+  }\n+\n+\n+  class Writer implements DataSourceV2Writer {\n+    private String path;\n+\n+    Writer(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataWriteFactory<Row> createWriterFactory() {\n+      return new JavaSimpleCSVDataWriteFactory(path);\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+\n+    }\n+\n+    @Override\n+    public void abort() {\n+\n+    }\n+  }\n+\n+  class InternalRowWriter implements DataSourceV2Writer, SupportsWriteInternalRow {\n+    private String path;\n+\n+    InternalRowWriter(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataWriteFactory<InternalRow> createInternalRowWriterFactory() {\n+      return new JavaInternalRowCSVDataWriteFactory(path);\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+\n+    }\n+\n+    @Override\n+    public void abort() {\n+\n+    }\n+  }\n+\n+  static class JavaSimpleCSVDataWriteFactory implements DataWriteFactory<Row>, DataWriter<Row> {\n+    private String path;\n+    private volatile PrintWriter fileWriter;\n+\n+    JavaSimpleCSVDataWriteFactory(String path) {\n+      this.path = path;\n+    }\n+\n+    JavaSimpleCSVDataWriteFactory(String path, PrintWriter fileWriter) {\n+      this.path = path;\n+      this.fileWriter = fileWriter;\n+    }\n+\n+    @Override\n+    public DataWriter<Row> createWriter(int stageId, int partitionId, int attemptNumber) {\n+      try {\n+        return new JavaSimpleCSVDataWriteFactory(\n+          path, new PrintWriter(new FileWriter(path, true)));\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public void write(Row record) {\n+      fileWriter.println(record.getLong(0) + \",\" + record.getLong(1));\n+    }\n+\n+    @Override\n+    public WriterCommitMessage commit() {\n+      fileWriter.close();\n+      return null;\n+    }\n+\n+    @Override\n+    public void abort() {\n+      fileWriter.close();"
  }],
  "prId": 19269
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "I'd include the path here.",
    "commit": "7eeb3b0bd15644d3facddefcdd2a218316573953",
    "createdAt": "2017-09-30T12:13:18Z",
    "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql.sources.v2;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.sources.v2.DataSourceV2;\n+import org.apache.spark.sql.sources.v2.DataSourceV2Options;\n+import org.apache.spark.sql.sources.v2.ReadSupport;\n+import org.apache.spark.sql.sources.v2.WriteSupport;\n+import org.apache.spark.sql.sources.v2.reader.DataReader;\n+import org.apache.spark.sql.sources.v2.reader.DataSourceV2Reader;\n+import org.apache.spark.sql.sources.v2.reader.ReadTask;\n+import org.apache.spark.sql.sources.v2.writer.*;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaSimpleWritableDataSource implements DataSourceV2, ReadSupport, WriteSupport {\n+  private StructType schema = new StructType().add(\"i\", \"long\").add(\"j\", \"long\");\n+\n+  class Reader implements DataSourceV2Reader {\n+    private String path;\n+\n+    Reader(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public StructType readSchema() {\n+      return schema;\n+    }\n+\n+    @Override\n+    public List<ReadTask<Row>> createReadTasks() {\n+      return java.util.Arrays.asList(new JavaSimpleCSVReadTask(path));\n+    }\n+  }\n+\n+  static class JavaSimpleCSVReadTask implements ReadTask<Row>, DataReader<Row> {\n+    private String path;\n+    private volatile Iterator<String> lines;\n+    private volatile String currentLine;\n+\n+    JavaSimpleCSVReadTask(Iterator<String> lines) {\n+      this.lines = lines;\n+    }\n+\n+    JavaSimpleCSVReadTask(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataReader<Row> createReader() {\n+      assert path != null;\n+      try {\n+        if (Files.exists(Paths.get(path))) {\n+          return new JavaSimpleCSVReadTask(Files.readAllLines(Paths.get(path)).iterator());\n+        } else {\n+          return new JavaSimpleCSVReadTask(Collections.emptyIterator());\n+        }\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public boolean next() {\n+      if (lines.hasNext()) {\n+        currentLine = lines.next();\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+\n+    @Override\n+    public Row get() {\n+      String[] values = currentLine.split(\",\");\n+      assert values.length == 2;\n+      long l1 = Long.valueOf(values[0]);\n+      long l2 = Long.valueOf(values[1]);\n+      return new GenericRow(new Object[] {l1, l2});\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+\n+    }\n+  }\n+\n+  @Override\n+  public DataSourceV2Reader createReader(DataSourceV2Options options) {\n+    return new Reader(options.get(\"path\").get());\n+  }\n+\n+\n+  class Writer implements DataSourceV2Writer {\n+    private String path;\n+\n+    Writer(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataWriteFactory<Row> createWriterFactory() {\n+      return new JavaSimpleCSVDataWriteFactory(path);\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+\n+    }\n+\n+    @Override\n+    public void abort() {\n+\n+    }\n+  }\n+\n+  class InternalRowWriter implements DataSourceV2Writer, SupportsWriteInternalRow {\n+    private String path;\n+\n+    InternalRowWriter(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataWriteFactory<InternalRow> createInternalRowWriterFactory() {\n+      return new JavaInternalRowCSVDataWriteFactory(path);\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+\n+    }\n+\n+    @Override\n+    public void abort() {\n+\n+    }\n+  }\n+\n+  static class JavaSimpleCSVDataWriteFactory implements DataWriteFactory<Row>, DataWriter<Row> {\n+    private String path;\n+    private volatile PrintWriter fileWriter;\n+\n+    JavaSimpleCSVDataWriteFactory(String path) {\n+      this.path = path;\n+    }\n+\n+    JavaSimpleCSVDataWriteFactory(String path, PrintWriter fileWriter) {\n+      this.path = path;\n+      this.fileWriter = fileWriter;\n+    }\n+\n+    @Override\n+    public DataWriter<Row> createWriter(int stageId, int partitionId, int attemptNumber) {\n+      try {\n+        return new JavaSimpleCSVDataWriteFactory(\n+          path, new PrintWriter(new FileWriter(path, true)));\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public void write(Row record) {\n+      fileWriter.println(record.getLong(0) + \",\" + record.getLong(1));\n+    }\n+\n+    @Override\n+    public WriterCommitMessage commit() {\n+      fileWriter.close();\n+      return null;\n+    }\n+\n+    @Override\n+    public void abort() {\n+      fileWriter.close();\n+      try {\n+        Files.delete(Paths.get(path));\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+  }\n+\n+  static class JavaInternalRowCSVDataWriteFactory implements\n+    DataWriteFactory<InternalRow>, DataWriter<InternalRow> {\n+\n+    private String path;\n+    private volatile PrintWriter fileWriter;\n+\n+    JavaInternalRowCSVDataWriteFactory(String path) {\n+      this.path = path;\n+    }\n+\n+    JavaInternalRowCSVDataWriteFactory(String path, PrintWriter fileWriter) {\n+      this.path = path;\n+      this.fileWriter = fileWriter;\n+    }\n+\n+    @Override\n+    public DataWriter<InternalRow> createWriter(int stageId, int partitionId, int attemptNumber) {\n+      try {\n+        return new JavaInternalRowCSVDataWriteFactory(\n+          path, new PrintWriter(new FileWriter(path, true)));\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public void write(InternalRow record) {\n+      fileWriter.println(record.getLong(0) + \",\" + record.getLong(1));\n+    }\n+\n+    @Override\n+    public WriterCommitMessage commit() {\n+      fileWriter.close();\n+      return null;\n+    }\n+\n+    @Override\n+    public void abort() {\n+      fileWriter.close();\n+      try {\n+        Files.delete(Paths.get(path));\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Optional<DataSourceV2Writer> createWriter(\n+      StructType schema, SaveMode mode, DataSourceV2Options options) {\n+    assert DataType.equalsStructurally(schema.asNullable(), this.schema.asNullable());\n+\n+    String path = options.get(\"path\").get();\n+    boolean unsafe = options.get(\"internal\").isPresent();\n+\n+    if (Files.exists(Paths.get(path))) {\n+      if (mode == SaveMode.ErrorIfExists) {\n+        throw new RuntimeException(\"data already exists.\");"
  }],
  "prId": 19269
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "IF these test classes worked with a Hadoop FS, they'd not just be testable against the local file:// FS, you could bring up a miniDFS cluster and see how that worked. Why? Gives a broader test of how well the API worked against HDFS and the various failure modes, but also of how you pass configuration down.",
    "commit": "7eeb3b0bd15644d3facddefcdd2a218316573953",
    "createdAt": "2017-09-30T12:18:32Z",
    "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql.sources.v2;\n+\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeRow;\n+import org.apache.spark.sql.sources.v2.DataSourceV2;\n+import org.apache.spark.sql.sources.v2.DataSourceV2Options;\n+import org.apache.spark.sql.sources.v2.ReadSupport;\n+import org.apache.spark.sql.sources.v2.WriteSupport;\n+import org.apache.spark.sql.sources.v2.reader.DataReader;\n+import org.apache.spark.sql.sources.v2.reader.DataSourceV2Reader;\n+import org.apache.spark.sql.sources.v2.reader.ReadTask;\n+import org.apache.spark.sql.sources.v2.writer.*;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaSimpleWritableDataSource implements DataSourceV2, ReadSupport, WriteSupport {\n+  private StructType schema = new StructType().add(\"i\", \"long\").add(\"j\", \"long\");\n+\n+  class Reader implements DataSourceV2Reader {\n+    private String path;\n+\n+    Reader(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public StructType readSchema() {\n+      return schema;\n+    }\n+\n+    @Override\n+    public List<ReadTask<Row>> createReadTasks() {\n+      return java.util.Arrays.asList(new JavaSimpleCSVReadTask(path));\n+    }\n+  }\n+\n+  static class JavaSimpleCSVReadTask implements ReadTask<Row>, DataReader<Row> {\n+    private String path;\n+    private volatile Iterator<String> lines;\n+    private volatile String currentLine;\n+\n+    JavaSimpleCSVReadTask(Iterator<String> lines) {\n+      this.lines = lines;\n+    }\n+\n+    JavaSimpleCSVReadTask(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataReader<Row> createReader() {\n+      assert path != null;\n+      try {\n+        if (Files.exists(Paths.get(path))) {\n+          return new JavaSimpleCSVReadTask(Files.readAllLines(Paths.get(path)).iterator());\n+        } else {\n+          return new JavaSimpleCSVReadTask(Collections.emptyIterator());\n+        }\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public boolean next() {\n+      if (lines.hasNext()) {\n+        currentLine = lines.next();\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+\n+    @Override\n+    public Row get() {\n+      String[] values = currentLine.split(\",\");\n+      assert values.length == 2;\n+      long l1 = Long.valueOf(values[0]);\n+      long l2 = Long.valueOf(values[1]);\n+      return new GenericRow(new Object[] {l1, l2});\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+\n+    }\n+  }\n+\n+  @Override\n+  public DataSourceV2Reader createReader(DataSourceV2Options options) {\n+    return new Reader(options.get(\"path\").get());\n+  }\n+\n+\n+  class Writer implements DataSourceV2Writer {\n+    private String path;\n+\n+    Writer(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataWriteFactory<Row> createWriterFactory() {\n+      return new JavaSimpleCSVDataWriteFactory(path);\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+\n+    }\n+\n+    @Override\n+    public void abort() {\n+\n+    }\n+  }\n+\n+  class InternalRowWriter implements DataSourceV2Writer, SupportsWriteInternalRow {\n+    private String path;\n+\n+    InternalRowWriter(String path) {\n+      this.path = path;\n+    }\n+\n+    @Override\n+    public DataWriteFactory<InternalRow> createInternalRowWriterFactory() {\n+      return new JavaInternalRowCSVDataWriteFactory(path);\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+\n+    }\n+\n+    @Override\n+    public void abort() {\n+\n+    }\n+  }\n+\n+  static class JavaSimpleCSVDataWriteFactory implements DataWriteFactory<Row>, DataWriter<Row> {"
  }],
  "prId": 19269
}]