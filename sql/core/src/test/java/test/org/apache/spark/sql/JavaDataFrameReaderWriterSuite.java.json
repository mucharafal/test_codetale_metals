[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "SPARK-XXXX?\n",
    "commit": "2539a947d382d8d6c21c59fe8a9420a15aad9b9a",
    "createdAt": "2016-06-17T20:39:45Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+* Licensed to the Apache Software Foundation (ASF) under one or more\n+* contributor license agreements.  See the NOTICE file distributed with\n+* this work for additional information regarding copyright ownership.\n+* The ASF licenses this file to You under the Apache License, Version 2.0\n+* (the \"License\"); you may not use this file except in compliance with\n+* the License.  You may obtain a copy of the License at\n+*\n+*    http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package test.org.apache.spark.sql;\n+\n+import java.io.File;\n+import java.util.HashMap;\n+\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.Utils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+public class JavaDataFrameReaderWriterSuite {\n+  private SparkSession spark = new TestSparkSession();\n+  private StructType schema = new StructType().add(\"s\", \"string\");\n+  private transient String input;\n+  private transient String output;\n+\n+  @Before\n+  public void setUp() {\n+    input = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"input\").toString();\n+    File f = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"output\");\n+    f.delete();\n+    output = f.toString();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    spark.stop();\n+    spark = null;\n+  }\n+\n+  @Test\n+  public void testFormatAPI() {\n+    spark\n+        .read()\n+        .format(\"org.apache.spark.sql.test\")\n+        .load()\n+        .write()\n+        .format(\"org.apache.spark.sql.test\")\n+        .save();\n+  }\n+\n+  @Test\n+  public void testOptionsAPI() {\n+    HashMap<String, String> map = new HashMap<String, String>();\n+    map.put(\"e\", \"1\");\n+    spark\n+        .read()\n+        .option(\"a\", \"1\")\n+        .option(\"b\", 1)\n+        .option(\"c\", 1.0)\n+        .option(\"d\", true)\n+        .options(map)\n+        .text()\n+        .write()\n+        .option(\"a\", \"1\")\n+        .option(\"b\", 1)\n+        .option(\"c\", 1.0)\n+        .option(\"d\", true)\n+        .options(map)\n+        .format(\"org.apache.spark.sql.test\")\n+        .save();\n+  }\n+\n+  @Test\n+  public void testSaveModeAPI() {\n+    spark\n+        .range(10)\n+        .write()\n+        .format(\"org.apache.spark.sql.test\")\n+        .mode(SaveMode.ErrorIfExists)\n+        .save();\n+  }\n+\n+  @Test\n+  public void testLoadAPI() {\n+    spark.read().format(\"org.apache.spark.sql.test\").load();\n+    spark.read().format(\"org.apache.spark.sql.test\").load(input);\n+    spark.read().format(\"org.apache.spark.sql.test\").load(input, input, input);\n+    spark.read().format(\"org.apache.spark.sql.test\").load(new String[]{input, input});\n+  }\n+\n+  @Test\n+  public void testTextAPI() {\n+    spark.read().text();\n+    spark.read().text(input);\n+    spark.read().text(input, input, input);\n+    spark.read().text(new String[]{input, input})\n+        .write().text(output);\n+  }\n+\n+  @Test\n+  public void testTextFileAPI() {\n+    spark.read().textFile();     // Disabled because of SPARK-XXXXX"
  }],
  "prId": 13727
}]