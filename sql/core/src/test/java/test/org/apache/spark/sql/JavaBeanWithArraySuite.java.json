[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "You need to add the license headers.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-16T08:58:38Z",
    "diffHunk": "@@ -0,0 +1,195 @@\n+package test.org.apache.spark.sql;",
    "line": 18
  }],
  "prId": 22708
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "The import orders here are not compliant with Spark codebase. You can follow the style in other tests like `JavaApplySchemaSuite`.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-16T09:01:09Z",
    "diffHunk": "@@ -0,0 +1,195 @@\n+package test.org.apache.spark.sql;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.*;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.*;"
  }],
  "prId": 22708
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "I'm wondering if we can use the latest and neat approach in this PR. Something like the following? Then, we can remove `createSchema()` here.\r\n```scala\r\n- .schema(schema)\r\n+ .schema(\"id int, intervals array<struct<startTime: bigint, endTime: bigint>>, values array<int>\")\r\n```",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T00:28:37Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "@vofque Please note the `startTime` and `endTime`. It should be case-sensitive.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T00:32:45Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)"
  }, {
    "author": {
      "login": "vofque"
    },
    "body": "OK. Thank you for advice.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T07:03:11Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)"
  }],
  "prId": 22708
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "indentation? (Why newline?)",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T00:45:00Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;\n+\n+        public Record() { }\n+\n+        Record(int id, List<Interval> intervals, List<Integer> values) {\n+            this.id = id;\n+            this.intervals = intervals;\n+            this.values = values;\n+        }\n+\n+        public int getId() {\n+            return id;\n+        }\n+\n+        public void setId(int id) {\n+            this.id = id;\n+        }\n+\n+        public List<Interval> getIntervals() {\n+            return intervals;\n+        }\n+\n+        public void setIntervals(List<Interval> intervals) {\n+            this.intervals = intervals;\n+        }\n+\n+        public List<Integer> getValues() {\n+            return values;\n+        }\n+\n+        public void setValues(List<Integer> values) {\n+            this.values = values;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Record)) return false;\n+            Record other = (Record) obj;\n+            return"
  }],
  "prId": 22708
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "indentation?",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T00:45:27Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;\n+\n+        public Record() { }\n+\n+        Record(int id, List<Interval> intervals, List<Integer> values) {\n+            this.id = id;\n+            this.intervals = intervals;\n+            this.values = values;\n+        }\n+\n+        public int getId() {\n+            return id;\n+        }\n+\n+        public void setId(int id) {\n+            this.id = id;\n+        }\n+\n+        public List<Interval> getIntervals() {\n+            return intervals;\n+        }\n+\n+        public void setIntervals(List<Interval> intervals) {\n+            this.intervals = intervals;\n+        }\n+\n+        public List<Integer> getValues() {\n+            return values;\n+        }\n+\n+        public void setValues(List<Integer> values) {\n+            this.values = values;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Record)) return false;\n+            Record other = (Record) obj;\n+            return\n+                    (other.id == this.id) &&\n+                    Util.equals(other.intervals, this.intervals) &&\n+                    Util.equals(other.values, this.values);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"{ id: %d, intervals: %s }\", id, intervals );\n+        }\n+    }\n+\n+    public static class Interval {\n+\n+        private long startTime;\n+        private long endTime;\n+\n+        public Interval() { }\n+\n+        Interval(long startTime, long endTime) {\n+            this.startTime = startTime;\n+            this.endTime = endTime;\n+        }\n+\n+        public long getStartTime() {\n+            return startTime;\n+        }\n+\n+        public void setStartTime(long startTime) {\n+            this.startTime = startTime;\n+        }\n+\n+        public long getEndTime() {\n+            return endTime;\n+        }\n+\n+        public void setEndTime(long endTime) {\n+            this.endTime = endTime;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Interval)) return false;\n+            Interval other = (Interval) obj;\n+            return"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "ah yes, in Spark we should use 2 space indention for java code as well",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T02:54:29Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;\n+\n+        public Record() { }\n+\n+        Record(int id, List<Interval> intervals, List<Integer> values) {\n+            this.id = id;\n+            this.intervals = intervals;\n+            this.values = values;\n+        }\n+\n+        public int getId() {\n+            return id;\n+        }\n+\n+        public void setId(int id) {\n+            this.id = id;\n+        }\n+\n+        public List<Interval> getIntervals() {\n+            return intervals;\n+        }\n+\n+        public void setIntervals(List<Interval> intervals) {\n+            this.intervals = intervals;\n+        }\n+\n+        public List<Integer> getValues() {\n+            return values;\n+        }\n+\n+        public void setValues(List<Integer> values) {\n+            this.values = values;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Record)) return false;\n+            Record other = (Record) obj;\n+            return\n+                    (other.id == this.id) &&\n+                    Util.equals(other.intervals, this.intervals) &&\n+                    Util.equals(other.values, this.values);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"{ id: %d, intervals: %s }\", id, intervals );\n+        }\n+    }\n+\n+    public static class Interval {\n+\n+        private long startTime;\n+        private long endTime;\n+\n+        public Interval() { }\n+\n+        Interval(long startTime, long endTime) {\n+            this.startTime = startTime;\n+            this.endTime = endTime;\n+        }\n+\n+        public long getStartTime() {\n+            return startTime;\n+        }\n+\n+        public void setStartTime(long startTime) {\n+            this.startTime = startTime;\n+        }\n+\n+        public long getEndTime() {\n+            return endTime;\n+        }\n+\n+        public void setEndTime(long endTime) {\n+            this.endTime = endTime;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Interval)) return false;\n+            Interval other = (Interval) obj;\n+            return"
  }],
  "prId": 22708
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "It seems that `values` are missed here. Is it intentional?",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T00:49:02Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;\n+\n+        public Record() { }\n+\n+        Record(int id, List<Interval> intervals, List<Integer> values) {\n+            this.id = id;\n+            this.intervals = intervals;\n+            this.values = values;\n+        }\n+\n+        public int getId() {\n+            return id;\n+        }\n+\n+        public void setId(int id) {\n+            this.id = id;\n+        }\n+\n+        public List<Interval> getIntervals() {\n+            return intervals;\n+        }\n+\n+        public void setIntervals(List<Interval> intervals) {\n+            this.intervals = intervals;\n+        }\n+\n+        public List<Integer> getValues() {\n+            return values;\n+        }\n+\n+        public void setValues(List<Integer> values) {\n+            this.values = values;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Record)) return false;\n+            Record other = (Record) obj;\n+            return\n+                    (other.id == this.id) &&\n+                    Util.equals(other.intervals, this.intervals) &&\n+                    Util.equals(other.values, this.values);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"{ id: %d, intervals: %s }\", id, intervals );"
  }],
  "prId": 22708
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "If we remove `createSchema`, we can remove Line 35 ~ 40 , too.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T00:50:36Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;"
  }],
  "prId": 22708
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Will this list of int affect the test? If no, maybe we can get rid of it to simplify the test.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T03:08:51Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;"
  }, {
    "author": {
      "login": "vofque"
    },
    "body": "The intention was to test a non struct case too. But I think, it's really not critical and we can get rid of it.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T07:39:32Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;"
  }],
  "prId": 22708
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "This is duplicate to your another PR. Maybe we can consider put two tests in one Java file so we don't need to have two `Interval`.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T03:14:03Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;\n+\n+        public Record() { }\n+\n+        Record(int id, List<Interval> intervals, List<Integer> values) {\n+            this.id = id;\n+            this.intervals = intervals;\n+            this.values = values;\n+        }\n+\n+        public int getId() {\n+            return id;\n+        }\n+\n+        public void setId(int id) {\n+            this.id = id;\n+        }\n+\n+        public List<Interval> getIntervals() {\n+            return intervals;\n+        }\n+\n+        public void setIntervals(List<Interval> intervals) {\n+            this.intervals = intervals;\n+        }\n+\n+        public List<Integer> getValues() {\n+            return values;\n+        }\n+\n+        public void setValues(List<Integer> values) {\n+            this.values = values;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Record)) return false;\n+            Record other = (Record) obj;\n+            return\n+                    (other.id == this.id) &&\n+                    Util.equals(other.intervals, this.intervals) &&\n+                    Util.equals(other.values, this.values);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"{ id: %d, intervals: %s }\", id, intervals );\n+        }\n+    }\n+\n+    public static class Interval {"
  }, {
    "author": {
      "login": "vofque"
    },
    "body": "But, I guess, to do that we need to have these both changes in one PR? \r\nOr correct another PR to add the second test in the same Java file later.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T07:50:33Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;\n+\n+        public Record() { }\n+\n+        Record(int id, List<Interval> intervals, List<Integer> values) {\n+            this.id = id;\n+            this.intervals = intervals;\n+            this.values = values;\n+        }\n+\n+        public int getId() {\n+            return id;\n+        }\n+\n+        public void setId(int id) {\n+            this.id = id;\n+        }\n+\n+        public List<Interval> getIntervals() {\n+            return intervals;\n+        }\n+\n+        public void setIntervals(List<Interval> intervals) {\n+            this.intervals = intervals;\n+        }\n+\n+        public List<Integer> getValues() {\n+            return values;\n+        }\n+\n+        public void setValues(List<Integer> values) {\n+            this.values = values;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Record)) return false;\n+            Record other = (Record) obj;\n+            return\n+                    (other.id == this.id) &&\n+                    Util.equals(other.intervals, this.intervals) &&\n+                    Util.equals(other.values, this.values);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"{ id: %d, intervals: %s }\", id, intervals );\n+        }\n+    }\n+\n+    public static class Interval {"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "We can have this merged first and rebase another PR to have another test in the same file too.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T08:10:14Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;\n+\n+        public Record() { }\n+\n+        Record(int id, List<Interval> intervals, List<Integer> values) {\n+            this.id = id;\n+            this.intervals = intervals;\n+            this.values = values;\n+        }\n+\n+        public int getId() {\n+            return id;\n+        }\n+\n+        public void setId(int id) {\n+            this.id = id;\n+        }\n+\n+        public List<Interval> getIntervals() {\n+            return intervals;\n+        }\n+\n+        public void setIntervals(List<Interval> intervals) {\n+            this.intervals = intervals;\n+        }\n+\n+        public List<Integer> getValues() {\n+            return values;\n+        }\n+\n+        public void setValues(List<Integer> values) {\n+            this.values = values;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Record)) return false;\n+            Record other = (Record) obj;\n+            return\n+                    (other.id == this.id) &&\n+                    Util.equals(other.intervals, this.intervals) &&\n+                    Util.equals(other.values, this.values);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"{ id: %d, intervals: %s }\", id, intervals );\n+        }\n+    }\n+\n+    public static class Interval {"
  }, {
    "author": {
      "login": "vofque"
    },
    "body": "OK, sure.",
    "commit": "46e942d96861670515330ae95d5067d287d86ba0",
    "createdAt": "2018-10-17T08:28:49Z",
    "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.test.TestSparkSession;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class JavaBeanWithArraySuite {\n+\n+    private static final List<Record> RECORDS = new ArrayList<>();\n+\n+    static {\n+        RECORDS.add(new Record(1,\n+                Arrays.asList(new Interval(111, 211), new Interval(121, 221)),\n+                Arrays.asList(11, 21, 31, 41)\n+        ));\n+        RECORDS.add(new Record(2,\n+                Arrays.asList(new Interval(112, 212), new Interval(122, 222)),\n+                Arrays.asList(12, 22, 32, 42)\n+        ));\n+        RECORDS.add(new Record(3,\n+                Arrays.asList(new Interval(113, 213), new Interval(123, 223)),\n+                Arrays.asList(13, 23, 33, 43)\n+        ));\n+    }\n+\n+    private TestSparkSession spark;\n+\n+    @Before\n+    public void setUp() {\n+        spark = new TestSparkSession();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        spark.stop();\n+        spark = null;\n+    }\n+\n+    @Test\n+    public void testBeanWithArrayFieldsDeserialization() {\n+\n+        StructType schema = createSchema();\n+        Encoder<Record> encoder = Encoders.bean(Record.class);\n+\n+        Dataset<Record> dataset = spark\n+                .read()\n+                .format(\"json\")\n+                .schema(schema)\n+                .load(\"src/test/resources/test-data/with-array-fields\")\n+                .as(encoder);\n+\n+        List<Record> records = dataset.collectAsList();\n+\n+        Assert.assertTrue(Util.equals(records, RECORDS));\n+    }\n+\n+    private static StructType createSchema() {\n+        StructField[] intervalFields = {\n+                new StructField(\"startTime\", DataTypes.LongType, true, Metadata.empty()),\n+                new StructField(\"endTime\", DataTypes.LongType, true, Metadata.empty())\n+        };\n+        DataType intervalType = new StructType(intervalFields);\n+\n+        DataType intervalsType = new ArrayType(intervalType, true);\n+\n+        DataType valuesType = new ArrayType(DataTypes.IntegerType, true);\n+\n+        StructField[] fields = {\n+                new StructField(\"id\", DataTypes.IntegerType, true, Metadata.empty()),\n+                new StructField(\"intervals\", intervalsType, true, Metadata.empty()),\n+                new StructField(\"values\", valuesType, true, Metadata.empty())\n+        };\n+        return new StructType(fields);\n+    }\n+\n+    public static class Record {\n+\n+        private int id;\n+        private List<Interval> intervals;\n+        private List<Integer> values;\n+\n+        public Record() { }\n+\n+        Record(int id, List<Interval> intervals, List<Integer> values) {\n+            this.id = id;\n+            this.intervals = intervals;\n+            this.values = values;\n+        }\n+\n+        public int getId() {\n+            return id;\n+        }\n+\n+        public void setId(int id) {\n+            this.id = id;\n+        }\n+\n+        public List<Interval> getIntervals() {\n+            return intervals;\n+        }\n+\n+        public void setIntervals(List<Interval> intervals) {\n+            this.intervals = intervals;\n+        }\n+\n+        public List<Integer> getValues() {\n+            return values;\n+        }\n+\n+        public void setValues(List<Integer> values) {\n+            this.values = values;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (!(obj instanceof Record)) return false;\n+            Record other = (Record) obj;\n+            return\n+                    (other.id == this.id) &&\n+                    Util.equals(other.intervals, this.intervals) &&\n+                    Util.equals(other.values, this.values);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"{ id: %d, intervals: %s }\", id, intervals );\n+        }\n+    }\n+\n+    public static class Interval {"
  }],
  "prId": 22708
}]