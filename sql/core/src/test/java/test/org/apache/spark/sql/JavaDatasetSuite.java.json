[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Java/Scala imports come first. In general you don't want to expand \\* imports. Just leave them as-is excepting where you need to add imports.\n",
    "commit": "cef9f61cc805504de9a78f335b3de545d69d89d9",
    "createdAt": "2016-04-07T08:03:32Z",
    "diffHunk": "@@ -17,33 +17,60 @@\n \n package test.org.apache.spark.sql;\n \n-import java.io.Serializable;\n-import java.math.BigDecimal;\n-import java.sql.Date;\n-import java.sql.Timestamp;\n-import java.util.*;\n-\n import com.google.common.base.Objects;\n-import org.junit.rules.ExpectedException;\n-import scala.Tuple2;\n-import scala.Tuple3;\n-import scala.Tuple4;\n-import scala.Tuple5;\n-\n-import org.junit.*;\n-\n import org.apache.spark.Accumulator;\n import org.apache.spark.SparkContext;\n-import org.apache.spark.api.java.function.*;\n import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.sql.*;\n-import org.apache.spark.sql.test.TestSQLContext;\n+import org.apache.spark.api.java.function.CoGroupFunction;\n+import org.apache.spark.api.java.function.FilterFunction;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.ForeachFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapGroupsFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.KeyValueGroupedDataset;\n+import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.encoders.OuterScopes;\n import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.test.TestSQLContext;\n import org.apache.spark.sql.types.StructType;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import scala.Tuple2;\n+import scala.Tuple3;\n+import scala.Tuple4;\n+import scala.Tuple5;\n \n-import static org.apache.spark.sql.functions.*;\n-import static org.apache.spark.sql.types.DataTypes.*;\n+import java.io.Serializable;"
  }],
  "prId": 12184
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: Just `List` but write a generic type\n",
    "commit": "cef9f61cc805504de9a78f335b3de545d69d89d9",
    "createdAt": "2016-04-07T08:03:51Z",
    "diffHunk": "@@ -454,6 +481,17 @@ public void testJavaEncoder() {\n     Assert.assertEquals(data, ds.collectAsList());\n   }\n \n+  @Test\n+  public void testRandomSplit() {\n+    List<String> data = Arrays.asList(\"hello\", \"world\", \"from\", \"spark\");\n+    Dataset<String> ds = context.createDataset(data, Encoders.STRING());\n+    double[] arraySplit = {1, 2, 3};\n+\n+    java.util.List randomSplit =  ds.randomSplitAsList(arraySplit, 1);"
  }],
  "prId": 12184
}]