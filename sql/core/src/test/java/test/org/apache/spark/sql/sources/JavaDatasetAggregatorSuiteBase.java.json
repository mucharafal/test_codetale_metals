[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Although you can remove \"throws Exception\" here while you're at it, it is not important. LGTM as is\n",
    "commit": "1cd9a260b26781682d160657c2f776b76d3de971",
    "createdAt": "2016-04-06T19:43:24Z",
    "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package test.org.apache.spark.sql.sources;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.KeyValueGroupedDataset;\n+import org.apache.spark.sql.test.TestSQLContext;\n+\n+/**\n+ * Common test base shared across this and Java8DatasetAggregatorSuite.\n+ */\n+public class JavaDatasetAggregatorSuiteBase implements Serializable {\n+  protected transient JavaSparkContext jsc;\n+  protected transient TestSQLContext context;\n+\n+  @Before\n+  public void setUp() {\n+    // Trigger static initializer of TestData\n+    SparkContext sc = new SparkContext(\"local[*]\", \"testing\");\n+    jsc = new JavaSparkContext(sc);\n+    context = new TestSQLContext(sc);\n+    context.loadTestData();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    context.sparkContext().stop();\n+    context = null;\n+    jsc = null;\n+  }\n+\n+  protected <T1, T2> Tuple2<T1, T2> tuple2(T1 t1, T2 t2) {\n+    return new Tuple2<>(t1, t2);\n+  }\n+\n+  protected KeyValueGroupedDataset<String, Tuple2<String, Integer>> generateGroupedDataset() {\n+    Encoder<Tuple2<String, Integer>> encoder = Encoders.tuple(Encoders.STRING(), Encoders.INT());\n+    List<Tuple2<String, Integer>> data =\n+      Arrays.asList(tuple2(\"a\", 1), tuple2(\"a\", 2), tuple2(\"b\", 3));\n+    Dataset<Tuple2<String, Integer>> ds = context.createDataset(data, encoder);\n+\n+    return ds.groupByKey(\n+      new MapFunction<Tuple2<String, Integer>, String>() {\n+        @Override\n+        public String call(Tuple2<String, Integer> value) throws Exception {",
    "line": 74
  }],
  "prId": 12212
}]