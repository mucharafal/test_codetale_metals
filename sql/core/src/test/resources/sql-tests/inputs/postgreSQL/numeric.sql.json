[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "So, this is converting `CASTING DURING INSERTING` into `CASTING WITH CAST syntax`.",
    "commit": "7c06f0d8cb6cdb0b9f11f9865f364e870c9740b7",
    "createdAt": "2019-11-13T19:05:08Z",
    "diffHunk": "@@ -657,16 +674,18 @@ SELECT AVG(val) FROM num_data;\n \n -- Check for appropriate rounding and overflow\n CREATE TABLE fract_only (id int, val decimal(4,4)) USING parquet;\n-INSERT INTO fract_only VALUES (1, '0.0');\n-INSERT INTO fract_only VALUES (2, '0.1');\n+-- PostgreSQL implicitly casts string literals to data with decimal types, but\n+-- Spark does not support that kind of implicit casts.\n+INSERT INTO fract_only VALUES (1, cast('0.0' as decimal(4,4)));\n+INSERT INTO fract_only VALUES (2, cast('0.1' as decimal(4,4)));\n -- [SPARK-27923] PostgreSQL throws an exception but Spark SQL is NULL\n--- INSERT INTO fract_only VALUES (3, '1.0');\t-- should fail\n-INSERT INTO fract_only VALUES (4, '-0.9999');\n-INSERT INTO fract_only VALUES (5, '0.99994');\n+-- INSERT INTO fract_only VALUES (3, '1.0' as decimal(4,4));\t-- should fail\n+INSERT INTO fract_only VALUES (4, cast('-0.9999' as decimal(4,4)));\n+INSERT INTO fract_only VALUES (5, cast('0.99994' as decimal(4,4)));"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "Ur... my fault.. I'll update later.",
    "commit": "7c06f0d8cb6cdb0b9f11f9865f364e870c9740b7",
    "createdAt": "2019-11-14T01:39:00Z",
    "diffHunk": "@@ -657,16 +674,18 @@ SELECT AVG(val) FROM num_data;\n \n -- Check for appropriate rounding and overflow\n CREATE TABLE fract_only (id int, val decimal(4,4)) USING parquet;\n-INSERT INTO fract_only VALUES (1, '0.0');\n-INSERT INTO fract_only VALUES (2, '0.1');\n+-- PostgreSQL implicitly casts string literals to data with decimal types, but\n+-- Spark does not support that kind of implicit casts.\n+INSERT INTO fract_only VALUES (1, cast('0.0' as decimal(4,4)));\n+INSERT INTO fract_only VALUES (2, cast('0.1' as decimal(4,4)));\n -- [SPARK-27923] PostgreSQL throws an exception but Spark SQL is NULL\n--- INSERT INTO fract_only VALUES (3, '1.0');\t-- should fail\n-INSERT INTO fract_only VALUES (4, '-0.9999');\n-INSERT INTO fract_only VALUES (5, '0.99994');\n+-- INSERT INTO fract_only VALUES (3, '1.0' as decimal(4,4));\t-- should fail\n+INSERT INTO fract_only VALUES (4, cast('-0.9999' as decimal(4,4)));\n+INSERT INTO fract_only VALUES (5, cast('0.99994' as decimal(4,4)));"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Oh, it was just a comment to clarify. I think it's okay for now `AS-IS` since we don't support the original query in any way.",
    "commit": "7c06f0d8cb6cdb0b9f11f9865f364e870c9740b7",
    "createdAt": "2019-11-14T02:04:44Z",
    "diffHunk": "@@ -657,16 +674,18 @@ SELECT AVG(val) FROM num_data;\n \n -- Check for appropriate rounding and overflow\n CREATE TABLE fract_only (id int, val decimal(4,4)) USING parquet;\n-INSERT INTO fract_only VALUES (1, '0.0');\n-INSERT INTO fract_only VALUES (2, '0.1');\n+-- PostgreSQL implicitly casts string literals to data with decimal types, but\n+-- Spark does not support that kind of implicit casts.\n+INSERT INTO fract_only VALUES (1, cast('0.0' as decimal(4,4)));\n+INSERT INTO fract_only VALUES (2, cast('0.1' as decimal(4,4)));\n -- [SPARK-27923] PostgreSQL throws an exception but Spark SQL is NULL\n--- INSERT INTO fract_only VALUES (3, '1.0');\t-- should fail\n-INSERT INTO fract_only VALUES (4, '-0.9999');\n-INSERT INTO fract_only VALUES (5, '0.99994');\n+-- INSERT INTO fract_only VALUES (3, '1.0' as decimal(4,4));\t-- should fail\n+INSERT INTO fract_only VALUES (4, cast('-0.9999' as decimal(4,4)));\n+INSERT INTO fract_only VALUES (5, cast('0.99994' as decimal(4,4)));"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "But, the original queries throw unsupported exceptions now, so most of queries in this file become invalid in Spark. In the last commit, I changed these string literals into floating-point literals for depending on INSERT assignment casts. Which do you think is better, `as-is` or `the last commit`?",
    "commit": "7c06f0d8cb6cdb0b9f11f9865f364e870c9740b7",
    "createdAt": "2019-11-14T04:25:53Z",
    "diffHunk": "@@ -657,16 +674,18 @@ SELECT AVG(val) FROM num_data;\n \n -- Check for appropriate rounding and overflow\n CREATE TABLE fract_only (id int, val decimal(4,4)) USING parquet;\n-INSERT INTO fract_only VALUES (1, '0.0');\n-INSERT INTO fract_only VALUES (2, '0.1');\n+-- PostgreSQL implicitly casts string literals to data with decimal types, but\n+-- Spark does not support that kind of implicit casts.\n+INSERT INTO fract_only VALUES (1, cast('0.0' as decimal(4,4)));\n+INSERT INTO fract_only VALUES (2, cast('0.1' as decimal(4,4)));\n -- [SPARK-27923] PostgreSQL throws an exception but Spark SQL is NULL\n--- INSERT INTO fract_only VALUES (3, '1.0');\t-- should fail\n-INSERT INTO fract_only VALUES (4, '-0.9999');\n-INSERT INTO fract_only VALUES (5, '0.99994');\n+-- INSERT INTO fract_only VALUES (3, '1.0' as decimal(4,4));\t-- should fail\n+INSERT INTO fract_only VALUES (4, cast('-0.9999' as decimal(4,4)));\n+INSERT INTO fract_only VALUES (5, cast('0.99994' as decimal(4,4)));"
  }],
  "prId": 26492
}]