[{
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "Thanks for the report! Can you add the query below as an example in the jira? I think that's a good reproducer.",
    "commit": "3dfa624c8fbc8d3efbf0cfd18dda690585b2f572",
    "createdAt": "2019-10-29T23:37:22Z",
    "diffHunk": "@@ -0,0 +1,399 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L913-L1278\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test user-defined window function with named args and default args\n+-- CREATE FUNCTION nth_value_def(val anyelement, n integer = 1) RETURNS anyelement\n+--   LANGUAGE internal WINDOW IMMUTABLE STRICT AS 'window_nth_value';\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- SELECT nth_value_def(n := 2, val := ten) OVER (PARTITION BY four), ten, four\n+--   FROM (SELECT * FROM tenk1 WHERE unique2 < 10 ORDER BY four, ten) s;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- SELECT nth_value_def(ten) OVER (PARTITION BY four), ten, four\n+--   FROM (SELECT * FROM tenk1 WHERE unique2 < 10 ORDER BY four, ten) s;\n+\n+--\n+-- Test the basic moving-aggregate machinery\n+--\n+\n+-- create aggregates that record the series of transform calls (these are\n+-- intentionally not true inverses)\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_sfunc_nonstrict(text, anyelement) RETURNS text AS\n+-- $$ SELECT COALESCE($1, '') || '*' || quote_nullable($2) $$\n+-- LANGUAGE SQL IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_msfunc_nonstrict(text, anyelement) RETURNS text AS\n+-- $$ SELECT COALESCE($1, '') || '+' || quote_nullable($2) $$\n+-- LANGUAGE SQL IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_minvfunc_nonstrict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '-' || quote_nullable($2) $$\n+-- LANGUAGE SQL IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_nonstrict (anyelement)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_nonstrict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_nonstrict,\n+-- \tminvfunc = logging_minvfunc_nonstrict\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_nonstrict_initcond (anyelement)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_nonstrict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_nonstrict,\n+-- \tminvfunc = logging_minvfunc_nonstrict,\n+-- \tinitcond = 'I',\n+-- \tminitcond = 'MI'\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_sfunc_strict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '*' || quote_nullable($2) $$\n+-- LANGUAGE SQL STRICT IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_msfunc_strict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '+' || quote_nullable($2) $$\n+-- LANGUAGE SQL STRICT IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_minvfunc_strict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '-' || quote_nullable($2) $$\n+-- LANGUAGE SQL STRICT IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_strict (text)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_strict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_strict,\n+-- \tminvfunc = logging_minvfunc_strict\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_strict_initcond (anyelement)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_strict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_strict,\n+-- \tminvfunc = logging_minvfunc_strict,\n+-- \tinitcond = 'I',\n+-- \tminitcond = 'MI'\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test strict and non-strict cases\n+-- SELECT\n+-- \tp::text || ',' || i::text || ':' || COALESCE(v::text, 'NULL') AS row,\n+-- \tlogging_agg_nonstrict(v) over wnd as nstrict,\n+-- \tlogging_agg_nonstrict_initcond(v) over wnd as nstrict_init,\n+-- \tlogging_agg_strict(v::text) over wnd as strict,\n+-- \tlogging_agg_strict_initcond(v) over wnd as strict_init\n+-- FROM (VALUES\n+-- \t(1, 1, NULL),\n+-- \t(1, 2, 'a'),\n+-- \t(1, 3, 'b'),\n+-- \t(1, 4, NULL),\n+-- \t(1, 5, NULL),\n+-- \t(1, 6, 'c'),\n+-- \t(2, 1, NULL),\n+-- \t(2, 2, 'x'),\n+-- \t(3, 1, 'z')\n+-- ) AS t(p, i, v)\n+-- WINDOW wnd AS (PARTITION BY P ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY p, i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- and again, but with filter\n+-- SELECT\n+-- \tp::text || ',' || i::text || ':' ||\n+-- \t\tCASE WHEN f THEN COALESCE(v::text, 'NULL') ELSE '-' END as row,\n+-- \tlogging_agg_nonstrict(v) filter(where f) over wnd as nstrict_filt,\n+-- \tlogging_agg_nonstrict_initcond(v) filter(where f) over wnd as nstrict_init_filt,\n+-- \tlogging_agg_strict(v::text) filter(where f) over wnd as strict_filt,\n+-- \tlogging_agg_strict_initcond(v) filter(where f) over wnd as strict_init_filt\n+-- FROM (VALUES\n+-- \t(1, 1, true,  NULL),\n+-- \t(1, 2, false, 'a'),\n+-- \t(1, 3, true,  'b'),\n+-- \t(1, 4, false, NULL),\n+-- \t(1, 5, false, NULL),\n+-- \t(1, 6, false, 'c'),\n+-- \t(2, 1, false, NULL),\n+-- \t(2, 2, true,  'x'),\n+-- \t(3, 1, true,  'z')\n+-- ) AS t(p, i, f, v)\n+-- WINDOW wnd AS (PARTITION BY p ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY p, i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test that volatile arguments disable moving-aggregate mode\n+-- SELECT\n+-- \ti::text || ':' || COALESCE(v::text, 'NULL') as row,\n+-- \tlogging_agg_strict(v::text)\n+-- \t\tover wnd as inverse,\n+-- \tlogging_agg_strict(v::text || CASE WHEN random() < 0 then '?' ELSE '' END)\n+-- \t\tover wnd as noinverse\n+-- FROM (VALUES\n+-- \t(1, 'a'),\n+-- \t(2, 'b'),\n+-- \t(3, 'c')\n+-- ) AS t(i, v)\n+-- WINDOW wnd AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- SELECT\n+-- \ti::text || ':' || COALESCE(v::text, 'NULL') as row,\n+-- \tlogging_agg_strict(v::text) filter(where true)\n+-- \t\tover wnd as inverse,\n+-- \tlogging_agg_strict(v::text) filter(where random() >= 0)\n+-- \t\tover wnd as noinverse\n+-- FROM (VALUES\n+-- \t(1, 'a'),\n+-- \t(2, 'b'),\n+-- \t(3, 'c')\n+-- ) AS t(i, v)\n+-- WINDOW wnd AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test that non-overlapping windows don't use inverse transitions\n+-- SELECT\n+-- \tlogging_agg_strict(v::text) OVER wnd\n+-- FROM (VALUES\n+-- \t(1, 'a'),\n+-- \t(2, 'b'),\n+-- \t(3, 'c')\n+-- ) AS t(i, v)\n+-- WINDOW wnd AS (ORDER BY i ROWS BETWEEN CURRENT ROW AND CURRENT ROW)\n+-- ORDER BY i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test that returning NULL from the inverse transition functions\n+-- restarts the aggregation from scratch. The second aggregate is supposed\n+-- to test cases where only some aggregates restart, the third one checks\n+-- that one aggregate restarting doesn't cause others to restart.\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION sum_int_randrestart_minvfunc(int4, int4) RETURNS int4 AS\n+-- $$ SELECT CASE WHEN random() < 0.2 THEN NULL ELSE $1 - $2 END $$\n+-- LANGUAGE SQL STRICT;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE sum_int_randomrestart (int4)\n+-- (\n+-- \tstype = int4,\n+-- \tsfunc = int4pl,\n+-- \tmstype = int4,\n+-- \tmsfunc = int4pl,\n+-- \tminvfunc = sum_int_randrestart_minvfunc\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- WITH\n+-- vs AS (\n+-- \tSELECT i, (random() * 100)::int4 AS v\n+-- \tFROM generate_series(1, 100) AS i\n+-- ),\n+-- sum_following AS (\n+-- \tSELECT i, SUM(v) OVER\n+-- \t\t(ORDER BY i DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS s\n+-- \tFROM vs\n+-- )\n+-- SELECT DISTINCT\n+-- \tsum_following.s = sum_int_randomrestart(v) OVER fwd AS eq1,\n+-- \t-sum_following.s = sum_int_randomrestart(-v) OVER fwd AS eq2,\n+-- \t100*3+(vs.i-1)*3 = length(logging_agg_nonstrict(''::text) OVER fwd) AS eq3\n+-- FROM vs\n+-- JOIN sum_following ON sum_following.i = vs.i\n+-- WINDOW fwd AS (\n+-- \tORDER BY vs.i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING\n+-- );\n+\n+--\n+-- Test various built-in aggregates that have moving-aggregate support\n+--\n+\n+-- test inverse transition functions handle NULLs properly\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1.5),(2,2.5),(3,NULL),(4,NULL)) t(i,v);\n+\n+-- [SPARK-28602] Spark does not recognize 'interval' type as 'numeric'\n+-- SELECT i,AVG(v::interval) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+--   FROM (VALUES(1,'1 sec'),(2,'2 sec'),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+-- The cast syntax is present in PgSQL for legacy reasons and Spark will not recognize a money field\n+-- SELECT i,SUM(v::money) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+--   FROM (VALUES(1,'1.10'),(2,'2.20'),(3,NULL),(4,NULL)) t(i,v);\n+\n+-- [SPARK-28602] Spark does not recognize 'interval' type as 'numeric'\n+-- SELECT i,SUM(cast(v as interval)) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+--   FROM (VALUES(1,'1 sec'),(2,'2 sec'),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1.1),(2,2.2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT SUM(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1.01),(2,2),(3,3)) v(i,n);\n+\n+SELECT i,COUNT(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,COUNT(*) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+-- For the following queries Spark result differs from PgSQL:\n+-- Spark handles division by zero as 'NaN' instead of 'NULL', which is the PgSQL behaviour\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+-- test that inverse transition functions work with various frame options\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND CURRENT ROW)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,3),(4,4)) t(i,v);\n+\n+-- [SPARK-29638] Spark handles 'NaN' as 0 in sums",
    "line": 364
  }, {
    "author": {
      "login": "DylanGuedes"
    },
    "body": "Sure. I added at the JIRA page.",
    "commit": "3dfa624c8fbc8d3efbf0cfd18dda690585b2f572",
    "createdAt": "2019-10-30T00:28:05Z",
    "diffHunk": "@@ -0,0 +1,399 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L913-L1278\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test user-defined window function with named args and default args\n+-- CREATE FUNCTION nth_value_def(val anyelement, n integer = 1) RETURNS anyelement\n+--   LANGUAGE internal WINDOW IMMUTABLE STRICT AS 'window_nth_value';\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- SELECT nth_value_def(n := 2, val := ten) OVER (PARTITION BY four), ten, four\n+--   FROM (SELECT * FROM tenk1 WHERE unique2 < 10 ORDER BY four, ten) s;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- SELECT nth_value_def(ten) OVER (PARTITION BY four), ten, four\n+--   FROM (SELECT * FROM tenk1 WHERE unique2 < 10 ORDER BY four, ten) s;\n+\n+--\n+-- Test the basic moving-aggregate machinery\n+--\n+\n+-- create aggregates that record the series of transform calls (these are\n+-- intentionally not true inverses)\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_sfunc_nonstrict(text, anyelement) RETURNS text AS\n+-- $$ SELECT COALESCE($1, '') || '*' || quote_nullable($2) $$\n+-- LANGUAGE SQL IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_msfunc_nonstrict(text, anyelement) RETURNS text AS\n+-- $$ SELECT COALESCE($1, '') || '+' || quote_nullable($2) $$\n+-- LANGUAGE SQL IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_minvfunc_nonstrict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '-' || quote_nullable($2) $$\n+-- LANGUAGE SQL IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_nonstrict (anyelement)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_nonstrict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_nonstrict,\n+-- \tminvfunc = logging_minvfunc_nonstrict\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_nonstrict_initcond (anyelement)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_nonstrict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_nonstrict,\n+-- \tminvfunc = logging_minvfunc_nonstrict,\n+-- \tinitcond = 'I',\n+-- \tminitcond = 'MI'\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_sfunc_strict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '*' || quote_nullable($2) $$\n+-- LANGUAGE SQL STRICT IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_msfunc_strict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '+' || quote_nullable($2) $$\n+-- LANGUAGE SQL STRICT IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_minvfunc_strict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '-' || quote_nullable($2) $$\n+-- LANGUAGE SQL STRICT IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_strict (text)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_strict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_strict,\n+-- \tminvfunc = logging_minvfunc_strict\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_strict_initcond (anyelement)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_strict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_strict,\n+-- \tminvfunc = logging_minvfunc_strict,\n+-- \tinitcond = 'I',\n+-- \tminitcond = 'MI'\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test strict and non-strict cases\n+-- SELECT\n+-- \tp::text || ',' || i::text || ':' || COALESCE(v::text, 'NULL') AS row,\n+-- \tlogging_agg_nonstrict(v) over wnd as nstrict,\n+-- \tlogging_agg_nonstrict_initcond(v) over wnd as nstrict_init,\n+-- \tlogging_agg_strict(v::text) over wnd as strict,\n+-- \tlogging_agg_strict_initcond(v) over wnd as strict_init\n+-- FROM (VALUES\n+-- \t(1, 1, NULL),\n+-- \t(1, 2, 'a'),\n+-- \t(1, 3, 'b'),\n+-- \t(1, 4, NULL),\n+-- \t(1, 5, NULL),\n+-- \t(1, 6, 'c'),\n+-- \t(2, 1, NULL),\n+-- \t(2, 2, 'x'),\n+-- \t(3, 1, 'z')\n+-- ) AS t(p, i, v)\n+-- WINDOW wnd AS (PARTITION BY P ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY p, i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- and again, but with filter\n+-- SELECT\n+-- \tp::text || ',' || i::text || ':' ||\n+-- \t\tCASE WHEN f THEN COALESCE(v::text, 'NULL') ELSE '-' END as row,\n+-- \tlogging_agg_nonstrict(v) filter(where f) over wnd as nstrict_filt,\n+-- \tlogging_agg_nonstrict_initcond(v) filter(where f) over wnd as nstrict_init_filt,\n+-- \tlogging_agg_strict(v::text) filter(where f) over wnd as strict_filt,\n+-- \tlogging_agg_strict_initcond(v) filter(where f) over wnd as strict_init_filt\n+-- FROM (VALUES\n+-- \t(1, 1, true,  NULL),\n+-- \t(1, 2, false, 'a'),\n+-- \t(1, 3, true,  'b'),\n+-- \t(1, 4, false, NULL),\n+-- \t(1, 5, false, NULL),\n+-- \t(1, 6, false, 'c'),\n+-- \t(2, 1, false, NULL),\n+-- \t(2, 2, true,  'x'),\n+-- \t(3, 1, true,  'z')\n+-- ) AS t(p, i, f, v)\n+-- WINDOW wnd AS (PARTITION BY p ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY p, i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test that volatile arguments disable moving-aggregate mode\n+-- SELECT\n+-- \ti::text || ':' || COALESCE(v::text, 'NULL') as row,\n+-- \tlogging_agg_strict(v::text)\n+-- \t\tover wnd as inverse,\n+-- \tlogging_agg_strict(v::text || CASE WHEN random() < 0 then '?' ELSE '' END)\n+-- \t\tover wnd as noinverse\n+-- FROM (VALUES\n+-- \t(1, 'a'),\n+-- \t(2, 'b'),\n+-- \t(3, 'c')\n+-- ) AS t(i, v)\n+-- WINDOW wnd AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- SELECT\n+-- \ti::text || ':' || COALESCE(v::text, 'NULL') as row,\n+-- \tlogging_agg_strict(v::text) filter(where true)\n+-- \t\tover wnd as inverse,\n+-- \tlogging_agg_strict(v::text) filter(where random() >= 0)\n+-- \t\tover wnd as noinverse\n+-- FROM (VALUES\n+-- \t(1, 'a'),\n+-- \t(2, 'b'),\n+-- \t(3, 'c')\n+-- ) AS t(i, v)\n+-- WINDOW wnd AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test that non-overlapping windows don't use inverse transitions\n+-- SELECT\n+-- \tlogging_agg_strict(v::text) OVER wnd\n+-- FROM (VALUES\n+-- \t(1, 'a'),\n+-- \t(2, 'b'),\n+-- \t(3, 'c')\n+-- ) AS t(i, v)\n+-- WINDOW wnd AS (ORDER BY i ROWS BETWEEN CURRENT ROW AND CURRENT ROW)\n+-- ORDER BY i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test that returning NULL from the inverse transition functions\n+-- restarts the aggregation from scratch. The second aggregate is supposed\n+-- to test cases where only some aggregates restart, the third one checks\n+-- that one aggregate restarting doesn't cause others to restart.\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION sum_int_randrestart_minvfunc(int4, int4) RETURNS int4 AS\n+-- $$ SELECT CASE WHEN random() < 0.2 THEN NULL ELSE $1 - $2 END $$\n+-- LANGUAGE SQL STRICT;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE sum_int_randomrestart (int4)\n+-- (\n+-- \tstype = int4,\n+-- \tsfunc = int4pl,\n+-- \tmstype = int4,\n+-- \tmsfunc = int4pl,\n+-- \tminvfunc = sum_int_randrestart_minvfunc\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- WITH\n+-- vs AS (\n+-- \tSELECT i, (random() * 100)::int4 AS v\n+-- \tFROM generate_series(1, 100) AS i\n+-- ),\n+-- sum_following AS (\n+-- \tSELECT i, SUM(v) OVER\n+-- \t\t(ORDER BY i DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS s\n+-- \tFROM vs\n+-- )\n+-- SELECT DISTINCT\n+-- \tsum_following.s = sum_int_randomrestart(v) OVER fwd AS eq1,\n+-- \t-sum_following.s = sum_int_randomrestart(-v) OVER fwd AS eq2,\n+-- \t100*3+(vs.i-1)*3 = length(logging_agg_nonstrict(''::text) OVER fwd) AS eq3\n+-- FROM vs\n+-- JOIN sum_following ON sum_following.i = vs.i\n+-- WINDOW fwd AS (\n+-- \tORDER BY vs.i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING\n+-- );\n+\n+--\n+-- Test various built-in aggregates that have moving-aggregate support\n+--\n+\n+-- test inverse transition functions handle NULLs properly\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1.5),(2,2.5),(3,NULL),(4,NULL)) t(i,v);\n+\n+-- [SPARK-28602] Spark does not recognize 'interval' type as 'numeric'\n+-- SELECT i,AVG(v::interval) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+--   FROM (VALUES(1,'1 sec'),(2,'2 sec'),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+-- The cast syntax is present in PgSQL for legacy reasons and Spark will not recognize a money field\n+-- SELECT i,SUM(v::money) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+--   FROM (VALUES(1,'1.10'),(2,'2.20'),(3,NULL),(4,NULL)) t(i,v);\n+\n+-- [SPARK-28602] Spark does not recognize 'interval' type as 'numeric'\n+-- SELECT i,SUM(cast(v as interval)) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+--   FROM (VALUES(1,'1 sec'),(2,'2 sec'),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1.1),(2,2.2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT SUM(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1.01),(2,2),(3,3)) v(i,n);\n+\n+SELECT i,COUNT(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,COUNT(*) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+-- For the following queries Spark result differs from PgSQL:\n+-- Spark handles division by zero as 'NaN' instead of 'NULL', which is the PgSQL behaviour\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+-- test that inverse transition functions work with various frame options\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND CURRENT ROW)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,3),(4,4)) t(i,v);\n+\n+-- [SPARK-29638] Spark handles 'NaN' as 0 in sums",
    "line": 364
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "Thanks!",
    "commit": "3dfa624c8fbc8d3efbf0cfd18dda690585b2f572",
    "createdAt": "2019-10-30T04:34:42Z",
    "diffHunk": "@@ -0,0 +1,399 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L913-L1278\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test user-defined window function with named args and default args\n+-- CREATE FUNCTION nth_value_def(val anyelement, n integer = 1) RETURNS anyelement\n+--   LANGUAGE internal WINDOW IMMUTABLE STRICT AS 'window_nth_value';\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- SELECT nth_value_def(n := 2, val := ten) OVER (PARTITION BY four), ten, four\n+--   FROM (SELECT * FROM tenk1 WHERE unique2 < 10 ORDER BY four, ten) s;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- SELECT nth_value_def(ten) OVER (PARTITION BY four), ten, four\n+--   FROM (SELECT * FROM tenk1 WHERE unique2 < 10 ORDER BY four, ten) s;\n+\n+--\n+-- Test the basic moving-aggregate machinery\n+--\n+\n+-- create aggregates that record the series of transform calls (these are\n+-- intentionally not true inverses)\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_sfunc_nonstrict(text, anyelement) RETURNS text AS\n+-- $$ SELECT COALESCE($1, '') || '*' || quote_nullable($2) $$\n+-- LANGUAGE SQL IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_msfunc_nonstrict(text, anyelement) RETURNS text AS\n+-- $$ SELECT COALESCE($1, '') || '+' || quote_nullable($2) $$\n+-- LANGUAGE SQL IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_minvfunc_nonstrict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '-' || quote_nullable($2) $$\n+-- LANGUAGE SQL IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_nonstrict (anyelement)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_nonstrict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_nonstrict,\n+-- \tminvfunc = logging_minvfunc_nonstrict\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_nonstrict_initcond (anyelement)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_nonstrict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_nonstrict,\n+-- \tminvfunc = logging_minvfunc_nonstrict,\n+-- \tinitcond = 'I',\n+-- \tminitcond = 'MI'\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_sfunc_strict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '*' || quote_nullable($2) $$\n+-- LANGUAGE SQL STRICT IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_msfunc_strict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '+' || quote_nullable($2) $$\n+-- LANGUAGE SQL STRICT IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION logging_minvfunc_strict(text, anyelement) RETURNS text AS\n+-- $$ SELECT $1 || '-' || quote_nullable($2) $$\n+-- LANGUAGE SQL STRICT IMMUTABLE;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_strict (text)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_strict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_strict,\n+-- \tminvfunc = logging_minvfunc_strict\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE logging_agg_strict_initcond (anyelement)\n+-- (\n+-- \tstype = text,\n+-- \tsfunc = logging_sfunc_strict,\n+-- \tmstype = text,\n+-- \tmsfunc = logging_msfunc_strict,\n+-- \tminvfunc = logging_minvfunc_strict,\n+-- \tinitcond = 'I',\n+-- \tminitcond = 'MI'\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test strict and non-strict cases\n+-- SELECT\n+-- \tp::text || ',' || i::text || ':' || COALESCE(v::text, 'NULL') AS row,\n+-- \tlogging_agg_nonstrict(v) over wnd as nstrict,\n+-- \tlogging_agg_nonstrict_initcond(v) over wnd as nstrict_init,\n+-- \tlogging_agg_strict(v::text) over wnd as strict,\n+-- \tlogging_agg_strict_initcond(v) over wnd as strict_init\n+-- FROM (VALUES\n+-- \t(1, 1, NULL),\n+-- \t(1, 2, 'a'),\n+-- \t(1, 3, 'b'),\n+-- \t(1, 4, NULL),\n+-- \t(1, 5, NULL),\n+-- \t(1, 6, 'c'),\n+-- \t(2, 1, NULL),\n+-- \t(2, 2, 'x'),\n+-- \t(3, 1, 'z')\n+-- ) AS t(p, i, v)\n+-- WINDOW wnd AS (PARTITION BY P ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY p, i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- and again, but with filter\n+-- SELECT\n+-- \tp::text || ',' || i::text || ':' ||\n+-- \t\tCASE WHEN f THEN COALESCE(v::text, 'NULL') ELSE '-' END as row,\n+-- \tlogging_agg_nonstrict(v) filter(where f) over wnd as nstrict_filt,\n+-- \tlogging_agg_nonstrict_initcond(v) filter(where f) over wnd as nstrict_init_filt,\n+-- \tlogging_agg_strict(v::text) filter(where f) over wnd as strict_filt,\n+-- \tlogging_agg_strict_initcond(v) filter(where f) over wnd as strict_init_filt\n+-- FROM (VALUES\n+-- \t(1, 1, true,  NULL),\n+-- \t(1, 2, false, 'a'),\n+-- \t(1, 3, true,  'b'),\n+-- \t(1, 4, false, NULL),\n+-- \t(1, 5, false, NULL),\n+-- \t(1, 6, false, 'c'),\n+-- \t(2, 1, false, NULL),\n+-- \t(2, 2, true,  'x'),\n+-- \t(3, 1, true,  'z')\n+-- ) AS t(p, i, f, v)\n+-- WINDOW wnd AS (PARTITION BY p ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY p, i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test that volatile arguments disable moving-aggregate mode\n+-- SELECT\n+-- \ti::text || ':' || COALESCE(v::text, 'NULL') as row,\n+-- \tlogging_agg_strict(v::text)\n+-- \t\tover wnd as inverse,\n+-- \tlogging_agg_strict(v::text || CASE WHEN random() < 0 then '?' ELSE '' END)\n+-- \t\tover wnd as noinverse\n+-- FROM (VALUES\n+-- \t(1, 'a'),\n+-- \t(2, 'b'),\n+-- \t(3, 'c')\n+-- ) AS t(i, v)\n+-- WINDOW wnd AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- SELECT\n+-- \ti::text || ':' || COALESCE(v::text, 'NULL') as row,\n+-- \tlogging_agg_strict(v::text) filter(where true)\n+-- \t\tover wnd as inverse,\n+-- \tlogging_agg_strict(v::text) filter(where random() >= 0)\n+-- \t\tover wnd as noinverse\n+-- FROM (VALUES\n+-- \t(1, 'a'),\n+-- \t(2, 'b'),\n+-- \t(3, 'c')\n+-- ) AS t(i, v)\n+-- WINDOW wnd AS (ORDER BY i ROWS BETWEEN 1 PRECEDING AND CURRENT ROW)\n+-- ORDER BY i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test that non-overlapping windows don't use inverse transitions\n+-- SELECT\n+-- \tlogging_agg_strict(v::text) OVER wnd\n+-- FROM (VALUES\n+-- \t(1, 'a'),\n+-- \t(2, 'b'),\n+-- \t(3, 'c')\n+-- ) AS t(i, v)\n+-- WINDOW wnd AS (ORDER BY i ROWS BETWEEN CURRENT ROW AND CURRENT ROW)\n+-- ORDER BY i;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- test that returning NULL from the inverse transition functions\n+-- restarts the aggregation from scratch. The second aggregate is supposed\n+-- to test cases where only some aggregates restart, the third one checks\n+-- that one aggregate restarting doesn't cause others to restart.\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE FUNCTION sum_int_randrestart_minvfunc(int4, int4) RETURNS int4 AS\n+-- $$ SELECT CASE WHEN random() < 0.2 THEN NULL ELSE $1 - $2 END $$\n+-- LANGUAGE SQL STRICT;\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- CREATE AGGREGATE sum_int_randomrestart (int4)\n+-- (\n+-- \tstype = int4,\n+-- \tsfunc = int4pl,\n+-- \tmstype = int4,\n+-- \tmsfunc = int4pl,\n+-- \tminvfunc = sum_int_randrestart_minvfunc\n+-- );\n+\n+-- Spark doesn't handle UDFs in SQL\n+-- WITH\n+-- vs AS (\n+-- \tSELECT i, (random() * 100)::int4 AS v\n+-- \tFROM generate_series(1, 100) AS i\n+-- ),\n+-- sum_following AS (\n+-- \tSELECT i, SUM(v) OVER\n+-- \t\t(ORDER BY i DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS s\n+-- \tFROM vs\n+-- )\n+-- SELECT DISTINCT\n+-- \tsum_following.s = sum_int_randomrestart(v) OVER fwd AS eq1,\n+-- \t-sum_following.s = sum_int_randomrestart(-v) OVER fwd AS eq2,\n+-- \t100*3+(vs.i-1)*3 = length(logging_agg_nonstrict(''::text) OVER fwd) AS eq3\n+-- FROM vs\n+-- JOIN sum_following ON sum_following.i = vs.i\n+-- WINDOW fwd AS (\n+-- \tORDER BY vs.i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING\n+-- );\n+\n+--\n+-- Test various built-in aggregates that have moving-aggregate support\n+--\n+\n+-- test inverse transition functions handle NULLs properly\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,AVG(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1.5),(2,2.5),(3,NULL),(4,NULL)) t(i,v);\n+\n+-- [SPARK-28602] Spark does not recognize 'interval' type as 'numeric'\n+-- SELECT i,AVG(v::interval) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+--   FROM (VALUES(1,'1 sec'),(2,'2 sec'),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+-- The cast syntax is present in PgSQL for legacy reasons and Spark will not recognize a money field\n+-- SELECT i,SUM(v::money) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+--   FROM (VALUES(1,'1.10'),(2,'2.20'),(3,NULL),(4,NULL)) t(i,v);\n+\n+-- [SPARK-28602] Spark does not recognize 'interval' type as 'numeric'\n+-- SELECT i,SUM(cast(v as interval)) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+--   FROM (VALUES(1,'1 sec'),(2,'2 sec'),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1.1),(2,2.2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT SUM(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1.01),(2,2),(3,3)) v(i,n);\n+\n+SELECT i,COUNT(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,COUNT(*) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VAR_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT VARIANCE(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_POP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+-- For the following queries Spark result differs from PgSQL:\n+-- Spark handles division by zero as 'NaN' instead of 'NULL', which is the PgSQL behaviour\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV_SAMP(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(1,NULL),(2,600),(3,470),(4,170),(5,430),(6,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+SELECT STDDEV(n) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)\n+  FROM (VALUES(0,NULL),(1,600),(2,470),(3,170),(4,430),(5,300)) r(i,n);\n+\n+-- test that inverse transition functions work with various frame options\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND CURRENT ROW)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,NULL),(4,NULL)) t(i,v);\n+\n+SELECT i,SUM(v) OVER (ORDER BY i ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)\n+  FROM (VALUES(1,1),(2,2),(3,3),(4,4)) t(i,v);\n+\n+-- [SPARK-29638] Spark handles 'NaN' as 0 in sums",
    "line": 364
  }],
  "prId": 26238
}]