[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Instead of the above, the following?\r\n```\r\nINSERT INTO empsalary VALUES\r\n ('develop', 10, 5200, date '2007-08-01'),\r\n ('sales', 1, 5000, date '2006-10-01'),\r\n ('personnel', 5, 3500, date '2007-12-10'),\r\n ('sales', 4, 4800, date '2007-08-08'),\r\n ('personnel', 2, 3900, date '2006-12-23'),\r\n ('develop', 7, 4200, date '2008-01-01'),\r\n ('develop', 9, 4500, date '2008-01-01'),\r\n ('sales', 3, 4800, date '2007-08-01'),\r\n ('develop', 8, 6000, date '2006-10-01'),\r\n ('develop', 11, 5200, date '2007-08-15');\r\n```",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-28T05:10:05Z",
    "diffHunk": "@@ -12,17 +12,16 @@ CREATE TABLE empsalary (\n     enroll_date date\n ) USING parquet;\n \n-INSERT INTO empsalary VALUES\n-('develop', 10, 5200, '2007-08-01'),\n-('sales', 1, 5000, '2006-10-01'),\n-('personnel', 5, 3500, '2007-12-10'),\n-('sales', 4, 4800, '2007-08-08'),\n-('personnel', 2, 3900, '2006-12-23'),\n-('develop', 7, 4200, '2008-01-01'),\n-('develop', 9, 4500, '2008-01-01'),\n-('sales', 3, 4800, '2007-08-01'),\n-('develop', 8, 6000, '2006-10-01'),\n-('develop', 11, 5200, '2007-08-15');\n+INSERT INTO empsalary VALUES ('develop', 10, 5200, cast ('2007-08-01' as date));\n+INSERT INTO empsalary VALUES ('sales', 1, 5000, cast ('2006-10-01' as date));\n+INSERT INTO empsalary VALUES ('personnel', 5, 3500, cast ('2007-12-10' as date));\n+INSERT INTO empsalary VALUES ('sales', 4, 4800, cast ('2007-08-08' as date));\n+INSERT INTO empsalary VALUES ('personnel', 2, 3900, cast ('2006-12-23' as date));\n+INSERT INTO empsalary VALUES ('develop', 7, 4200, cast ('2008-01-01' as date));\n+INSERT INTO empsalary VALUES ('develop', 9, 4500, cast ('2008-01-01' as date));\n+INSERT INTO empsalary VALUES ('sales', 3, 4800, cast ('2007-08-01' as date));\n+INSERT INTO empsalary VALUES ('develop', 8, 6000, cast ('2006-10-01' as date));\n+INSERT INTO empsalary VALUES ('develop', 11, 5200, cast ('2007-08-15' as date));"
  }],
  "prId": 26274
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Ur, does this work?\r\n```\r\nspark-sql> select cast ('11:00 BST' as timestamp);\r\nNULL\r\nTime taken: 2.248 seconds, Fetched 1 row(s)\r\n```",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-28T05:14:18Z",
    "diffHunk": "@@ -36,17 +35,17 @@ create table datetimes (\n     f_timestamp timestamp\n ) using parquet;\n \n-insert into datetimes values\n-(1, '11:00', '11:00 BST', '1 year', '2000-10-19 10:23:54+01', '2000-10-19 10:23:54'),\n-(2, '12:00', '12:00 BST', '2 years', '2001-10-19 10:23:54+01', '2001-10-19 10:23:54'),\n-(3, '13:00', '13:00 BST', '3 years', '2001-10-19 10:23:54+01', '2001-10-19 10:23:54'),\n-(4, '14:00', '14:00 BST', '4 years', '2002-10-19 10:23:54+01', '2002-10-19 10:23:54'),\n-(5, '15:00', '15:00 BST', '5 years', '2003-10-19 10:23:54+01', '2003-10-19 10:23:54'),\n-(6, '15:00', '15:00 BST', '5 years', '2004-10-19 10:23:54+01', '2004-10-19 10:23:54'),\n-(7, '17:00', '17:00 BST', '7 years', '2005-10-19 10:23:54+01', '2005-10-19 10:23:54'),\n-(8, '18:00', '18:00 BST', '8 years', '2006-10-19 10:23:54+01', '2006-10-19 10:23:54'),\n-(9, '19:00', '19:00 BST', '9 years', '2007-10-19 10:23:54+01', '2007-10-19 10:23:54'),\n-(10, '20:00', '20:00 BST', '10 years', '2008-10-19 10:23:54+01', '2008-10-19 10:23:54');\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values (1, cast ('11:00' as timestamp), cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), cast('2000-10-19 10:23:54' as timestamp));"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "IIRC, there exists a JIRA issue to tracking this timestamp issue.",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-28T05:15:46Z",
    "diffHunk": "@@ -36,17 +35,17 @@ create table datetimes (\n     f_timestamp timestamp\n ) using parquet;\n \n-insert into datetimes values\n-(1, '11:00', '11:00 BST', '1 year', '2000-10-19 10:23:54+01', '2000-10-19 10:23:54'),\n-(2, '12:00', '12:00 BST', '2 years', '2001-10-19 10:23:54+01', '2001-10-19 10:23:54'),\n-(3, '13:00', '13:00 BST', '3 years', '2001-10-19 10:23:54+01', '2001-10-19 10:23:54'),\n-(4, '14:00', '14:00 BST', '4 years', '2002-10-19 10:23:54+01', '2002-10-19 10:23:54'),\n-(5, '15:00', '15:00 BST', '5 years', '2003-10-19 10:23:54+01', '2003-10-19 10:23:54'),\n-(6, '15:00', '15:00 BST', '5 years', '2004-10-19 10:23:54+01', '2004-10-19 10:23:54'),\n-(7, '17:00', '17:00 BST', '7 years', '2005-10-19 10:23:54+01', '2005-10-19 10:23:54'),\n-(8, '18:00', '18:00 BST', '8 years', '2006-10-19 10:23:54+01', '2006-10-19 10:23:54'),\n-(9, '19:00', '19:00 BST', '9 years', '2007-10-19 10:23:54+01', '2007-10-19 10:23:54'),\n-(10, '20:00', '20:00 BST', '10 years', '2008-10-19 10:23:54+01', '2008-10-19 10:23:54');\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values (1, cast ('11:00' as timestamp), cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), cast('2000-10-19 10:23:54' as timestamp));"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`timestamp '2000-10-19 10:23:54+01'` seems to have another corresponding JIRA issue, too.\r\n\r\nFor the other cases, we can use `typeConstructor` `timestamp`.\r\n```\r\nspark-sql> select timestamp '11:00';\r\n2019-10-27 11:00:00\r\nspark-sql> select timestamp '2000-10-19 10:23:54';\r\n2000-10-19 10:23:54\r\nTime taken: 0.05 seconds, Fetched 1 row(s)\r\n```",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-28T05:21:41Z",
    "diffHunk": "@@ -36,17 +35,17 @@ create table datetimes (\n     f_timestamp timestamp\n ) using parquet;\n \n-insert into datetimes values\n-(1, '11:00', '11:00 BST', '1 year', '2000-10-19 10:23:54+01', '2000-10-19 10:23:54'),\n-(2, '12:00', '12:00 BST', '2 years', '2001-10-19 10:23:54+01', '2001-10-19 10:23:54'),\n-(3, '13:00', '13:00 BST', '3 years', '2001-10-19 10:23:54+01', '2001-10-19 10:23:54'),\n-(4, '14:00', '14:00 BST', '4 years', '2002-10-19 10:23:54+01', '2002-10-19 10:23:54'),\n-(5, '15:00', '15:00 BST', '5 years', '2003-10-19 10:23:54+01', '2003-10-19 10:23:54'),\n-(6, '15:00', '15:00 BST', '5 years', '2004-10-19 10:23:54+01', '2004-10-19 10:23:54'),\n-(7, '17:00', '17:00 BST', '7 years', '2005-10-19 10:23:54+01', '2005-10-19 10:23:54'),\n-(8, '18:00', '18:00 BST', '8 years', '2006-10-19 10:23:54+01', '2006-10-19 10:23:54'),\n-(9, '19:00', '19:00 BST', '9 years', '2007-10-19 10:23:54+01', '2007-10-19 10:23:54'),\n-(10, '20:00', '20:00 BST', '10 years', '2008-10-19 10:23:54+01', '2008-10-19 10:23:54');\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values (1, cast ('11:00' as timestamp), cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), cast('2000-10-19 10:23:54' as timestamp));"
  }],
  "prId": 26274
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "This seems to be not a `EXPLAIN` statement. Only the above one does.\r\nShallow enable this query?",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-30T23:26:36Z",
    "diffHunk": "@@ -0,0 +1,455 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911\n+\n+CREATE TEMPORARY VIEW tenk2 AS SELECT * FROM tenk1;\n+\n+CREATE TABLE empsalary (\n+    depname string,\n+    empno integer,\n+    salary int,\n+    enroll_date date\n+) USING parquet;\n+\n+INSERT INTO empsalary VALUES\n+  ('develop', 10, 5200, date '2007-08-01'),\n+  ('sales', 1, 5000, date '2006-10-01'),\n+  ('personnel', 5, 3500, date '2007-12-10'),\n+  ('sales', 4, 4800, date '2007-08-08'),\n+  ('personnel', 2, 3900, date '2006-12-23'),\n+  ('develop', 7, 4200, date '2008-01-01'),\n+  ('develop', 9, 4500, date '2008-01-01'),\n+  ('sales', 3, 4800, date '2007-08-01'),\n+  ('develop', 8, 6000, date '2006-10-01'),\n+  ('develop', 11, 5200, date '2007-08-15');\n+\n+-- Test in_range for other datetime datatypes\n+\n+-- Spark only supports timestamp\n+-- [SPARK-29636] Spark can't parse '11:00 BST' or '2000-10-19 10:23:54+01' signatures to timestamp\n+create table datetimes (\n+    id int,\n+    f_time timestamp,\n+    f_timetz timestamp,\n+    f_interval timestamp,\n+    f_timestamptz timestamp,\n+    f_timestamp timestamp\n+) using parquet;\n+\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values\n+(1, timestamp '11:00', cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), timestamp '2000-10-19 10:23:54'),\n+(2, timestamp '12:00', cast ('12:00 BST' as timestamp), cast ('2 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(3, timestamp '13:00', cast ('13:00 BST' as timestamp), cast ('3 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(4, timestamp '14:00', cast ('14:00 BST' as timestamp), cast ('4 years' as timestamp), cast ('2002-10-19 10:23:54+01' as timestamp), timestamp '2002-10-19 10:23:54'),\n+(5, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2003-10-19 10:23:54+01' as timestamp), timestamp '2003-10-19 10:23:54'),\n+(6, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2004-10-19 10:23:54+01' as timestamp), timestamp '2004-10-19 10:23:54'),\n+(7, timestamp '17:00', cast ('17:00 BST' as timestamp), cast ('7 years' as timestamp), cast ('2005-10-19 10:23:54+01' as timestamp), timestamp '2005-10-19 10:23:54'),\n+(8, timestamp '18:00', cast ('18:00 BST' as timestamp), cast ('8 years' as timestamp), cast ('2006-10-19 10:23:54+01' as timestamp), timestamp '2006-10-19 10:23:54'),\n+(9, timestamp '19:00', cast ('19:00 BST' as timestamp), cast ('9 years' as timestamp), cast ('2007-10-19 10:23:54+01' as timestamp), timestamp '2007-10-19 10:23:54'),\n+(10, timestamp '20:00', cast ('20:00 BST' as timestamp), cast ('10 years' as timestamp), cast ('2008-10-19 10:23:54+01' as timestamp), timestamp '2008-10-19 10:23:54');\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- RANGE offset PRECEDING/FOLLOWING error cases\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by enroll_date, salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by depname range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between 1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between -1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between 1 preceding and -2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between '1 year' preceding and '-2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- GROUPS tests\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and current row),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between current row and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 following and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and 2 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 preceding),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 0 preceding and 0 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following\n+--   exclude current row), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude group), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude ties), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following),unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude current row), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude group), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude ties), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select first_value(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lead(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28508] Support for range frame+row frame in the same query\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select last(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lag(salary)         over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- select first_value(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude current row),\n+-- lead(salary) over(order by enroll_date groups between 1 following and 3 following exclude ties),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude ties),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select last(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude group),\n+-- lag(salary) over(order by enroll_date groups between 1 following and 3 following exclude group),\n+-- salary, enroll_date from empsalary;\n+\n+-- Show differences in offset interpretation between ROWS, RANGE, and GROUPS\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         SELECT * FROM range(1, 36, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         select 1 union all select 1 union all select 1 union all\n+--         SELECT * FROM range(5, 50, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+-- with UNION\n+\n+SELECT count(*) OVER (PARTITION BY four) FROM (SELECT * FROM tenk1 UNION ALL SELECT * FROM tenk2)s LIMIT 0;\n+\n+-- check some degenerate cases\n+\n+create table t1 (f1 int, f2 int) using parquet;\n+insert into t1 values (1,1),(1,2),(2,2);\n+\n+select f1, sum(f1) over (partition by f1\n+                         range between 1 preceding and 1 following)\n+from t1 where f1 = f2; -- error, must have order by\n+\n+-- Since EXPLAIN clause rely on host physical location, it is commented out\n+-- explain\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+-- select f1, sum(f1) over (partition by f1 order by f2"
  }],
  "prId": 26274
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Could you move this to at line 40?",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-30T23:27:57Z",
    "diffHunk": "@@ -0,0 +1,455 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911\n+\n+CREATE TEMPORARY VIEW tenk2 AS SELECT * FROM tenk1;\n+\n+CREATE TABLE empsalary (\n+    depname string,\n+    empno integer,\n+    salary int,\n+    enroll_date date\n+) USING parquet;\n+\n+INSERT INTO empsalary VALUES\n+  ('develop', 10, 5200, date '2007-08-01'),\n+  ('sales', 1, 5000, date '2006-10-01'),\n+  ('personnel', 5, 3500, date '2007-12-10'),\n+  ('sales', 4, 4800, date '2007-08-08'),\n+  ('personnel', 2, 3900, date '2006-12-23'),\n+  ('develop', 7, 4200, date '2008-01-01'),\n+  ('develop', 9, 4500, date '2008-01-01'),\n+  ('sales', 3, 4800, date '2007-08-01'),\n+  ('develop', 8, 6000, date '2006-10-01'),\n+  ('develop', 11, 5200, date '2007-08-15');\n+\n+-- Test in_range for other datetime datatypes\n+\n+-- Spark only supports timestamp\n+-- [SPARK-29636] Spark can't parse '11:00 BST' or '2000-10-19 10:23:54+01' signatures to timestamp",
    "line": 30
  }],
  "prId": 26274
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Let's remove this duplicated line 147 . (We have line 149).",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-30T23:37:12Z",
    "diffHunk": "@@ -0,0 +1,455 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911\n+\n+CREATE TEMPORARY VIEW tenk2 AS SELECT * FROM tenk1;\n+\n+CREATE TABLE empsalary (\n+    depname string,\n+    empno integer,\n+    salary int,\n+    enroll_date date\n+) USING parquet;\n+\n+INSERT INTO empsalary VALUES\n+  ('develop', 10, 5200, date '2007-08-01'),\n+  ('sales', 1, 5000, date '2006-10-01'),\n+  ('personnel', 5, 3500, date '2007-12-10'),\n+  ('sales', 4, 4800, date '2007-08-08'),\n+  ('personnel', 2, 3900, date '2006-12-23'),\n+  ('develop', 7, 4200, date '2008-01-01'),\n+  ('develop', 9, 4500, date '2008-01-01'),\n+  ('sales', 3, 4800, date '2007-08-01'),\n+  ('develop', 8, 6000, date '2006-10-01'),\n+  ('develop', 11, 5200, date '2007-08-15');\n+\n+-- Test in_range for other datetime datatypes\n+\n+-- Spark only supports timestamp\n+-- [SPARK-29636] Spark can't parse '11:00 BST' or '2000-10-19 10:23:54+01' signatures to timestamp\n+create table datetimes (\n+    id int,\n+    f_time timestamp,\n+    f_timetz timestamp,\n+    f_interval timestamp,\n+    f_timestamptz timestamp,\n+    f_timestamp timestamp\n+) using parquet;\n+\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values\n+(1, timestamp '11:00', cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), timestamp '2000-10-19 10:23:54'),\n+(2, timestamp '12:00', cast ('12:00 BST' as timestamp), cast ('2 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(3, timestamp '13:00', cast ('13:00 BST' as timestamp), cast ('3 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(4, timestamp '14:00', cast ('14:00 BST' as timestamp), cast ('4 years' as timestamp), cast ('2002-10-19 10:23:54+01' as timestamp), timestamp '2002-10-19 10:23:54'),\n+(5, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2003-10-19 10:23:54+01' as timestamp), timestamp '2003-10-19 10:23:54'),\n+(6, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2004-10-19 10:23:54+01' as timestamp), timestamp '2004-10-19 10:23:54'),\n+(7, timestamp '17:00', cast ('17:00 BST' as timestamp), cast ('7 years' as timestamp), cast ('2005-10-19 10:23:54+01' as timestamp), timestamp '2005-10-19 10:23:54'),\n+(8, timestamp '18:00', cast ('18:00 BST' as timestamp), cast ('8 years' as timestamp), cast ('2006-10-19 10:23:54+01' as timestamp), timestamp '2006-10-19 10:23:54'),\n+(9, timestamp '19:00', cast ('19:00 BST' as timestamp), cast ('9 years' as timestamp), cast ('2007-10-19 10:23:54+01' as timestamp), timestamp '2007-10-19 10:23:54'),\n+(10, timestamp '20:00', cast ('20:00 BST' as timestamp), cast ('10 years' as timestamp), cast ('2008-10-19 10:23:54+01' as timestamp), timestamp '2008-10-19 10:23:54');\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- RANGE offset PRECEDING/FOLLOWING error cases\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by enroll_date, salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by depname range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between 1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between -1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between 1 preceding and -2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between '1 year' preceding and '-2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- GROUPS tests\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses"
  }],
  "prId": 26274
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Like the original, let's remove this empty line 260.",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-30T23:38:02Z",
    "diffHunk": "@@ -0,0 +1,455 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911\n+\n+CREATE TEMPORARY VIEW tenk2 AS SELECT * FROM tenk1;\n+\n+CREATE TABLE empsalary (\n+    depname string,\n+    empno integer,\n+    salary int,\n+    enroll_date date\n+) USING parquet;\n+\n+INSERT INTO empsalary VALUES\n+  ('develop', 10, 5200, date '2007-08-01'),\n+  ('sales', 1, 5000, date '2006-10-01'),\n+  ('personnel', 5, 3500, date '2007-12-10'),\n+  ('sales', 4, 4800, date '2007-08-08'),\n+  ('personnel', 2, 3900, date '2006-12-23'),\n+  ('develop', 7, 4200, date '2008-01-01'),\n+  ('develop', 9, 4500, date '2008-01-01'),\n+  ('sales', 3, 4800, date '2007-08-01'),\n+  ('develop', 8, 6000, date '2006-10-01'),\n+  ('develop', 11, 5200, date '2007-08-15');\n+\n+-- Test in_range for other datetime datatypes\n+\n+-- Spark only supports timestamp\n+-- [SPARK-29636] Spark can't parse '11:00 BST' or '2000-10-19 10:23:54+01' signatures to timestamp\n+create table datetimes (\n+    id int,\n+    f_time timestamp,\n+    f_timetz timestamp,\n+    f_interval timestamp,\n+    f_timestamptz timestamp,\n+    f_timestamp timestamp\n+) using parquet;\n+\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values\n+(1, timestamp '11:00', cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), timestamp '2000-10-19 10:23:54'),\n+(2, timestamp '12:00', cast ('12:00 BST' as timestamp), cast ('2 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(3, timestamp '13:00', cast ('13:00 BST' as timestamp), cast ('3 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(4, timestamp '14:00', cast ('14:00 BST' as timestamp), cast ('4 years' as timestamp), cast ('2002-10-19 10:23:54+01' as timestamp), timestamp '2002-10-19 10:23:54'),\n+(5, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2003-10-19 10:23:54+01' as timestamp), timestamp '2003-10-19 10:23:54'),\n+(6, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2004-10-19 10:23:54+01' as timestamp), timestamp '2004-10-19 10:23:54'),\n+(7, timestamp '17:00', cast ('17:00 BST' as timestamp), cast ('7 years' as timestamp), cast ('2005-10-19 10:23:54+01' as timestamp), timestamp '2005-10-19 10:23:54'),\n+(8, timestamp '18:00', cast ('18:00 BST' as timestamp), cast ('8 years' as timestamp), cast ('2006-10-19 10:23:54+01' as timestamp), timestamp '2006-10-19 10:23:54'),\n+(9, timestamp '19:00', cast ('19:00 BST' as timestamp), cast ('9 years' as timestamp), cast ('2007-10-19 10:23:54+01' as timestamp), timestamp '2007-10-19 10:23:54'),\n+(10, timestamp '20:00', cast ('20:00 BST' as timestamp), cast ('10 years' as timestamp), cast ('2008-10-19 10:23:54+01' as timestamp), timestamp '2008-10-19 10:23:54');\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- RANGE offset PRECEDING/FOLLOWING error cases\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by enroll_date, salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by depname range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between 1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between -1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between 1 preceding and -2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between '1 year' preceding and '-2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- GROUPS tests\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and current row),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between current row and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 following and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and 2 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 preceding),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 0 preceding and 0 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following\n+--   exclude current row), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude group), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude ties), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following),unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude current row), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude group), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude ties), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select first_value(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lead(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28508] Support for range frame+row frame in the same query\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select last(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lag(salary)         over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- select first_value(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude current row),\n+-- lead(salary) over(order by enroll_date groups between 1 following and 3 following exclude ties),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude ties),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select last(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude group),\n+-- lag(salary) over(order by enroll_date groups between 1 following and 3 following exclude group),\n+-- salary, enroll_date from empsalary;\n+\n+-- Show differences in offset interpretation between ROWS, RANGE, and GROUPS\n+"
  }],
  "prId": 26274
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Let's remove this empty line 309.",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-30T23:38:29Z",
    "diffHunk": "@@ -0,0 +1,455 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911\n+\n+CREATE TEMPORARY VIEW tenk2 AS SELECT * FROM tenk1;\n+\n+CREATE TABLE empsalary (\n+    depname string,\n+    empno integer,\n+    salary int,\n+    enroll_date date\n+) USING parquet;\n+\n+INSERT INTO empsalary VALUES\n+  ('develop', 10, 5200, date '2007-08-01'),\n+  ('sales', 1, 5000, date '2006-10-01'),\n+  ('personnel', 5, 3500, date '2007-12-10'),\n+  ('sales', 4, 4800, date '2007-08-08'),\n+  ('personnel', 2, 3900, date '2006-12-23'),\n+  ('develop', 7, 4200, date '2008-01-01'),\n+  ('develop', 9, 4500, date '2008-01-01'),\n+  ('sales', 3, 4800, date '2007-08-01'),\n+  ('develop', 8, 6000, date '2006-10-01'),\n+  ('develop', 11, 5200, date '2007-08-15');\n+\n+-- Test in_range for other datetime datatypes\n+\n+-- Spark only supports timestamp\n+-- [SPARK-29636] Spark can't parse '11:00 BST' or '2000-10-19 10:23:54+01' signatures to timestamp\n+create table datetimes (\n+    id int,\n+    f_time timestamp,\n+    f_timetz timestamp,\n+    f_interval timestamp,\n+    f_timestamptz timestamp,\n+    f_timestamp timestamp\n+) using parquet;\n+\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values\n+(1, timestamp '11:00', cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), timestamp '2000-10-19 10:23:54'),\n+(2, timestamp '12:00', cast ('12:00 BST' as timestamp), cast ('2 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(3, timestamp '13:00', cast ('13:00 BST' as timestamp), cast ('3 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(4, timestamp '14:00', cast ('14:00 BST' as timestamp), cast ('4 years' as timestamp), cast ('2002-10-19 10:23:54+01' as timestamp), timestamp '2002-10-19 10:23:54'),\n+(5, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2003-10-19 10:23:54+01' as timestamp), timestamp '2003-10-19 10:23:54'),\n+(6, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2004-10-19 10:23:54+01' as timestamp), timestamp '2004-10-19 10:23:54'),\n+(7, timestamp '17:00', cast ('17:00 BST' as timestamp), cast ('7 years' as timestamp), cast ('2005-10-19 10:23:54+01' as timestamp), timestamp '2005-10-19 10:23:54'),\n+(8, timestamp '18:00', cast ('18:00 BST' as timestamp), cast ('8 years' as timestamp), cast ('2006-10-19 10:23:54+01' as timestamp), timestamp '2006-10-19 10:23:54'),\n+(9, timestamp '19:00', cast ('19:00 BST' as timestamp), cast ('9 years' as timestamp), cast ('2007-10-19 10:23:54+01' as timestamp), timestamp '2007-10-19 10:23:54'),\n+(10, timestamp '20:00', cast ('20:00 BST' as timestamp), cast ('10 years' as timestamp), cast ('2008-10-19 10:23:54+01' as timestamp), timestamp '2008-10-19 10:23:54');\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- RANGE offset PRECEDING/FOLLOWING error cases\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by enroll_date, salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by depname range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between 1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between -1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between 1 preceding and -2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between '1 year' preceding and '-2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- GROUPS tests\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and current row),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between current row and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 following and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and 2 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 preceding),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 0 preceding and 0 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following\n+--   exclude current row), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude group), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude ties), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following),unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude current row), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude group), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude ties), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select first_value(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lead(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28508] Support for range frame+row frame in the same query\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select last(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lag(salary)         over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- select first_value(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude current row),\n+-- lead(salary) over(order by enroll_date groups between 1 following and 3 following exclude ties),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude ties),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select last(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude group),\n+-- lag(salary) over(order by enroll_date groups between 1 following and 3 following exclude group),\n+-- salary, enroll_date from empsalary;\n+\n+-- Show differences in offset interpretation between ROWS, RANGE, and GROUPS\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         SELECT * FROM range(1, 36, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         select 1 union all select 1 union all select 1 union all\n+--         SELECT * FROM range(5, 50, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+-- with UNION\n+"
  }],
  "prId": 26274
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "ditto for line 313.",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-30T23:38:41Z",
    "diffHunk": "@@ -0,0 +1,455 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911\n+\n+CREATE TEMPORARY VIEW tenk2 AS SELECT * FROM tenk1;\n+\n+CREATE TABLE empsalary (\n+    depname string,\n+    empno integer,\n+    salary int,\n+    enroll_date date\n+) USING parquet;\n+\n+INSERT INTO empsalary VALUES\n+  ('develop', 10, 5200, date '2007-08-01'),\n+  ('sales', 1, 5000, date '2006-10-01'),\n+  ('personnel', 5, 3500, date '2007-12-10'),\n+  ('sales', 4, 4800, date '2007-08-08'),\n+  ('personnel', 2, 3900, date '2006-12-23'),\n+  ('develop', 7, 4200, date '2008-01-01'),\n+  ('develop', 9, 4500, date '2008-01-01'),\n+  ('sales', 3, 4800, date '2007-08-01'),\n+  ('develop', 8, 6000, date '2006-10-01'),\n+  ('develop', 11, 5200, date '2007-08-15');\n+\n+-- Test in_range for other datetime datatypes\n+\n+-- Spark only supports timestamp\n+-- [SPARK-29636] Spark can't parse '11:00 BST' or '2000-10-19 10:23:54+01' signatures to timestamp\n+create table datetimes (\n+    id int,\n+    f_time timestamp,\n+    f_timetz timestamp,\n+    f_interval timestamp,\n+    f_timestamptz timestamp,\n+    f_timestamp timestamp\n+) using parquet;\n+\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values\n+(1, timestamp '11:00', cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), timestamp '2000-10-19 10:23:54'),\n+(2, timestamp '12:00', cast ('12:00 BST' as timestamp), cast ('2 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(3, timestamp '13:00', cast ('13:00 BST' as timestamp), cast ('3 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(4, timestamp '14:00', cast ('14:00 BST' as timestamp), cast ('4 years' as timestamp), cast ('2002-10-19 10:23:54+01' as timestamp), timestamp '2002-10-19 10:23:54'),\n+(5, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2003-10-19 10:23:54+01' as timestamp), timestamp '2003-10-19 10:23:54'),\n+(6, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2004-10-19 10:23:54+01' as timestamp), timestamp '2004-10-19 10:23:54'),\n+(7, timestamp '17:00', cast ('17:00 BST' as timestamp), cast ('7 years' as timestamp), cast ('2005-10-19 10:23:54+01' as timestamp), timestamp '2005-10-19 10:23:54'),\n+(8, timestamp '18:00', cast ('18:00 BST' as timestamp), cast ('8 years' as timestamp), cast ('2006-10-19 10:23:54+01' as timestamp), timestamp '2006-10-19 10:23:54'),\n+(9, timestamp '19:00', cast ('19:00 BST' as timestamp), cast ('9 years' as timestamp), cast ('2007-10-19 10:23:54+01' as timestamp), timestamp '2007-10-19 10:23:54'),\n+(10, timestamp '20:00', cast ('20:00 BST' as timestamp), cast ('10 years' as timestamp), cast ('2008-10-19 10:23:54+01' as timestamp), timestamp '2008-10-19 10:23:54');\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- RANGE offset PRECEDING/FOLLOWING error cases\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by enroll_date, salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by depname range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between 1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between -1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between 1 preceding and -2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between '1 year' preceding and '-2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- GROUPS tests\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and current row),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between current row and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 following and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and 2 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 preceding),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 0 preceding and 0 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following\n+--   exclude current row), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude group), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude ties), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following),unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude current row), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude group), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude ties), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select first_value(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lead(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28508] Support for range frame+row frame in the same query\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select last(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lag(salary)         over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- select first_value(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude current row),\n+-- lead(salary) over(order by enroll_date groups between 1 following and 3 following exclude ties),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude ties),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select last(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude group),\n+-- lag(salary) over(order by enroll_date groups between 1 following and 3 following exclude group),\n+-- salary, enroll_date from empsalary;\n+\n+-- Show differences in offset interpretation between ROWS, RANGE, and GROUPS\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         SELECT * FROM range(1, 36, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         select 1 union all select 1 union all select 1 union all\n+--         SELECT * FROM range(5, 50, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+-- with UNION\n+\n+SELECT count(*) OVER (PARTITION BY four) FROM (SELECT * FROM tenk1 UNION ALL SELECT * FROM tenk2)s LIMIT 0;\n+\n+-- check some degenerate cases\n+"
  }],
  "prId": 26274
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Ditto for line 365.",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-30T23:39:11Z",
    "diffHunk": "@@ -0,0 +1,455 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911\n+\n+CREATE TEMPORARY VIEW tenk2 AS SELECT * FROM tenk1;\n+\n+CREATE TABLE empsalary (\n+    depname string,\n+    empno integer,\n+    salary int,\n+    enroll_date date\n+) USING parquet;\n+\n+INSERT INTO empsalary VALUES\n+  ('develop', 10, 5200, date '2007-08-01'),\n+  ('sales', 1, 5000, date '2006-10-01'),\n+  ('personnel', 5, 3500, date '2007-12-10'),\n+  ('sales', 4, 4800, date '2007-08-08'),\n+  ('personnel', 2, 3900, date '2006-12-23'),\n+  ('develop', 7, 4200, date '2008-01-01'),\n+  ('develop', 9, 4500, date '2008-01-01'),\n+  ('sales', 3, 4800, date '2007-08-01'),\n+  ('develop', 8, 6000, date '2006-10-01'),\n+  ('develop', 11, 5200, date '2007-08-15');\n+\n+-- Test in_range for other datetime datatypes\n+\n+-- Spark only supports timestamp\n+-- [SPARK-29636] Spark can't parse '11:00 BST' or '2000-10-19 10:23:54+01' signatures to timestamp\n+create table datetimes (\n+    id int,\n+    f_time timestamp,\n+    f_timetz timestamp,\n+    f_interval timestamp,\n+    f_timestamptz timestamp,\n+    f_timestamp timestamp\n+) using parquet;\n+\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values\n+(1, timestamp '11:00', cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), timestamp '2000-10-19 10:23:54'),\n+(2, timestamp '12:00', cast ('12:00 BST' as timestamp), cast ('2 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(3, timestamp '13:00', cast ('13:00 BST' as timestamp), cast ('3 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(4, timestamp '14:00', cast ('14:00 BST' as timestamp), cast ('4 years' as timestamp), cast ('2002-10-19 10:23:54+01' as timestamp), timestamp '2002-10-19 10:23:54'),\n+(5, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2003-10-19 10:23:54+01' as timestamp), timestamp '2003-10-19 10:23:54'),\n+(6, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2004-10-19 10:23:54+01' as timestamp), timestamp '2004-10-19 10:23:54'),\n+(7, timestamp '17:00', cast ('17:00 BST' as timestamp), cast ('7 years' as timestamp), cast ('2005-10-19 10:23:54+01' as timestamp), timestamp '2005-10-19 10:23:54'),\n+(8, timestamp '18:00', cast ('18:00 BST' as timestamp), cast ('8 years' as timestamp), cast ('2006-10-19 10:23:54+01' as timestamp), timestamp '2006-10-19 10:23:54'),\n+(9, timestamp '19:00', cast ('19:00 BST' as timestamp), cast ('9 years' as timestamp), cast ('2007-10-19 10:23:54+01' as timestamp), timestamp '2007-10-19 10:23:54'),\n+(10, timestamp '20:00', cast ('20:00 BST' as timestamp), cast ('10 years' as timestamp), cast ('2008-10-19 10:23:54+01' as timestamp), timestamp '2008-10-19 10:23:54');\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- RANGE offset PRECEDING/FOLLOWING error cases\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by enroll_date, salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by depname range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between 1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between -1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between 1 preceding and -2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between '1 year' preceding and '-2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- GROUPS tests\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and current row),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between current row and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 following and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and 2 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 preceding),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 0 preceding and 0 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following\n+--   exclude current row), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude group), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude ties), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following),unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude current row), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude group), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude ties), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select first_value(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lead(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28508] Support for range frame+row frame in the same query\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select last(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lag(salary)         over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- select first_value(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude current row),\n+-- lead(salary) over(order by enroll_date groups between 1 following and 3 following exclude ties),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude ties),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select last(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude group),\n+-- lag(salary) over(order by enroll_date groups between 1 following and 3 following exclude group),\n+-- salary, enroll_date from empsalary;\n+\n+-- Show differences in offset interpretation between ROWS, RANGE, and GROUPS\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         SELECT * FROM range(1, 36, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         select 1 union all select 1 union all select 1 union all\n+--         SELECT * FROM range(5, 50, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+-- with UNION\n+\n+SELECT count(*) OVER (PARTITION BY four) FROM (SELECT * FROM tenk1 UNION ALL SELECT * FROM tenk2)s LIMIT 0;\n+\n+-- check some degenerate cases\n+\n+create table t1 (f1 int, f2 int) using parquet;\n+insert into t1 values (1,1),(1,2),(2,2);\n+\n+select f1, sum(f1) over (partition by f1\n+                         range between 1 preceding and 1 following)\n+from t1 where f1 = f2; -- error, must have order by\n+\n+-- Since EXPLAIN clause rely on host physical location, it is commented out\n+-- explain\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+--                          range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+select f1, sum(f1) over (partition by f1, f1 order by f2\n+range between 2 preceding and 1 preceding)\n+from t1 where f1 = f2;\n+\n+select f1, sum(f1) over (partition by f1, f2 order by f2\n+range between 1 following and 2 following)\n+from t1 where f1 = f2;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1,\n+-- groups between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+-- Since EXPLAIN clause rely on host physical location, it is commented out\n+-- explain\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- groups between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1, f1 order by f2\n+-- groups between 2 preceding and 1 preceding)\n+-- from t1 where f1 = f2;\n+ \n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1, f2 order by f2\n+-- groups between 1 following and 2 following)\n+-- from t1 where f1 = f2;\n+\n+-- ordering by a non-integer constant is allowed\n+"
  }],
  "prId": 26274
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "? So, are you suggesting to comment out line 380?\r\nIf we can have line 380, let's remove line 379.",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-10-30T23:40:32Z",
    "diffHunk": "@@ -0,0 +1,455 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911\n+\n+CREATE TEMPORARY VIEW tenk2 AS SELECT * FROM tenk1;\n+\n+CREATE TABLE empsalary (\n+    depname string,\n+    empno integer,\n+    salary int,\n+    enroll_date date\n+) USING parquet;\n+\n+INSERT INTO empsalary VALUES\n+  ('develop', 10, 5200, date '2007-08-01'),\n+  ('sales', 1, 5000, date '2006-10-01'),\n+  ('personnel', 5, 3500, date '2007-12-10'),\n+  ('sales', 4, 4800, date '2007-08-08'),\n+  ('personnel', 2, 3900, date '2006-12-23'),\n+  ('develop', 7, 4200, date '2008-01-01'),\n+  ('develop', 9, 4500, date '2008-01-01'),\n+  ('sales', 3, 4800, date '2007-08-01'),\n+  ('develop', 8, 6000, date '2006-10-01'),\n+  ('develop', 11, 5200, date '2007-08-15');\n+\n+-- Test in_range for other datetime datatypes\n+\n+-- Spark only supports timestamp\n+-- [SPARK-29636] Spark can't parse '11:00 BST' or '2000-10-19 10:23:54+01' signatures to timestamp\n+create table datetimes (\n+    id int,\n+    f_time timestamp,\n+    f_timetz timestamp,\n+    f_interval timestamp,\n+    f_timestamptz timestamp,\n+    f_timestamp timestamp\n+) using parquet;\n+\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values\n+(1, timestamp '11:00', cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), timestamp '2000-10-19 10:23:54'),\n+(2, timestamp '12:00', cast ('12:00 BST' as timestamp), cast ('2 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(3, timestamp '13:00', cast ('13:00 BST' as timestamp), cast ('3 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(4, timestamp '14:00', cast ('14:00 BST' as timestamp), cast ('4 years' as timestamp), cast ('2002-10-19 10:23:54+01' as timestamp), timestamp '2002-10-19 10:23:54'),\n+(5, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2003-10-19 10:23:54+01' as timestamp), timestamp '2003-10-19 10:23:54'),\n+(6, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2004-10-19 10:23:54+01' as timestamp), timestamp '2004-10-19 10:23:54'),\n+(7, timestamp '17:00', cast ('17:00 BST' as timestamp), cast ('7 years' as timestamp), cast ('2005-10-19 10:23:54+01' as timestamp), timestamp '2005-10-19 10:23:54'),\n+(8, timestamp '18:00', cast ('18:00 BST' as timestamp), cast ('8 years' as timestamp), cast ('2006-10-19 10:23:54+01' as timestamp), timestamp '2006-10-19 10:23:54'),\n+(9, timestamp '19:00', cast ('19:00 BST' as timestamp), cast ('9 years' as timestamp), cast ('2007-10-19 10:23:54+01' as timestamp), timestamp '2007-10-19 10:23:54'),\n+(10, timestamp '20:00', cast ('20:00 BST' as timestamp), cast ('10 years' as timestamp), cast ('2008-10-19 10:23:54+01' as timestamp), timestamp '2008-10-19 10:23:54');\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- RANGE offset PRECEDING/FOLLOWING error cases\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by enroll_date, salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by depname range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between 1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between -1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between 1 preceding and -2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between '1 year' preceding and '-2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- GROUPS tests\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and current row),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between current row and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 following and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and 2 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 preceding),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 0 preceding and 0 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following\n+--   exclude current row), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude group), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude ties), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following),unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude current row), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude group), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude ties), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select first_value(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lead(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28508] Support for range frame+row frame in the same query\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select last(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lag(salary)         over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- select first_value(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude current row),\n+-- lead(salary) over(order by enroll_date groups between 1 following and 3 following exclude ties),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude ties),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select last(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude group),\n+-- lag(salary) over(order by enroll_date groups between 1 following and 3 following exclude group),\n+-- salary, enroll_date from empsalary;\n+\n+-- Show differences in offset interpretation between ROWS, RANGE, and GROUPS\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         SELECT * FROM range(1, 36, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         select 1 union all select 1 union all select 1 union all\n+--         SELECT * FROM range(5, 50, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+-- with UNION\n+\n+SELECT count(*) OVER (PARTITION BY four) FROM (SELECT * FROM tenk1 UNION ALL SELECT * FROM tenk2)s LIMIT 0;\n+\n+-- check some degenerate cases\n+\n+create table t1 (f1 int, f2 int) using parquet;\n+insert into t1 values (1,1),(1,2),(2,2);\n+\n+select f1, sum(f1) over (partition by f1\n+                         range between 1 preceding and 1 following)\n+from t1 where f1 = f2; -- error, must have order by\n+\n+-- Since EXPLAIN clause rely on host physical location, it is commented out\n+-- explain\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+--                          range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+select f1, sum(f1) over (partition by f1, f1 order by f2\n+range between 2 preceding and 1 preceding)\n+from t1 where f1 = f2;\n+\n+select f1, sum(f1) over (partition by f1, f2 order by f2\n+range between 1 following and 2 following)\n+from t1 where f1 = f2;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1,\n+-- groups between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+-- Since EXPLAIN clause rely on host physical location, it is commented out\n+-- explain\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- groups between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1, f1 order by f2\n+-- groups between 2 preceding and 1 preceding)\n+-- from t1 where f1 = f2;\n+ \n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1, f2 order by f2\n+-- groups between 1 following and 2 following)\n+-- from t1 where f1 = f2;\n+\n+-- ordering by a non-integer constant is allowed\n+\n+SELECT rank() OVER (ORDER BY length('abc'));\n+\n+-- can't order by another window function\n+-- [SPARK-28566] window functions should not be allowed in window definitions\n+-- SELECT rank() OVER (ORDER BY rank() OVER (ORDER BY random()));\n+\n+-- some other errors\n+SELECT * FROM empsalary WHERE row_number() OVER (ORDER BY salary) < 10;\n+\n+SELECT * FROM empsalary INNER JOIN tenk1 ON row_number() OVER (ORDER BY salary) < 10;\n+\n+SELECT rank() OVER (ORDER BY 1), count(*) FROM empsalary GROUP BY 1;\n+\n+-- Since random() result may change due to seed issues, the behavior is actually unstable"
  }, {
    "author": {
      "login": "DylanGuedes"
    },
    "body": "No, I was just pointing out that the result in .out file will probably differ from PostgreSQL result.",
    "commit": "c6c16a469268b180175961c13c5f7c88ec54a9bf",
    "createdAt": "2019-11-01T01:58:56Z",
    "diffHunk": "@@ -0,0 +1,455 @@\n+-- Portions Copyright (c) 1996-2019, PostgreSQL Global Development Group\n+--\n+-- Window Functions Testing\n+-- https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911\n+\n+CREATE TEMPORARY VIEW tenk2 AS SELECT * FROM tenk1;\n+\n+CREATE TABLE empsalary (\n+    depname string,\n+    empno integer,\n+    salary int,\n+    enroll_date date\n+) USING parquet;\n+\n+INSERT INTO empsalary VALUES\n+  ('develop', 10, 5200, date '2007-08-01'),\n+  ('sales', 1, 5000, date '2006-10-01'),\n+  ('personnel', 5, 3500, date '2007-12-10'),\n+  ('sales', 4, 4800, date '2007-08-08'),\n+  ('personnel', 2, 3900, date '2006-12-23'),\n+  ('develop', 7, 4200, date '2008-01-01'),\n+  ('develop', 9, 4500, date '2008-01-01'),\n+  ('sales', 3, 4800, date '2007-08-01'),\n+  ('develop', 8, 6000, date '2006-10-01'),\n+  ('develop', 11, 5200, date '2007-08-15');\n+\n+-- Test in_range for other datetime datatypes\n+\n+-- Spark only supports timestamp\n+-- [SPARK-29636] Spark can't parse '11:00 BST' or '2000-10-19 10:23:54+01' signatures to timestamp\n+create table datetimes (\n+    id int,\n+    f_time timestamp,\n+    f_timetz timestamp,\n+    f_interval timestamp,\n+    f_timestamptz timestamp,\n+    f_timestamp timestamp\n+) using parquet;\n+\n+-- Spark cannot safely cast StringType to TimestampType\n+insert into datetimes values\n+(1, timestamp '11:00', cast ('11:00 BST' as timestamp), cast ('1 year' as timestamp), cast ('2000-10-19 10:23:54+01' as timestamp), timestamp '2000-10-19 10:23:54'),\n+(2, timestamp '12:00', cast ('12:00 BST' as timestamp), cast ('2 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(3, timestamp '13:00', cast ('13:00 BST' as timestamp), cast ('3 years' as timestamp), cast ('2001-10-19 10:23:54+01' as timestamp), timestamp '2001-10-19 10:23:54'),\n+(4, timestamp '14:00', cast ('14:00 BST' as timestamp), cast ('4 years' as timestamp), cast ('2002-10-19 10:23:54+01' as timestamp), timestamp '2002-10-19 10:23:54'),\n+(5, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2003-10-19 10:23:54+01' as timestamp), timestamp '2003-10-19 10:23:54'),\n+(6, timestamp '15:00', cast ('15:00 BST' as timestamp), cast ('5 years' as timestamp), cast ('2004-10-19 10:23:54+01' as timestamp), timestamp '2004-10-19 10:23:54'),\n+(7, timestamp '17:00', cast ('17:00 BST' as timestamp), cast ('7 years' as timestamp), cast ('2005-10-19 10:23:54+01' as timestamp), timestamp '2005-10-19 10:23:54'),\n+(8, timestamp '18:00', cast ('18:00 BST' as timestamp), cast ('8 years' as timestamp), cast ('2006-10-19 10:23:54+01' as timestamp), timestamp '2006-10-19 10:23:54'),\n+(9, timestamp '19:00', cast ('19:00 BST' as timestamp), cast ('9 years' as timestamp), cast ('2007-10-19 10:23:54+01' as timestamp), timestamp '2007-10-19 10:23:54'),\n+(10, timestamp '20:00', cast ('20:00 BST' as timestamp), cast ('10 years' as timestamp), cast ('2008-10-19 10:23:54+01' as timestamp), timestamp '2008-10-19 10:23:54');\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_time, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_time desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timetz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timetz desc range between\n+--              '70 min' preceding and '2 hours' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_interval, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_interval desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamptz, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamptz desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- [SPARK-28429] SQL Datetime util function being casted to double instead of timestamp\n+-- select id, f_timestamp, first(id) over w, last(id) over w\n+-- from datetimes\n+-- window w as (order by f_timestamp desc range between\n+--              '1 year' preceding and '1 year' following);\n+\n+-- RANGE offset PRECEDING/FOLLOWING error cases\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by enroll_date, salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select sum(salary) over (order by depname range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between 1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between -1 preceding and 2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between 1 preceding and -2 following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by salary range between '1 year' preceding and '2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select max(enroll_date) over (order by enroll_date range between '1 year' preceding and '-2 years' following\n+-- \texclude ties), salary, enroll_date from empsalary;\n+\n+-- GROUPS tests\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and current row),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between current row and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 preceding and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 1 following and unbounded following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between unbounded preceding and 2 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 preceding),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (order by four groups between 0 preceding and 0 following),\n+-- unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four groups between 2 preceding and 1 following\n+--   exclude current row), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude group), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- SELECT sum(unique1) over (order by four range between 2 preceding and 1 following\n+--   exclude ties), unique1, four\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following),unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude current row), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude group), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- SELECT sum(unique1) over (partition by ten\n+--   order by four groups between 0 preceding and 0 following exclude ties), unique1, four, ten\n+-- FROM tenk1 WHERE unique1 < 10;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select first_value(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lead(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28508] Support for range frame+row frame in the same query\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select last(salary) over(order by enroll_date groups between 1 preceding and 1 following),\n+-- lag(salary)         over(order by enroll_date groups between 1 preceding and 1 following),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-27951] ANSI SQL: NTH_VALUE function\n+-- select first_value(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude current row),\n+-- lead(salary) over(order by enroll_date groups between 1 following and 3 following exclude ties),\n+-- nth_value(salary, 1) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude ties),\n+-- salary, enroll_date from empsalary;\n+\n+-- [SPARK-28428] Spark `exclude` always expecting `()`\n+-- select last(salary) over(order by enroll_date groups between 1 following and 3 following\n+--   exclude group),\n+-- lag(salary) over(order by enroll_date groups between 1 following and 3 following exclude group),\n+-- salary, enroll_date from empsalary;\n+\n+-- Show differences in offset interpretation between ROWS, RANGE, and GROUPS\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        SELECT * FROM range(1, 36, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         SELECT * FROM range(1, 36, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x rows between 1 preceding and 1 following);\n+\n+WITH cte (x) AS (\n+        select 1 union all select 1 union all select 1 union all\n+        SELECT * FROM range(5, 50, 2)\n+)\n+SELECT x, (sum(x) over w)\n+FROM cte\n+WINDOW w AS (ORDER BY x range between 1 preceding and 1 following);\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- WITH cte (x) AS (\n+--         select 1 union all select 1 union all select 1 union all\n+--         SELECT * FROM range(5, 50, 2)\n+-- )\n+-- SELECT x, (sum(x) over w)\n+-- FROM cte\n+-- WINDOW w AS (ORDER BY x groups between 1 preceding and 1 following);\n+\n+-- with UNION\n+\n+SELECT count(*) OVER (PARTITION BY four) FROM (SELECT * FROM tenk1 UNION ALL SELECT * FROM tenk2)s LIMIT 0;\n+\n+-- check some degenerate cases\n+\n+create table t1 (f1 int, f2 int) using parquet;\n+insert into t1 values (1,1),(1,2),(2,2);\n+\n+select f1, sum(f1) over (partition by f1\n+                         range between 1 preceding and 1 following)\n+from t1 where f1 = f2; -- error, must have order by\n+\n+-- Since EXPLAIN clause rely on host physical location, it is commented out\n+-- explain\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+--                          range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+select f1, sum(f1) over (partition by f1, f1 order by f2\n+range between 2 preceding and 1 preceding)\n+from t1 where f1 = f2;\n+\n+select f1, sum(f1) over (partition by f1, f2 order by f2\n+range between 1 following and 2 following)\n+from t1 where f1 = f2;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1,\n+-- groups between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+-- Since EXPLAIN clause rely on host physical location, it is commented out\n+-- explain\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- range between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1 order by f2\n+-- groups between 1 preceding and 1 following)\n+-- from t1 where f1 = f2;\n+\n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1, f1 order by f2\n+-- groups between 2 preceding and 1 preceding)\n+-- from t1 where f1 = f2;\n+ \n+-- [SPARK-28648] Adds support to `groups` unit type in window clauses\n+-- select f1, sum(f1) over (partition by f1, f2 order by f2\n+-- groups between 1 following and 2 following)\n+-- from t1 where f1 = f2;\n+\n+-- ordering by a non-integer constant is allowed\n+\n+SELECT rank() OVER (ORDER BY length('abc'));\n+\n+-- can't order by another window function\n+-- [SPARK-28566] window functions should not be allowed in window definitions\n+-- SELECT rank() OVER (ORDER BY rank() OVER (ORDER BY random()));\n+\n+-- some other errors\n+SELECT * FROM empsalary WHERE row_number() OVER (ORDER BY salary) < 10;\n+\n+SELECT * FROM empsalary INNER JOIN tenk1 ON row_number() OVER (ORDER BY salary) < 10;\n+\n+SELECT rank() OVER (ORDER BY 1), count(*) FROM empsalary GROUP BY 1;\n+\n+-- Since random() result may change due to seed issues, the behavior is actually unstable"
  }],
  "prId": 26274
}]