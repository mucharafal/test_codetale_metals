[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "what's the difference between data filters and pushed filters?",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-08T06:28:22Z",
    "diffHunk": "@@ -58,6 +58,12 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+DataFilters: [isnotnull(key#x), (key#x > 0)]\n+Format: Parquet\n+Location [not included in comparison]sql/core/spark-warehouse/[not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,0)]",
    "line": 6
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@cloud-fan Actually i don't know for sure. Looking at the output, could it be one is the catalyst's view of the filter and the other is the datasource's view of the filter i.e after we translate it ? I am guessing here :-)",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-08T07:10:52Z",
    "diffHunk": "@@ -58,6 +58,12 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+DataFilters: [isnotnull(key#x), (key#x > 0)]\n+Format: Parquet\n+Location [not included in comparison]sql/core/spark-warehouse/[not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,0)]",
    "line": 6
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@cloud-fan Checked the code. Our guess was right - fyi\r\n```\r\nprivate val pushedDownFilters = dataFilters.flatMap(DataSourceStrategy.translateFilter)\r\n  logInfo(s\"Pushed Filters: ${pushedDownFilters.mkString(\",\")}\")\r\n```",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-08T07:20:45Z",
    "diffHunk": "@@ -58,6 +58,12 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+DataFilters: [isnotnull(key#x), (key#x > 0)]\n+Format: Parquet\n+Location [not included in comparison]sql/core/spark-warehouse/[not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,0)]",
    "line": 6
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Then I think we only need to mention pushedDownFilters and DPP filters. Other filters are ignored/has no effect.",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-08T08:21:14Z",
    "diffHunk": "@@ -58,6 +58,12 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+DataFilters: [isnotnull(key#x), (key#x > 0)]\n+Format: Parquet\n+Location [not included in comparison]sql/core/spark-warehouse/[not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,0)]",
    "line": 6
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@cloud-fan OK.",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-08T14:02:54Z",
    "diffHunk": "@@ -58,6 +58,12 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+DataFilters: [isnotnull(key#x), (key#x > 0)]\n+Format: Parquet\n+Location [not included in comparison]sql/core/spark-warehouse/[not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,0)]",
    "line": 6
  }],
  "prId": 26042
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This is already in the node name `Scan parquet default.explain_temp1`",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-10T07:44:23Z",
    "diffHunk": "@@ -58,6 +58,11 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+Format: Parquet"
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@cloud-fan I can filter it. But wanted to double check first. The one we print in the node name is `relation.toString` and the one printed here is `relation.format.toString`. Are they going to be same always ?",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-10T18:25:46Z",
    "diffHunk": "@@ -58,6 +58,11 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+Format: Parquet"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "`relation.toString` always contain the format name AFAIK.",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-15T02:30:00Z",
    "diffHunk": "@@ -58,6 +58,11 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+Format: Parquet"
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@cloud-fan OK.. sounds good wenchen. I will drop this then.",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-15T04:44:48Z",
    "diffHunk": "@@ -58,6 +58,11 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+Format: Parquet"
  }],
  "prId": 26042
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Can we have a test case to show the DPP filter?",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-10T07:46:36Z",
    "diffHunk": "@@ -654,6 +740,11 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+Format: Parquet\n+Location [not included in comparison]sql/core/spark-warehouse/[not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,10)]\n+ReadSchema: struct<key:int,val:int>",
    "line": 200
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@cloud-fan I have enhanced the test in ExplainSuite.",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-11T17:58:42Z",
    "diffHunk": "@@ -654,6 +740,11 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+Format: Parquet\n+Location [not included in comparison]sql/core/spark-warehouse/[not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,10)]\n+ReadSchema: struct<key:int,val:int>",
    "line": 200
  }],
  "prId": 26042
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "do we have a test case to display the DPP filter?",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-15T07:16:53Z",
    "diffHunk": "@@ -110,6 +114,10 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+Location [not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,0)]",
    "line": 17
  }, {
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@cloud-fan I have enhanced the test in ExplainSuite to test the DPP filter. What do you think ? Perhaps we need some data in order to trigger it ? I am using the test @gatorsmile mentioned in the JIRA in ExplainSuite.",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-15T07:22:12Z",
    "diffHunk": "@@ -110,6 +114,10 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+Location [not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,0)]",
    "line": 17
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I'm asking it because I don't see where we extract the DPP filter in this PR. You can see from `FileSourceScanExec#dynamicallySelectedPartitions` that we can get DPP filter by `partitionFilters.filter(isDynamicPruningFilter)`.",
    "commit": "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "createdAt": "2019-10-15T07:47:13Z",
    "diffHunk": "@@ -110,6 +114,10 @@ struct<plan:string>\n \n (1) Scan parquet default.explain_temp1 \n Output: [key#x, val#x]\n+Batched: true\n+Location [not included in comparison]\n+PushedFilters: [IsNotNull(key), GreaterThan(key,0)]",
    "line": 17
  }],
  "prId": 26042
}]