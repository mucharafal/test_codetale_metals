[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Hmm ...\r\n\r\n```diff\r\n -- !query 3\r\n-SELECT val_long, cate, sum(val_long) OVER(PARTITION BY cate ORDER BY val_long\r\n-ROWS BETWEEN CURRENT ROW AND 2147483648 FOLLOWING) FROM testData ORDER BY cate, val_long\r\n+SELECT val_long, udf(cate), sum(val_long) OVER(PARTITION BY cate ORDER BY val_long\r\n+ROWS BETWEEN CURRENT ROW AND CAST(2147483648 AS int) FOLLOWING) FROM testData ORDER BY cate, val_long\r\n -- !query 3 schema\r\n-struct<>\r\n+struct<val_long:bigint,CAST(udf(cast(cate as string)) AS STRING):string,sum(val_long) OVER (PARTITION BY cate ORDER BY val_long ASC NULLS FIRST ROWS BETWEEN CURRENT ROW AND CAST(2147483648 AS INT) FOLLOWING):bigint>\r\n -- !query 3 output\r\n-org.apache.spark.sql.AnalysisException\r\n-cannot resolve 'ROWS BETWEEN CURRENT ROW AND 2147483648L FOLLOWING' due to data type mismatch: The data type of the upper bound 'bigint' does not match the expected data type 'int'.; line 1 pos 41\r\n+NULL   NULL    1\r\n+1      NULL    1\r\n+1      a       2147483654\r\n+1      a       2147483653\r\n+2      a       2147483652\r\n+2147483650     a       2147483650\r\n+NULL   b       2147483653\r\n+3      b       2147483653\r\n+2147483650     b       2147483650\r\n```\r\n\r\nDo you know why this works when it's wrapped by udf?",
    "commit": "e6ce51a7a8039b14326605adb4b3c57fad9557ef",
    "createdAt": "2019-07-24T00:49:33Z",
    "diffHunk": "@@ -0,0 +1,389 @@\n+-- Automatically generated by SQLQueryTestSuite\n+-- Number of queries: 23\n+\n+\n+-- !query 0\n+CREATE OR REPLACE TEMPORARY VIEW testData AS SELECT * FROM VALUES\n+(null, 1L, 1.0D, date(\"2017-08-01\"), timestamp(1501545600), \"a\"),\n+(1, 1L, 1.0D, date(\"2017-08-01\"), timestamp(1501545600), \"a\"),\n+(1, 2L, 2.5D, date(\"2017-08-02\"), timestamp(1502000000), \"a\"),\n+(2, 2147483650L, 100.001D, date(\"2020-12-31\"), timestamp(1609372800), \"a\"),\n+(1, null, 1.0D, date(\"2017-08-01\"), timestamp(1501545600), \"b\"),\n+(2, 3L, 3.3D, date(\"2017-08-03\"), timestamp(1503000000), \"b\"),\n+(3, 2147483650L, 100.001D, date(\"2020-12-31\"), timestamp(1609372800), \"b\"),\n+(null, null, null, null, null, null),\n+(3, 1L, 1.0D, date(\"2017-08-01\"), timestamp(1501545600), null)\n+AS testData(val, val_long, val_double, val_date, val_timestamp, cate)\n+-- !query 0 schema\n+struct<>\n+-- !query 0 output\n+\n+\n+\n+-- !query 1\n+SELECT udf(val), cate, count(val) OVER(PARTITION BY cate ORDER BY val ROWS CURRENT ROW) FROM testData\n+ORDER BY cate, val\n+-- !query 1 schema\n+struct<CAST(udf(cast(val as string)) AS INT):int,cate:string,count(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST ROWS BETWEEN CURRENT ROW AND CURRENT ROW):bigint>\n+-- !query 1 output\n+NULL\tNULL\t0\n+3\tNULL\t1\n+NULL\ta\t0\n+1\ta\t1\n+1\ta\t1\n+2\ta\t1\n+1\tb\t1\n+2\tb\t1\n+3\tb\t1\n+\n+\n+-- !query 2\n+SELECT udf(val), cate, sum(val) OVER(PARTITION BY cate ORDER BY val\n+ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING) FROM testData ORDER BY cate, val\n+-- !query 2 schema\n+struct<CAST(udf(cast(val as string)) AS INT):int,cate:string,sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING):bigint>\n+-- !query 2 output\n+NULL\tNULL\t3\n+3\tNULL\t3\n+NULL\ta\t1\n+1\ta\t2\n+1\ta\t4\n+2\ta\t4\n+1\tb\t3\n+2\tb\t6\n+3\tb\t6\n+\n+\n+-- !query 3",
    "line": 57
  }, {
    "author": {
      "login": "younggyuchun"
    },
    "body": "as I put CAST function:\r\nCAST(2147483648 AS int)\r\n\r\nbut I put it back to 2147483648 ",
    "commit": "e6ce51a7a8039b14326605adb4b3c57fad9557ef",
    "createdAt": "2019-07-24T19:18:31Z",
    "diffHunk": "@@ -0,0 +1,389 @@\n+-- Automatically generated by SQLQueryTestSuite\n+-- Number of queries: 23\n+\n+\n+-- !query 0\n+CREATE OR REPLACE TEMPORARY VIEW testData AS SELECT * FROM VALUES\n+(null, 1L, 1.0D, date(\"2017-08-01\"), timestamp(1501545600), \"a\"),\n+(1, 1L, 1.0D, date(\"2017-08-01\"), timestamp(1501545600), \"a\"),\n+(1, 2L, 2.5D, date(\"2017-08-02\"), timestamp(1502000000), \"a\"),\n+(2, 2147483650L, 100.001D, date(\"2020-12-31\"), timestamp(1609372800), \"a\"),\n+(1, null, 1.0D, date(\"2017-08-01\"), timestamp(1501545600), \"b\"),\n+(2, 3L, 3.3D, date(\"2017-08-03\"), timestamp(1503000000), \"b\"),\n+(3, 2147483650L, 100.001D, date(\"2020-12-31\"), timestamp(1609372800), \"b\"),\n+(null, null, null, null, null, null),\n+(3, 1L, 1.0D, date(\"2017-08-01\"), timestamp(1501545600), null)\n+AS testData(val, val_long, val_double, val_date, val_timestamp, cate)\n+-- !query 0 schema\n+struct<>\n+-- !query 0 output\n+\n+\n+\n+-- !query 1\n+SELECT udf(val), cate, count(val) OVER(PARTITION BY cate ORDER BY val ROWS CURRENT ROW) FROM testData\n+ORDER BY cate, val\n+-- !query 1 schema\n+struct<CAST(udf(cast(val as string)) AS INT):int,cate:string,count(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST ROWS BETWEEN CURRENT ROW AND CURRENT ROW):bigint>\n+-- !query 1 output\n+NULL\tNULL\t0\n+3\tNULL\t1\n+NULL\ta\t0\n+1\ta\t1\n+1\ta\t1\n+2\ta\t1\n+1\tb\t1\n+2\tb\t1\n+3\tb\t1\n+\n+\n+-- !query 2\n+SELECT udf(val), cate, sum(val) OVER(PARTITION BY cate ORDER BY val\n+ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING) FROM testData ORDER BY cate, val\n+-- !query 2 schema\n+struct<CAST(udf(cast(val as string)) AS INT):int,cate:string,sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING):bigint>\n+-- !query 2 output\n+NULL\tNULL\t3\n+3\tNULL\t3\n+NULL\ta\t1\n+1\ta\t2\n+1\ta\t4\n+2\ta\t4\n+1\tb\t3\n+2\tb\t6\n+3\tb\t6\n+\n+\n+-- !query 3",
    "line": 57
  }],
  "prId": 25195
}]