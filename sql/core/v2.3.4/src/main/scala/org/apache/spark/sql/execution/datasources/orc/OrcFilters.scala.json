[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "All the functions in this object are different between v2.3.4 and v1.2?",
    "commit": "11bc98284566ae93caffa7d947543c095de03c75",
    "createdAt": "2019-03-27T05:21:45Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc\n+\n+import org.apache.orc.storage.common.`type`.HiveDecimal\n+import org.apache.orc.storage.ql.io.sarg.{PredicateLeaf, SearchArgument}\n+import org.apache.orc.storage.ql.io.sarg.SearchArgument.Builder\n+import org.apache.orc.storage.ql.io.sarg.SearchArgumentFactory.newBuilder\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable\n+\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Helper object for building ORC `SearchArgument`s, which are used for ORC predicate push-down.\n+ *\n+ * Due to limitation of ORC `SearchArgument` builder, we had to end up with a pretty weird double-\n+ * checking pattern when converting `And`/`Or`/`Not` filters.\n+ *\n+ * An ORC `SearchArgument` must be built in one pass using a single builder.  For example, you can't\n+ * build `a = 1` and `b = 2` first, and then combine them into `a = 1 AND b = 2`.  This is quite\n+ * different from the cases in Spark SQL or Parquet, where complex filters can be easily built using\n+ * existing simpler ones.\n+ *\n+ * The annoying part is that, `SearchArgument` builder methods like `startAnd()`, `startOr()`, and\n+ * `startNot()` mutate internal state of the builder instance.  This forces us to translate all\n+ * convertible filters with a single builder instance. However, before actually converting a filter,\n+ * we've no idea whether it can be recognized by ORC or not. Thus, when an inconvertible filter is\n+ * found, we may already end up with a builder whose internal state is inconsistent.\n+ *\n+ * For example, to convert an `And` filter with builder `b`, we call `b.startAnd()` first, and then\n+ * try to convert its children.  Say we convert `left` child successfully, but find that `right`\n+ * child is inconvertible.  Alas, `b.startAnd()` call can't be rolled back, and `b` is inconsistent\n+ * now.\n+ *\n+ * The workaround employed here is that, for `And`/`Or`/`Not`, we first try to convert their\n+ * children with brand new builders, and only do the actual conversion with the right builder\n+ * instance when the children are proven to be convertible.\n+ *\n+ * P.S.: Hive seems to use `SearchArgument` together with `ExprNodeGenericFuncDesc` only.  Usage of\n+ * builder methods mentioned above can only be found in test code, where all tested filters are\n+ * known to be convertible.\n+ */\n+private[sql] object OrcFilters extends OrcFiltersBase {",
    "line": 59
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Talk with @wangyum offline. All these functions need a change in v2.3.4. \r\n\r\n",
    "commit": "11bc98284566ae93caffa7d947543c095de03c75",
    "createdAt": "2019-03-27T05:31:15Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.orc\n+\n+import org.apache.orc.storage.common.`type`.HiveDecimal\n+import org.apache.orc.storage.ql.io.sarg.{PredicateLeaf, SearchArgument}\n+import org.apache.orc.storage.ql.io.sarg.SearchArgument.Builder\n+import org.apache.orc.storage.ql.io.sarg.SearchArgumentFactory.newBuilder\n+import org.apache.orc.storage.serde2.io.HiveDecimalWritable\n+\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Helper object for building ORC `SearchArgument`s, which are used for ORC predicate push-down.\n+ *\n+ * Due to limitation of ORC `SearchArgument` builder, we had to end up with a pretty weird double-\n+ * checking pattern when converting `And`/`Or`/`Not` filters.\n+ *\n+ * An ORC `SearchArgument` must be built in one pass using a single builder.  For example, you can't\n+ * build `a = 1` and `b = 2` first, and then combine them into `a = 1 AND b = 2`.  This is quite\n+ * different from the cases in Spark SQL or Parquet, where complex filters can be easily built using\n+ * existing simpler ones.\n+ *\n+ * The annoying part is that, `SearchArgument` builder methods like `startAnd()`, `startOr()`, and\n+ * `startNot()` mutate internal state of the builder instance.  This forces us to translate all\n+ * convertible filters with a single builder instance. However, before actually converting a filter,\n+ * we've no idea whether it can be recognized by ORC or not. Thus, when an inconvertible filter is\n+ * found, we may already end up with a builder whose internal state is inconsistent.\n+ *\n+ * For example, to convert an `And` filter with builder `b`, we call `b.startAnd()` first, and then\n+ * try to convert its children.  Say we convert `left` child successfully, but find that `right`\n+ * child is inconvertible.  Alas, `b.startAnd()` call can't be rolled back, and `b` is inconsistent\n+ * now.\n+ *\n+ * The workaround employed here is that, for `And`/`Or`/`Not`, we first try to convert their\n+ * children with brand new builders, and only do the actual conversion with the right builder\n+ * instance when the children are proven to be convertible.\n+ *\n+ * P.S.: Hive seems to use `SearchArgument` together with `ExprNodeGenericFuncDesc` only.  Usage of\n+ * builder methods mentioned above can only be found in test code, where all tested filters are\n+ * known to be convertible.\n+ */\n+private[sql] object OrcFilters extends OrcFiltersBase {",
    "line": 59
  }],
  "prId": 24119
}]