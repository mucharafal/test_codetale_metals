[{
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Why?\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-09-19T21:03:16Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import scala.language.implicitConversions\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import scala.collection.JavaConversions._\n+import org.apache.spark.Logging\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.Partition\n+import org.apache.hadoop.{io => hadoopIo}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.mapred.InputFormat\n+import java.util.Properties\n+import org.apache.hadoop.hive.serde2.Deserializer\n+\n+/*hive-0.13.1 support shimmer layer*/\n+object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+  def getTableDesc(serdeClass: Class[_ <: Deserializer], inputFormatClass: Class[_ <: InputFormat[_, _]], outputFormatClass: Class[_], properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+  /*handle the difference in HiveQuerySuite*/\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def convertCatalystString2Hive(s: String) = s\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   * */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty",
    "line": 106
  }, {
    "author": {
      "login": "zhzhan"
    },
    "body": "I didn't see appendReadColumns handles empty strings, and met error in this case during testing.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-09-22T05:35:35Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import scala.language.implicitConversions\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import scala.collection.JavaConversions._\n+import org.apache.spark.Logging\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.Partition\n+import org.apache.hadoop.{io => hadoopIo}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.mapred.InputFormat\n+import java.util.Properties\n+import org.apache.hadoop.hive.serde2.Deserializer\n+\n+/*hive-0.13.1 support shimmer layer*/\n+object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+  def getTableDesc(serdeClass: Class[_ <: Deserializer], inputFormatClass: Class[_ <: InputFormat[_, _]], outputFormatClass: Class[_], properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+  /*handle the difference in HiveQuerySuite*/\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def convertCatalystString2Hive(s: String) = s\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   * */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty",
    "line": 106
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Why in else?\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-09-19T21:03:34Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import scala.language.implicitConversions\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import scala.collection.JavaConversions._\n+import org.apache.spark.Logging\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.Partition\n+import org.apache.hadoop.{io => hadoopIo}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.mapred.InputFormat\n+import java.util.Properties\n+import org.apache.hadoop.hive.serde2.Deserializer\n+\n+/*hive-0.13.1 support shimmer layer*/\n+object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+  def getTableDesc(serdeClass: Class[_ <: Deserializer], inputFormatClass: Class[_ <: InputFormat[_, _]], outputFormatClass: Class[_], properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+  /*handle the difference in HiveQuerySuite*/\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def convertCatalystString2Hive(s: String) = s\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   * */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   * */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    } else {\n+      appendReadColumnNames(conf, names)"
  }, {
    "author": {
      "login": "zhzhan"
    },
    "body": "good catch\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-09-22T05:46:32Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import scala.language.implicitConversions\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import scala.collection.JavaConversions._\n+import org.apache.spark.Logging\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.Partition\n+import org.apache.hadoop.{io => hadoopIo}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.mapred.InputFormat\n+import java.util.Properties\n+import org.apache.hadoop.hive.serde2.Deserializer\n+\n+/*hive-0.13.1 support shimmer layer*/\n+object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+  def getTableDesc(serdeClass: Class[_ <: Deserializer], inputFormatClass: Class[_ <: InputFormat[_, _]], outputFormatClass: Class[_], properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+  /*handle the difference in HiveQuerySuite*/\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def convertCatalystString2Hive(s: String) = s\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   * */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   * */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    } else {\n+      appendReadColumnNames(conf, names)"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Fix formatting. Add TODO.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-09-19T22:19:27Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import scala.language.implicitConversions\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import scala.collection.JavaConversions._\n+import org.apache.spark.Logging\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.Partition\n+import org.apache.hadoop.{io => hadoopIo}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.mapred.InputFormat\n+import java.util.Properties\n+import org.apache.hadoop.hive.serde2.Deserializer\n+\n+/*hive-0.13.1 support shimmer layer*/\n+object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Fix formatting.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-09-19T22:20:27Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import scala.language.implicitConversions\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import scala.collection.JavaConversions._\n+import org.apache.spark.Logging\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.Partition\n+import org.apache.hadoop.{io => hadoopIo}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.mapred.InputFormat\n+import java.util.Properties\n+import org.apache.hadoop.hive.serde2.Deserializer\n+\n+/*hive-0.13.1 support shimmer layer*/\n+object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+  def getTableDesc(serdeClass: Class[_ <: Deserializer], inputFormatClass: Class[_ <: InputFormat[_, _]], outputFormatClass: Class[_], properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+  /*handle the difference in HiveQuerySuite*/\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def convertCatalystString2Hive(s: String) = s\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   * */"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Insert newlines in between all these methods.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-09-19T22:22:04Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import scala.language.implicitConversions\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import scala.collection.JavaConversions._\n+import org.apache.spark.Logging\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.Partition\n+import org.apache.hadoop.{io => hadoopIo}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.mapred.InputFormat\n+import java.util.Properties\n+import org.apache.hadoop.hive.serde2.Deserializer\n+\n+/*hive-0.13.1 support shimmer layer*/\n+object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+  def getTableDesc(serdeClass: Class[_ <: Deserializer], inputFormatClass: Class[_ <: InputFormat[_, _]], outputFormatClass: Class[_], properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Why did you choose inheritance for the Context but implicits for the FileSinkDesc?  I'm pretty wary of implicits that do any massaging of types...\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-09-19T22:23:46Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import scala.language.implicitConversions\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import scala.collection.JavaConversions._\n+import org.apache.spark.Logging\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.Partition\n+import org.apache.hadoop.{io => hadoopIo}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.mapred.InputFormat\n+import java.util.Properties\n+import org.apache.hadoop.hive.serde2.Deserializer\n+\n+/*hive-0.13.1 support shimmer layer*/\n+object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+  def getTableDesc(serdeClass: Class[_ <: Deserializer], inputFormatClass: Class[_ <: InputFormat[_, _]], outputFormatClass: Class[_], properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+  /*handle the difference in HiveQuerySuite*/\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def convertCatalystString2Hive(s: String) = s\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   * */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   * */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    } else {\n+      appendReadColumnNames(conf, names)\n+    }\n+  }\n+\n+  /*\n+   * Bug introdiced in hive-0.13. FileSinkDesc is serilizable, but its member path is not.\n+   * Fix it through wrapper.\n+   * */\n+  implicit def wrapperToFileSinkDesc(w: ShimFileSinkDesc): FileSinkDesc = {\n+    var f = new FileSinkDesc(new Path(w.dir), w.tableInfo, w.compressed)\n+    f.setCompressed(w.compressed)\n+    f.setCompressCodec(w.compressCodec)\n+    f.setCompressType(w.compressType)\n+    f.setTableInfo(w.tableInfo)\n+    f\n+  }\n+\n+  implicit class wrapperToPartition(p: Partition) {\n+    def getDataLocationPath: Path = p.getDataLocation\n+  }\n+}\n+\n+class ShimContext(conf: Configuration) extends Context(conf) {\n+  def  getExternalTmpPath(path: Path): Path = {\n+    super.getExternalTmpPath (path.toUri)\n+  }\n+}\n+\n+class ShimFileSinkDesc(var dir: String, var tableInfo: TableDesc, var compressed: Boolean)"
  }, {
    "author": {
      "login": "zhzhan"
    },
    "body": "Will remove ShimContext and implement it in HiveShim for consistency. ShimFileSinkDesc is a walkaround for the bug of FileSinkDesc introduced in hive 0.13.1. Comparing to other techniques, this way is straightforward. Otherwise, we have to somehow serialize the path in the driver side, and reconstruct it in the executor side, which is more complicated. \n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-09-22T05:59:28Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import scala.language.implicitConversions\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import scala.collection.JavaConversions._\n+import org.apache.spark.Logging\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.Partition\n+import org.apache.hadoop.{io => hadoopIo}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.mapred.InputFormat\n+import java.util.Properties\n+import org.apache.hadoop.hive.serde2.Deserializer\n+\n+/*hive-0.13.1 support shimmer layer*/\n+object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+  def getTableDesc(serdeClass: Class[_ <: Deserializer], inputFormatClass: Class[_ <: InputFormat[_, _]], outputFormatClass: Class[_], properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+  /*handle the difference in HiveQuerySuite*/\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def convertCatalystString2Hive(s: String) = s\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   * */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   * */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    } else {\n+      appendReadColumnNames(conf, names)\n+    }\n+  }\n+\n+  /*\n+   * Bug introdiced in hive-0.13. FileSinkDesc is serilizable, but its member path is not.\n+   * Fix it through wrapper.\n+   * */\n+  implicit def wrapperToFileSinkDesc(w: ShimFileSinkDesc): FileSinkDesc = {\n+    var f = new FileSinkDesc(new Path(w.dir), w.tableInfo, w.compressed)\n+    f.setCompressed(w.compressed)\n+    f.setCompressCodec(w.compressCodec)\n+    f.setCompressType(w.compressType)\n+    f.setTableInfo(w.tableInfo)\n+    f\n+  }\n+\n+  implicit class wrapperToPartition(p: Partition) {\n+    def getDataLocationPath: Path = p.getDataLocation\n+  }\n+}\n+\n+class ShimContext(conf: Configuration) extends Context(conf) {\n+  def  getExternalTmpPath(path: Path): Path = {\n+    super.getExternalTmpPath (path.toUri)\n+  }\n+}\n+\n+class ShimFileSinkDesc(var dir: String, var tableInfo: TableDesc, var compressed: Boolean)"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "I am pretty confused about it. I think Hive needs to serialize FileSinkDesc when the query plan needs to be serialized. So, how does Hive handle this issue?\n\nAlso, serilizable=>serializable.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-04T14:34:46Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+\n+  def getTableDesc(\n+    serdeClass: Class[_ <: Deserializer],\n+    inputFormatClass: Class[_ <: InputFormat[_, _]],\n+    outputFormatClass: Class[_],\n+    properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+\n+  /** The string used to denote an empty comments field in the schema. */\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    }\n+    appendReadColumnNames(conf, names)\n+  }\n+\n+  def getExternalTmpPath(context: Context, path: Path) = {\n+    context.getExternalTmpPath(path.toUri)\n+  }\n+\n+  def getDataLocationPath(p: Partition) = p.getDataLocation\n+\n+  def getAllPartitionsOf(client: Hive, tbl: Table) =  client.getAllPartitionsOf(tbl)\n+\n+  /*\n+   * Bug introdiced in hive-0.13. FileSinkDesc is serilizable, but its member path is not.\n+   * Fix it through wrapper."
  }, {
    "author": {
      "login": "zhzhan"
    },
    "body": "Not sure how hive handle this issue, but FileSinkDesc is certainly not serializable with Path variable. Test suite also complains this issue.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-05T08:18:36Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+\n+  def getTableDesc(\n+    serdeClass: Class[_ <: Deserializer],\n+    inputFormatClass: Class[_ <: InputFormat[_, _]],\n+    outputFormatClass: Class[_],\n+    properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+\n+  /** The string used to denote an empty comments field in the schema. */\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    }\n+    appendReadColumnNames(conf, names)\n+  }\n+\n+  def getExternalTmpPath(context: Context, path: Path) = {\n+    context.getExternalTmpPath(path.toUri)\n+  }\n+\n+  def getDataLocationPath(p: Partition) = p.getDataLocation\n+\n+  def getAllPartitionsOf(client: Hive, tbl: Table) =  client.getAllPartitionsOf(tbl)\n+\n+  /*\n+   * Bug introdiced in hive-0.13. FileSinkDesc is serilizable, but its member path is not.\n+   * Fix it through wrapper."
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Seems it is not necessary.\n\nAlso, 2 space indentation.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-04T14:45:16Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+\n+  def getTableDesc(\n+    serdeClass: Class[_ <: Deserializer],\n+    inputFormatClass: Class[_ <: InputFormat[_, _]],\n+    outputFormatClass: Class[_],\n+    properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+\n+  /** The string used to denote an empty comments field in the schema. */\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    }\n+    appendReadColumnNames(conf, names)\n+  }\n+\n+  def getExternalTmpPath(context: Context, path: Path) = {\n+    context.getExternalTmpPath(path.toUri)\n+  }\n+\n+  def getDataLocationPath(p: Partition) = p.getDataLocation\n+\n+  def getAllPartitionsOf(client: Hive, tbl: Table) =  client.getAllPartitionsOf(tbl)\n+\n+  /*\n+   * Bug introdiced in hive-0.13. FileSinkDesc is serilizable, but its member path is not.\n+   * Fix it through wrapper.\n+   * */\n+  implicit def wrapperToFileSinkDesc(w: ShimFileSinkDesc): FileSinkDesc = {\n+        var f = new FileSinkDesc(new Path(w.dir), w.tableInfo, w.compressed)\n+        f.setCompressed(w.compressed)"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "If we need to use `ShimFileSinkDesc`, how about we add a method in it to return the actual `FileSinkDesc` instead of using an implicit method?\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-04T14:52:46Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+\n+  def getTableDesc(\n+    serdeClass: Class[_ <: Deserializer],\n+    inputFormatClass: Class[_ <: InputFormat[_, _]],\n+    outputFormatClass: Class[_],\n+    properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+\n+  /** The string used to denote an empty comments field in the schema. */\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    }\n+    appendReadColumnNames(conf, names)\n+  }\n+\n+  def getExternalTmpPath(context: Context, path: Path) = {\n+    context.getExternalTmpPath(path.toUri)\n+  }\n+\n+  def getDataLocationPath(p: Partition) = p.getDataLocation\n+\n+  def getAllPartitionsOf(client: Hive, tbl: Table) =  client.getAllPartitionsOf(tbl)\n+\n+  /*\n+   * Bug introdiced in hive-0.13. FileSinkDesc is serilizable, but its member path is not.\n+   * Fix it through wrapper.\n+   * */\n+  implicit def wrapperToFileSinkDesc(w: ShimFileSinkDesc): FileSinkDesc = {"
  }, {
    "author": {
      "login": "zhzhan"
    },
    "body": "I thought about this method, but it potentially make the ShimFileSinkDesc harder to maintain in case we need to track some internal state of FileSinkDesc. If you think returning actual FileSinkDesc is better, please let me know.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-05T08:16:37Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+\n+  def getTableDesc(\n+    serdeClass: Class[_ <: Deserializer],\n+    inputFormatClass: Class[_ <: InputFormat[_, _]],\n+    outputFormatClass: Class[_],\n+    properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+\n+  /** The string used to denote an empty comments field in the schema. */\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    }\n+    appendReadColumnNames(conf, names)\n+  }\n+\n+  def getExternalTmpPath(context: Context, path: Path) = {\n+    context.getExternalTmpPath(path.toUri)\n+  }\n+\n+  def getDataLocationPath(p: Partition) = p.getDataLocation\n+\n+  def getAllPartitionsOf(client: Hive, tbl: Table) =  client.getAllPartitionsOf(tbl)\n+\n+  /*\n+   * Bug introdiced in hive-0.13. FileSinkDesc is serilizable, but its member path is not.\n+   * Fix it through wrapper.\n+   * */\n+  implicit def wrapperToFileSinkDesc(w: ShimFileSinkDesc): FileSinkDesc = {"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "```\nif () {\n  ...\n} else {\n  ...\n}\n```\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-04T15:04:32Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+\n+  def getTableDesc(\n+    serdeClass: Class[_ <: Deserializer],\n+    inputFormatClass: Class[_ <: InputFormat[_, _]],\n+    outputFormatClass: Class[_],\n+    properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+\n+  /** The string used to denote an empty comments field in the schema. */\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Why no null and empty check at here?\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-04T15:05:24Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\"\n+\n+  def getTableDesc(\n+    serdeClass: Class[_ <: Deserializer],\n+    inputFormatClass: Class[_ <: InputFormat[_, _]],\n+    outputFormatClass: Class[_],\n+    properties: Properties) = {\n+    new TableDesc(inputFormatClass, outputFormatClass, properties)\n+  }\n+\n+  def getStatsSetupConstTotalSize = StatsSetupConst.TOTAL_SIZE\n+\n+  def createDefaultDBIfNeeded(context: HiveContext) ={\n+    context.runSqlHive(\"CREATE DATABASE default\")\n+    context.runSqlHive(\"USE default\")\n+  }\n+\n+  /** The string used to denote an empty comments field in the schema. */\n+  def getEmptyCommentsFieldValue = \"\"\n+\n+  def getCommandProcessor(cmd: Array[String], conf: HiveConf) =  {\n+    CommandProcessorFactory.get(cmd, conf)\n+  }\n+\n+  def createDecimal(bd: java.math.BigDecimal): HiveDecimal = {\n+    HiveDecimal.create(bd)\n+  }\n+\n+  /*\n+   * This function in hive-0.13 become private, but we have to do this to walkaround hive bug\n+   */\n+  private def appendReadColumnNames(conf: Configuration, cols: Seq[String]) {\n+    val old: String = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, \"\")\n+    val result: StringBuilder = new StringBuilder(old)\n+    var first: Boolean = old.isEmpty\n+\n+    for (col <- cols) {\n+      if (first) {\n+        first = false\n+      }\n+      else {\n+        result.append(',')\n+      }\n+      result.append(col)\n+    }\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, result.toString)\n+  }\n+\n+  /*\n+   * Cannot use ColumnProjectionUtils.appendReadColumns directly, if ids is null or empty\n+   */\n+  def appendReadColumns(conf: Configuration, ids: Seq[Integer], names: Seq[String]) {\n+    if (ids != null && ids.size > 0) {\n+      ColumnProjectionUtils.appendReadColumns(conf, ids)\n+    }\n+    appendReadColumnNames(conf, names)"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Can you double check it? I am not sure \"DECIMAL in hive-0.12 is actually DECIMAL(10,0)\". From the code, seems precision is unbounded.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-04T17:22:46Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Yeah I think you are right, it is unbounded in Hive 12.  Spark SQL also will use unbounded precision decimals internally, so when its not specified thats what we should assume.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-04T20:10:34Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)"
  }, {
    "author": {
      "login": "zhzhan"
    },
    "body": "Change the comments, and make the parser accept any decimal(precision, scale). But the gap is still there between hive0.13.1 and spark support.\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-05T08:09:43Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)"
  }],
  "prId": 2241
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Let's say we connect to a existing hive 0.13 metastore. If there is a decimal column with a user-defined precision and scale, will we see parsing error?\n",
    "commit": "3ece9051a2b29ce20fc7557b22605e3fe8198f55",
    "createdAt": "2014-10-04T17:28:43Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.util.Properties\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.common.StatsSetupConst\n+import org.apache.hadoop.hive.common.`type`.{HiveDecimal}\n+import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.{Table, Hive, Partition}\n+import org.apache.hadoop.hive.ql.plan.{FileSinkDesc, TableDesc}\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory\n+import org.apache.hadoop.hive.serde2.{ColumnProjectionUtils, Deserializer}\n+import org.apache.hadoop.mapred.InputFormat\n+import org.apache.spark.Logging\n+import org.apache.hadoop.{io => hadoopIo}\n+import scala.collection.JavaConversions._\n+import scala.language.implicitConversions\n+\n+/**\n+ * A compatibility layer for interacting with Hive version 0.13.1.\n+ */\n+private[hive] object HiveShim {\n+  val version = \"0.13.1\"\n+  /*\n+   * TODO: hive-0.13 support DECIMAL(precision, scale), DECIMAL in hive-0.12 is actually DECIMAL(10,0)\n+   * Full support of new decimal feature need to be fixed in seperate PR.\n+   */\n+  val metastoreDecimal = \"decimal(10,0)\""
  }],
  "prId": 2241
}]