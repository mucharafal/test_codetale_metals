[{
  "comments": [{
    "author": {
      "login": "wangyum"
    },
    "body": "read performance with Filter pushdown?",
    "commit": "6849a87a424140f15ddc308cee4a0087715f2f0f",
    "createdAt": "2018-10-23T10:38:44Z",
    "diffHunk": "@@ -21,207 +21,212 @@ import scala.concurrent.duration._\n \n import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox\n \n-import org.apache.spark.benchmark.Benchmark\n-import org.apache.spark.sql.Column\n-import org.apache.spark.sql.catalyst.FunctionIdentifier\n-import org.apache.spark.sql.catalyst.catalog.CatalogFunction\n+import org.apache.spark.benchmark.{Benchmark, BenchmarkBase}\n+import org.apache.spark.sql.{Column, SparkSession}\n import org.apache.spark.sql.catalyst.expressions.Literal\n import org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile\n-import org.apache.spark.sql.hive.HiveSessionCatalog\n+import org.apache.spark.sql.catalyst.plans.SQLHelper\n import org.apache.spark.sql.hive.execution.TestingTypedCount\n-import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.hive.test.TestHive\n import org.apache.spark.sql.internal.SQLConf\n import org.apache.spark.sql.types.LongType\n \n-class ObjectHashAggregateExecBenchmark extends BenchmarkWithCodegen with TestHiveSingleton {\n-  ignore(\"Hive UDAF vs Spark AF\") {\n-    val N = 2 << 15\n-\n-    val benchmark = new Benchmark(\n-      name = \"hive udaf vs spark af\",\n-      valuesPerIteration = N,\n-      minNumIters = 5,\n-      warmupTime = 5.seconds,\n-      minTime = 10.seconds,\n-      outputPerIteration = true\n-    )\n-\n-    registerHiveFunction(\"hive_percentile_approx\", classOf[GenericUDAFPercentileApprox])\n-\n-    sparkSession.range(N).createOrReplaceTempView(\"t\")\n-\n-    benchmark.addCase(\"hive udaf w/o group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      sparkSession.sql(\"SELECT hive_percentile_approx(id, 0.5) FROM t\").collect()\n-    }\n-\n-    benchmark.addCase(\"spark af w/o group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.sql(\"SELECT percentile_approx(id, 0.5) FROM t\").collect()\n-    }\n-\n-    benchmark.addCase(\"hive udaf w/ group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      sparkSession.sql(\n-        s\"SELECT hive_percentile_approx(id, 0.5) FROM t GROUP BY CAST(id / ${N / 4} AS BIGINT)\"\n-      ).collect()\n-    }\n-\n-    benchmark.addCase(\"spark af w/ group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.sql(\n-        s\"SELECT percentile_approx(id, 0.5) FROM t GROUP BY CAST(id / ${N / 4} AS BIGINT)\"\n-      ).collect()\n-    }\n-\n-    benchmark.addCase(\"spark af w/ group by w/ fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.conf.set(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key, \"2\")\n-      sparkSession.sql(\n-        s\"SELECT percentile_approx(id, 0.5) FROM t GROUP BY CAST(id / ${N / 4} AS BIGINT)\"\n-      ).collect()\n-    }\n-\n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.10.5\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-\n-    hive udaf vs spark af:                   Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    hive udaf w/o group by                        5326 / 5408          0.0       81264.2       1.0X\n-    spark af w/o group by                           93 /  111          0.7        1415.6      57.4X\n-    hive udaf w/ group by                         3804 / 3946          0.0       58050.1       1.4X\n-    spark af w/ group by w/o fallback               71 /   90          0.9        1085.7      74.8X\n-    spark af w/ group by w/ fallback                98 /  111          0.7        1501.6      54.1X\n-     */\n-  }\n-\n-  ignore(\"ObjectHashAggregateExec vs SortAggregateExec - typed_count\") {\n-    val N: Long = 1024 * 1024 * 100\n-\n-    val benchmark = new Benchmark(\n-      name = \"object agg v.s. sort agg\",\n-      valuesPerIteration = N,\n-      minNumIters = 1,\n-      warmupTime = 10.seconds,\n-      minTime = 45.seconds,\n-      outputPerIteration = true\n-    )\n-\n-    import sparkSession.implicits._\n-\n-    def typed_count(column: Column): Column =\n-      Column(TestingTypedCount(column.expr).toAggregateExpression())\n-\n-    val df = sparkSession.range(N)\n-\n-    benchmark.addCase(\"sort agg w/ group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      df.groupBy($\"id\" < (N / 2)).agg(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      df.groupBy($\"id\" < (N / 2)).agg(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/ fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.conf.set(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key, \"2\")\n-      df.groupBy($\"id\" < (N / 2)).agg(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"sort agg w/o group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      df.select(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/o group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      df.select(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.10.5\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-\n-    object agg v.s. sort agg:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    sort agg w/ group by                        31251 / 31908          3.4         298.0       1.0X\n-    object agg w/ group by w/o fallback           6903 / 7141         15.2          65.8       4.5X\n-    object agg w/ group by w/ fallback          20945 / 21613          5.0         199.7       1.5X\n-    sort agg w/o group by                         4734 / 5463         22.1          45.2       6.6X\n-    object agg w/o group by w/o fallback          4310 / 4529         24.3          41.1       7.3X\n-     */\n-  }\n-\n-  ignore(\"ObjectHashAggregateExec vs SortAggregateExec - percentile_approx\") {\n-    val N = 2 << 20\n-\n-    val benchmark = new Benchmark(\n-      name = \"object agg v.s. sort agg\",\n-      valuesPerIteration = N,\n-      minNumIters = 5,\n-      warmupTime = 15.seconds,\n-      minTime = 45.seconds,\n-      outputPerIteration = true\n-    )\n-\n-    import sparkSession.implicits._\n-\n-    val df = sparkSession.range(N).coalesce(1)\n-\n-    benchmark.addCase(\"sort agg w/ group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      df.groupBy($\"id\" / (N / 4) cast LongType).agg(percentile_approx($\"id\", 0.5)).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      df.groupBy($\"id\" / (N / 4) cast LongType).agg(percentile_approx($\"id\", 0.5)).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/ fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.conf.set(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key, \"2\")\n-      df.groupBy($\"id\" / (N / 4) cast LongType).agg(percentile_approx($\"id\", 0.5)).collect()\n+/**\n+ * Benchmark to measure read performance with Filter pushdown."
  }, {
    "author": {
      "login": "peter-toth"
    },
    "body": "Thanks @wangyum , fixed.",
    "commit": "6849a87a424140f15ddc308cee4a0087715f2f0f",
    "createdAt": "2018-10-23T16:28:55Z",
    "diffHunk": "@@ -21,207 +21,212 @@ import scala.concurrent.duration._\n \n import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox\n \n-import org.apache.spark.benchmark.Benchmark\n-import org.apache.spark.sql.Column\n-import org.apache.spark.sql.catalyst.FunctionIdentifier\n-import org.apache.spark.sql.catalyst.catalog.CatalogFunction\n+import org.apache.spark.benchmark.{Benchmark, BenchmarkBase}\n+import org.apache.spark.sql.{Column, SparkSession}\n import org.apache.spark.sql.catalyst.expressions.Literal\n import org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile\n-import org.apache.spark.sql.hive.HiveSessionCatalog\n+import org.apache.spark.sql.catalyst.plans.SQLHelper\n import org.apache.spark.sql.hive.execution.TestingTypedCount\n-import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.hive.test.TestHive\n import org.apache.spark.sql.internal.SQLConf\n import org.apache.spark.sql.types.LongType\n \n-class ObjectHashAggregateExecBenchmark extends BenchmarkWithCodegen with TestHiveSingleton {\n-  ignore(\"Hive UDAF vs Spark AF\") {\n-    val N = 2 << 15\n-\n-    val benchmark = new Benchmark(\n-      name = \"hive udaf vs spark af\",\n-      valuesPerIteration = N,\n-      minNumIters = 5,\n-      warmupTime = 5.seconds,\n-      minTime = 10.seconds,\n-      outputPerIteration = true\n-    )\n-\n-    registerHiveFunction(\"hive_percentile_approx\", classOf[GenericUDAFPercentileApprox])\n-\n-    sparkSession.range(N).createOrReplaceTempView(\"t\")\n-\n-    benchmark.addCase(\"hive udaf w/o group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      sparkSession.sql(\"SELECT hive_percentile_approx(id, 0.5) FROM t\").collect()\n-    }\n-\n-    benchmark.addCase(\"spark af w/o group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.sql(\"SELECT percentile_approx(id, 0.5) FROM t\").collect()\n-    }\n-\n-    benchmark.addCase(\"hive udaf w/ group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      sparkSession.sql(\n-        s\"SELECT hive_percentile_approx(id, 0.5) FROM t GROUP BY CAST(id / ${N / 4} AS BIGINT)\"\n-      ).collect()\n-    }\n-\n-    benchmark.addCase(\"spark af w/ group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.sql(\n-        s\"SELECT percentile_approx(id, 0.5) FROM t GROUP BY CAST(id / ${N / 4} AS BIGINT)\"\n-      ).collect()\n-    }\n-\n-    benchmark.addCase(\"spark af w/ group by w/ fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.conf.set(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key, \"2\")\n-      sparkSession.sql(\n-        s\"SELECT percentile_approx(id, 0.5) FROM t GROUP BY CAST(id / ${N / 4} AS BIGINT)\"\n-      ).collect()\n-    }\n-\n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.10.5\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-\n-    hive udaf vs spark af:                   Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    hive udaf w/o group by                        5326 / 5408          0.0       81264.2       1.0X\n-    spark af w/o group by                           93 /  111          0.7        1415.6      57.4X\n-    hive udaf w/ group by                         3804 / 3946          0.0       58050.1       1.4X\n-    spark af w/ group by w/o fallback               71 /   90          0.9        1085.7      74.8X\n-    spark af w/ group by w/ fallback                98 /  111          0.7        1501.6      54.1X\n-     */\n-  }\n-\n-  ignore(\"ObjectHashAggregateExec vs SortAggregateExec - typed_count\") {\n-    val N: Long = 1024 * 1024 * 100\n-\n-    val benchmark = new Benchmark(\n-      name = \"object agg v.s. sort agg\",\n-      valuesPerIteration = N,\n-      minNumIters = 1,\n-      warmupTime = 10.seconds,\n-      minTime = 45.seconds,\n-      outputPerIteration = true\n-    )\n-\n-    import sparkSession.implicits._\n-\n-    def typed_count(column: Column): Column =\n-      Column(TestingTypedCount(column.expr).toAggregateExpression())\n-\n-    val df = sparkSession.range(N)\n-\n-    benchmark.addCase(\"sort agg w/ group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      df.groupBy($\"id\" < (N / 2)).agg(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      df.groupBy($\"id\" < (N / 2)).agg(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/ fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.conf.set(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key, \"2\")\n-      df.groupBy($\"id\" < (N / 2)).agg(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"sort agg w/o group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      df.select(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/o group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      df.select(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.10.5\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-\n-    object agg v.s. sort agg:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    sort agg w/ group by                        31251 / 31908          3.4         298.0       1.0X\n-    object agg w/ group by w/o fallback           6903 / 7141         15.2          65.8       4.5X\n-    object agg w/ group by w/ fallback          20945 / 21613          5.0         199.7       1.5X\n-    sort agg w/o group by                         4734 / 5463         22.1          45.2       6.6X\n-    object agg w/o group by w/o fallback          4310 / 4529         24.3          41.1       7.3X\n-     */\n-  }\n-\n-  ignore(\"ObjectHashAggregateExec vs SortAggregateExec - percentile_approx\") {\n-    val N = 2 << 20\n-\n-    val benchmark = new Benchmark(\n-      name = \"object agg v.s. sort agg\",\n-      valuesPerIteration = N,\n-      minNumIters = 5,\n-      warmupTime = 15.seconds,\n-      minTime = 45.seconds,\n-      outputPerIteration = true\n-    )\n-\n-    import sparkSession.implicits._\n-\n-    val df = sparkSession.range(N).coalesce(1)\n-\n-    benchmark.addCase(\"sort agg w/ group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      df.groupBy($\"id\" / (N / 4) cast LongType).agg(percentile_approx($\"id\", 0.5)).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      df.groupBy($\"id\" / (N / 4) cast LongType).agg(percentile_approx($\"id\", 0.5)).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/ fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.conf.set(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key, \"2\")\n-      df.groupBy($\"id\" / (N / 4) cast LongType).agg(percentile_approx($\"id\", 0.5)).collect()\n+/**\n+ * Benchmark to measure read performance with Filter pushdown."
  }],
  "prId": 22804
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Hi, @peter-toth . Thank you for making this PR.\r\nCurrently, `runBenchmarkSuite` is too long. Could you make a separate function for each test case? For example, `ignore(\"Hive UDAF vs Spark AF\")` can be a single function. And `runBenchmarkSuite` will call a series of those functions.",
    "commit": "6849a87a424140f15ddc308cee4a0087715f2f0f",
    "createdAt": "2018-10-23T22:06:27Z",
    "diffHunk": "@@ -21,207 +21,212 @@ import scala.concurrent.duration._\n \n import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox\n \n-import org.apache.spark.benchmark.Benchmark\n-import org.apache.spark.sql.Column\n-import org.apache.spark.sql.catalyst.FunctionIdentifier\n-import org.apache.spark.sql.catalyst.catalog.CatalogFunction\n+import org.apache.spark.benchmark.{Benchmark, BenchmarkBase}\n+import org.apache.spark.sql.{Column, SparkSession}\n import org.apache.spark.sql.catalyst.expressions.Literal\n import org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile\n-import org.apache.spark.sql.hive.HiveSessionCatalog\n+import org.apache.spark.sql.catalyst.plans.SQLHelper\n import org.apache.spark.sql.hive.execution.TestingTypedCount\n-import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.hive.test.TestHive\n import org.apache.spark.sql.internal.SQLConf\n import org.apache.spark.sql.types.LongType\n \n-class ObjectHashAggregateExecBenchmark extends BenchmarkWithCodegen with TestHiveSingleton {\n-  ignore(\"Hive UDAF vs Spark AF\") {\n-    val N = 2 << 15\n-\n-    val benchmark = new Benchmark(\n-      name = \"hive udaf vs spark af\",\n-      valuesPerIteration = N,\n-      minNumIters = 5,\n-      warmupTime = 5.seconds,\n-      minTime = 10.seconds,\n-      outputPerIteration = true\n-    )\n-\n-    registerHiveFunction(\"hive_percentile_approx\", classOf[GenericUDAFPercentileApprox])\n-\n-    sparkSession.range(N).createOrReplaceTempView(\"t\")\n-\n-    benchmark.addCase(\"hive udaf w/o group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      sparkSession.sql(\"SELECT hive_percentile_approx(id, 0.5) FROM t\").collect()\n-    }\n-\n-    benchmark.addCase(\"spark af w/o group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.sql(\"SELECT percentile_approx(id, 0.5) FROM t\").collect()\n-    }\n-\n-    benchmark.addCase(\"hive udaf w/ group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      sparkSession.sql(\n-        s\"SELECT hive_percentile_approx(id, 0.5) FROM t GROUP BY CAST(id / ${N / 4} AS BIGINT)\"\n-      ).collect()\n-    }\n-\n-    benchmark.addCase(\"spark af w/ group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.sql(\n-        s\"SELECT percentile_approx(id, 0.5) FROM t GROUP BY CAST(id / ${N / 4} AS BIGINT)\"\n-      ).collect()\n-    }\n-\n-    benchmark.addCase(\"spark af w/ group by w/ fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.conf.set(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key, \"2\")\n-      sparkSession.sql(\n-        s\"SELECT percentile_approx(id, 0.5) FROM t GROUP BY CAST(id / ${N / 4} AS BIGINT)\"\n-      ).collect()\n-    }\n-\n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.10.5\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-\n-    hive udaf vs spark af:                   Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    hive udaf w/o group by                        5326 / 5408          0.0       81264.2       1.0X\n-    spark af w/o group by                           93 /  111          0.7        1415.6      57.4X\n-    hive udaf w/ group by                         3804 / 3946          0.0       58050.1       1.4X\n-    spark af w/ group by w/o fallback               71 /   90          0.9        1085.7      74.8X\n-    spark af w/ group by w/ fallback                98 /  111          0.7        1501.6      54.1X\n-     */\n-  }\n-\n-  ignore(\"ObjectHashAggregateExec vs SortAggregateExec - typed_count\") {\n-    val N: Long = 1024 * 1024 * 100\n-\n-    val benchmark = new Benchmark(\n-      name = \"object agg v.s. sort agg\",\n-      valuesPerIteration = N,\n-      minNumIters = 1,\n-      warmupTime = 10.seconds,\n-      minTime = 45.seconds,\n-      outputPerIteration = true\n-    )\n-\n-    import sparkSession.implicits._\n-\n-    def typed_count(column: Column): Column =\n-      Column(TestingTypedCount(column.expr).toAggregateExpression())\n-\n-    val df = sparkSession.range(N)\n-\n-    benchmark.addCase(\"sort agg w/ group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      df.groupBy($\"id\" < (N / 2)).agg(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      df.groupBy($\"id\" < (N / 2)).agg(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/ fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.conf.set(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key, \"2\")\n-      df.groupBy($\"id\" < (N / 2)).agg(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"sort agg w/o group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      df.select(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/o group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      df.select(typed_count($\"id\")).collect()\n-    }\n-\n-    benchmark.run()\n-\n-    /*\n-    Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14 on Mac OS X 10.10.5\n-    Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz\n-\n-    object agg v.s. sort agg:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-    ------------------------------------------------------------------------------------------------\n-    sort agg w/ group by                        31251 / 31908          3.4         298.0       1.0X\n-    object agg w/ group by w/o fallback           6903 / 7141         15.2          65.8       4.5X\n-    object agg w/ group by w/ fallback          20945 / 21613          5.0         199.7       1.5X\n-    sort agg w/o group by                         4734 / 5463         22.1          45.2       6.6X\n-    object agg w/o group by w/o fallback          4310 / 4529         24.3          41.1       7.3X\n-     */\n-  }\n-\n-  ignore(\"ObjectHashAggregateExec vs SortAggregateExec - percentile_approx\") {\n-    val N = 2 << 20\n-\n-    val benchmark = new Benchmark(\n-      name = \"object agg v.s. sort agg\",\n-      valuesPerIteration = N,\n-      minNumIters = 5,\n-      warmupTime = 15.seconds,\n-      minTime = 45.seconds,\n-      outputPerIteration = true\n-    )\n-\n-    import sparkSession.implicits._\n-\n-    val df = sparkSession.range(N).coalesce(1)\n-\n-    benchmark.addCase(\"sort agg w/ group by\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"false\")\n-      df.groupBy($\"id\" / (N / 4) cast LongType).agg(percentile_approx($\"id\", 0.5)).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/o fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      df.groupBy($\"id\" / (N / 4) cast LongType).agg(percentile_approx($\"id\", 0.5)).collect()\n-    }\n-\n-    benchmark.addCase(\"object agg w/ group by w/ fallback\") { _ =>\n-      sparkSession.conf.set(SQLConf.USE_OBJECT_HASH_AGG.key, \"true\")\n-      sparkSession.conf.set(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key, \"2\")\n-      df.groupBy($\"id\" / (N / 4) cast LongType).agg(percentile_approx($\"id\", 0.5)).collect()\n+/**\n+ * Benchmark to measure hash based aggregation.\n+ * To run this benchmark:\n+ * {{{\n+ *   1. without sbt: bin/spark-submit --class <this class>\n+ *        --jars <spark catalyst test jar>,<spark core test jar>,<spark hive jar>\n+ *        --packages org.spark-project.hive:hive-exec:1.2.1.spark2\n+ *        <spark hive test jar>\n+ *   2. build/sbt \"hive/test:runMain <this class>\"\n+ *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"hive/test:runMain <this class>\"\n+ *      Results will be written to \"benchmarks/ObjectHashAggregateExecBenchmark-results.txt\".\n+ * }}}\n+ */\n+object ObjectHashAggregateExecBenchmark extends BenchmarkBase with SQLHelper {\n+\n+  val spark: SparkSession = TestHive.sparkSession\n+\n+  override def runBenchmarkSuite(): Unit = {\n+    runBenchmark(\"Hive UDAF vs Spark AF\") {"
  }],
  "prId": 22804
}]