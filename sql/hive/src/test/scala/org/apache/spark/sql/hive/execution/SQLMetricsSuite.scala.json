[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "The above two test cases are moved to `sql/execution/metric/SQLMetricsSuite.scala`",
    "commit": "3dad1275f33b54a34f43cfe81cb16c8b1dc826a4",
    "createdAt": "2017-08-31T05:59:04Z",
    "diffHunk": "@@ -17,112 +17,10 @@\n \n package org.apache.spark.sql.hive.execution\n \n-import java.io.File\n-\n-import org.apache.spark.sql.catalyst.TableIdentifier\n-import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.execution.metric.SQLMetricsTestUtils\n import org.apache.spark.sql.hive.test.TestHiveSingleton\n-import org.apache.spark.sql.test.SQLTestUtils\n-import org.apache.spark.util.Utils\n-\n-class SQLMetricsSuite extends SQLTestUtils with TestHiveSingleton {\n-  import spark.implicits._\n-\n-  /**\n-   * Get execution metrics for the SQL execution and verify metrics values.\n-   *\n-   * @param metricsValues the expected metric values (numFiles, numPartitions, numOutputRows).\n-   * @param func the function can produce execution id after running.\n-   */\n-  private def verifyWriteDataMetrics(metricsValues: Seq[Int])(func: => Unit): Unit = {\n-    val previousExecutionIds = spark.sharedState.listener.executionIdToData.keySet\n-    // Run the given function to trigger query execution.\n-    func\n-    spark.sparkContext.listenerBus.waitUntilEmpty(10000)\n-    val executionIds =\n-      spark.sharedState.listener.executionIdToData.keySet.diff(previousExecutionIds)\n-    assert(executionIds.size == 1)\n-    val executionId = executionIds.head\n-\n-    val executionData = spark.sharedState.listener.getExecution(executionId).get\n-    val executedNode = executionData.physicalPlanGraph.nodes.head\n-\n-    val metricsNames = Seq(\n-      \"number of written files\",\n-      \"number of dynamic part\",\n-      \"number of output rows\")\n-\n-    val metrics = spark.sharedState.listener.getExecutionMetrics(executionId)\n-\n-    metricsNames.zip(metricsValues).foreach { case (metricsName, expected) =>\n-      val sqlMetric = executedNode.metrics.find(_.name == metricsName)\n-      assert(sqlMetric.isDefined)\n-      val accumulatorId = sqlMetric.get.accumulatorId\n-      val metricValue = metrics(accumulatorId).replaceAll(\",\", \"\").toInt\n-      assert(metricValue == expected)\n-    }\n-\n-    val totalNumBytesMetric = executedNode.metrics.find(_.name == \"bytes of written output\").get\n-    val totalNumBytes = metrics(totalNumBytesMetric.accumulatorId).replaceAll(\",\", \"\").toInt\n-    assert(totalNumBytes > 0)\n-  }\n-\n-  private def testMetricsNonDynamicPartition(\n-      dataFormat: String,\n-      tableName: String): Unit = {\n-    withTable(tableName) {\n-      Seq((1, 2)).toDF(\"i\", \"j\")\n-        .write.format(dataFormat).mode(\"overwrite\").saveAsTable(tableName)\n-\n-      val tableLocation =\n-        new File(spark.sessionState.catalog.getTableMetadata(TableIdentifier(tableName)).location)\n \n-      // 2 files, 100 rows, 0 dynamic partition.\n-      verifyWriteDataMetrics(Seq(2, 0, 100)) {\n-        (0 until 100).map(i => (i, i + 1)).toDF(\"i\", \"j\").repartition(2)\n-          .write.format(dataFormat).mode(\"overwrite\").insertInto(tableName)\n-      }\n-      assert(Utils.recursiveList(tableLocation).count(_.getName.startsWith(\"part-\")) == 2)\n-    }\n-  }\n-\n-  private def testMetricsDynamicPartition(\n-      provider: String,\n-      dataFormat: String,\n-      tableName: String): Unit = {\n-    withTempPath { dir =>\n-      spark.sql(\n-        s\"\"\"\n-           |CREATE TABLE $tableName(a int, b int)\n-           |USING $provider\n-           |PARTITIONED BY(a)\n-           |LOCATION '${dir.toURI}'\n-         \"\"\".stripMargin)\n-      val table = spark.sessionState.catalog.getTableMetadata(TableIdentifier(tableName))\n-      assert(table.location == makeQualifiedPath(dir.getAbsolutePath))\n-\n-      val df = spark.range(start = 0, end = 40, step = 1, numPartitions = 1)\n-        .selectExpr(\"id a\", \"id b\")\n-\n-      // 40 files, 80 rows, 40 dynamic partitions.\n-      verifyWriteDataMetrics(Seq(40, 40, 80)) {\n-        df.union(df).repartition(2, $\"a\")\n-          .write\n-          .format(dataFormat)\n-          .mode(\"overwrite\")\n-          .insertInto(tableName)\n-      }\n-      assert(Utils.recursiveList(dir).count(_.getName.startsWith(\"part-\")) == 40)\n-    }\n-  }\n-\n-  test(\"writing data out metrics: parquet\") {\n-    testMetricsNonDynamicPartition(\"parquet\", \"t1\")\n-  }\n-\n-  test(\"writing data out metrics with dynamic partition: parquet\") {",
    "line": 108
  }],
  "prId": 19092
}]