[{
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "ditto.",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-28T15:06:53Z",
    "diffHunk": "@@ -0,0 +1,512 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+  private case class PartitionDefinition(\n+                                          column: String,"
  }, {
    "author": {
      "login": "krisgeus"
    },
    "body": "Fixed",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-29T21:56:59Z",
    "diffHunk": "@@ -0,0 +1,512 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+  private case class PartitionDefinition(\n+                                          column: String,"
  }],
  "prId": 21893
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "?",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T15:06:38Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val avroCheckTableSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${avroPartitionTable}\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        // Selecting data from the partition currently fails because it tries to\n+        // read avro data with parquet reader\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+      }\n+      //      }"
  }, {
    "author": {
      "login": "krisgeus"
    },
    "body": "See above. Removed",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T16:03:59Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val avroCheckTableSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${avroPartitionTable}\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        // Selecting data from the partition currently fails because it tries to\n+        // read avro data with parquet reader\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+      }\n+      //      }"
  }],
  "prId": 21893
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "same indent problem.",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T15:08:39Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val avroCheckTableSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${avroPartitionTable}\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        // Selecting data from the partition currently fails because it tries to\n+        // read avro data with parquet reader\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+      }\n+      //      }\n+    }\n+  }\n+\n+  private def createMultiformatPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"PARQUET\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"AVRO\")\n+      )\n+    )\n+  }\n+\n+  private def createParquetPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"PARQUET\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"PARQUET\")\n+      )\n+    )\n+  }\n+\n+  private def createAvroPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"AVRO\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"AVRO\")\n+      )\n+    )\n+  }\n+\n+  private def createTableWithPartitions(table: String,\n+                                        baseDir: File,"
  }, {
    "author": {
      "login": "krisgeus"
    },
    "body": "Fixed",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T16:04:24Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val avroCheckTableSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${avroPartitionTable}\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        // Selecting data from the partition currently fails because it tries to\n+        // read avro data with parquet reader\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+      }\n+      //      }\n+    }\n+  }\n+\n+  private def createMultiformatPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"PARQUET\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"AVRO\")\n+      )\n+    )\n+  }\n+\n+  private def createParquetPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"PARQUET\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"PARQUET\")\n+      )\n+    )\n+  }\n+\n+  private def createAvroPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"AVRO\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"AVRO\")\n+      )\n+    )\n+  }\n+\n+  private def createTableWithPartitions(table: String,\n+                                        baseDir: File,"
  }],
  "prId": 21893
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "ditto.",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T15:08:49Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val avroCheckTableSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${avroPartitionTable}\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        // Selecting data from the partition currently fails because it tries to\n+        // read avro data with parquet reader\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+      }\n+      //      }\n+    }\n+  }\n+\n+  private def createMultiformatPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"PARQUET\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"AVRO\")\n+      )\n+    )\n+  }\n+\n+  private def createParquetPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"PARQUET\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"PARQUET\")\n+      )\n+    )\n+  }\n+\n+  private def createAvroPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"AVRO\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"AVRO\")\n+      )\n+    )\n+  }\n+\n+  private def createTableWithPartitions(table: String,\n+                                        baseDir: File,\n+                                        partitions: List[PartitionDefinition],\n+                                        avro: Boolean = false\n+                                       ): Unit = {\n+    if (avro) {\n+      createAvroExternalTable(table, baseDir.toURI)\n+    } else {\n+      createParquetExternalTable(table, baseDir.toURI)\n+    }\n+    addPartitions(table, partitions)\n+    partitions.foreach(p => setPartitionFormat(table, p))\n+  }\n+\n+  private def createAvroCheckTable(avroTable: String, partition: PartitionDefinition): Unit = {\n+    // The only valid way to insert avro data into the avro partition\n+    // is to create a new avro table directly on the location of the avro partition\n+    val avroSchema =\n+    \"\"\"{\n+      |  \"name\": \"baseRecord\",\n+      |  \"type\": \"record\",\n+      |  \"fields\": [{\n+      |    \"name\": \"key\",\n+      |    \"type\": [\"null\", \"int\"],\n+      |    \"default\": null\n+      |  },\n+      |  {\n+      |    \"name\": \"value\",\n+      |    \"type\": [\"null\", \"string\"],\n+      |    \"default\": null\n+      |  }]\n+      |}\n+    \"\"\".stripMargin\n+\n+    // Creates the Avro table\n+    sql(\n+      s\"\"\"\n+         |CREATE TABLE $avroTable\n+         |ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\n+         |STORED AS\n+         |  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n+         |  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n+         |LOCATION '${partition.location}'\n+         |TBLPROPERTIES ('avro.schema.literal' = '$avroSchema')\n+            \"\"\".stripMargin\n+    )\n+  }\n+\n+  private def createPqCheckTable(pqTable: String, partition: PartitionDefinition): Unit = {\n+\n+    // Creates the Parquet table\n+    sql(\n+      s\"\"\"\n+         |CREATE TABLE $pqTable (key INT, value STRING)\n+         |STORED AS PARQUET\n+         |LOCATION '${partition.location}'\n+            \"\"\".stripMargin\n+    )\n+  }\n+\n+  private def createParquetExternalTable(table: String, location: URI): DataFrame = {\n+    sql(\n+      s\"\"\"\n+         |CREATE EXTERNAL TABLE $table (key INT, value STRING)\n+         |PARTITIONED BY (dt STRING)\n+         |STORED AS PARQUET\n+         |LOCATION '$location'\n+      \"\"\".stripMargin\n+    )\n+  }\n+\n+  private def createAvroExternalTable(table: String, location: URI): DataFrame = {\n+    val avroSchema =\n+      \"\"\"{\n+        |  \"name\": \"baseRecord\",\n+        |  \"type\": \"record\",\n+        |  \"fields\": [{\n+        |    \"name\": \"key\",\n+        |    \"type\": [\"null\", \"int\"],\n+        |    \"default\": null\n+        |  },\n+        |  {\n+        |    \"name\": \"value\",\n+        |    \"type\": [\"null\", \"string\"],\n+        |    \"default\": null\n+        |  }]\n+        |}\n+      \"\"\".stripMargin\n+    sql(\n+      s\"\"\"\n+         |CREATE EXTERNAL TABLE $table (key INT, value STRING)\n+         |PARTITIONED BY (dt STRING)\n+         |STORED AS\n+         |  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n+         |  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n+         |LOCATION '$location'\n+         |TBLPROPERTIES ('avro.schema.literal' = '$avroSchema')\n+      \"\"\".stripMargin\n+    )\n+  }\n+\n+  private def addPartitions(table: String, partitionDefs: List[PartitionDefinition]): DataFrame = {\n+    val partitions = partitionDefs\n+      .map(definition => s\"PARTITION ${definition.toSpec} LOCATION '${definition.location}'\")\n+      .mkString(\"\\n\")\n+\n+    sql(\n+      s\"\"\"\n+         |ALTER TABLE $table ADD\n+         |$partitions\n+      \"\"\".stripMargin\n+    )\n+\n+  }\n+\n+  private def setPartitionFormat(\n+                                  table: String,"
  }, {
    "author": {
      "login": "krisgeus"
    },
    "body": "Fixed",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T16:04:35Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val avroCheckTableSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${avroPartitionTable}\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        // Selecting data from the partition currently fails because it tries to\n+        // read avro data with parquet reader\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+      }\n+      //      }\n+    }\n+  }\n+\n+  private def createMultiformatPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"PARQUET\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"AVRO\")\n+      )\n+    )\n+  }\n+\n+  private def createParquetPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"PARQUET\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"PARQUET\")\n+      )\n+    )\n+  }\n+\n+  private def createAvroPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {\n+    val basePath = baseDir.getCanonicalPath\n+    val partitionPath_part1 = new File(basePath + s\"/$partitionCol=$partitionVal1\")\n+    val partitionPath_part2 = new File(basePath + s\"/$partitionCol=$partitionVal2\")\n+\n+    List(\n+      PartitionDefinition(\n+        partitionCol, partitionVal1, partitionPath_part1.toURI, format = Some(\"AVRO\")\n+      ),\n+      PartitionDefinition(\n+        partitionCol, partitionVal2, partitionPath_part2.toURI, format = Some(\"AVRO\")\n+      )\n+    )\n+  }\n+\n+  private def createTableWithPartitions(table: String,\n+                                        baseDir: File,\n+                                        partitions: List[PartitionDefinition],\n+                                        avro: Boolean = false\n+                                       ): Unit = {\n+    if (avro) {\n+      createAvroExternalTable(table, baseDir.toURI)\n+    } else {\n+      createParquetExternalTable(table, baseDir.toURI)\n+    }\n+    addPartitions(table, partitions)\n+    partitions.foreach(p => setPartitionFormat(table, p))\n+  }\n+\n+  private def createAvroCheckTable(avroTable: String, partition: PartitionDefinition): Unit = {\n+    // The only valid way to insert avro data into the avro partition\n+    // is to create a new avro table directly on the location of the avro partition\n+    val avroSchema =\n+    \"\"\"{\n+      |  \"name\": \"baseRecord\",\n+      |  \"type\": \"record\",\n+      |  \"fields\": [{\n+      |    \"name\": \"key\",\n+      |    \"type\": [\"null\", \"int\"],\n+      |    \"default\": null\n+      |  },\n+      |  {\n+      |    \"name\": \"value\",\n+      |    \"type\": [\"null\", \"string\"],\n+      |    \"default\": null\n+      |  }]\n+      |}\n+    \"\"\".stripMargin\n+\n+    // Creates the Avro table\n+    sql(\n+      s\"\"\"\n+         |CREATE TABLE $avroTable\n+         |ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\n+         |STORED AS\n+         |  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n+         |  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n+         |LOCATION '${partition.location}'\n+         |TBLPROPERTIES ('avro.schema.literal' = '$avroSchema')\n+            \"\"\".stripMargin\n+    )\n+  }\n+\n+  private def createPqCheckTable(pqTable: String, partition: PartitionDefinition): Unit = {\n+\n+    // Creates the Parquet table\n+    sql(\n+      s\"\"\"\n+         |CREATE TABLE $pqTable (key INT, value STRING)\n+         |STORED AS PARQUET\n+         |LOCATION '${partition.location}'\n+            \"\"\".stripMargin\n+    )\n+  }\n+\n+  private def createParquetExternalTable(table: String, location: URI): DataFrame = {\n+    sql(\n+      s\"\"\"\n+         |CREATE EXTERNAL TABLE $table (key INT, value STRING)\n+         |PARTITIONED BY (dt STRING)\n+         |STORED AS PARQUET\n+         |LOCATION '$location'\n+      \"\"\".stripMargin\n+    )\n+  }\n+\n+  private def createAvroExternalTable(table: String, location: URI): DataFrame = {\n+    val avroSchema =\n+      \"\"\"{\n+        |  \"name\": \"baseRecord\",\n+        |  \"type\": \"record\",\n+        |  \"fields\": [{\n+        |    \"name\": \"key\",\n+        |    \"type\": [\"null\", \"int\"],\n+        |    \"default\": null\n+        |  },\n+        |  {\n+        |    \"name\": \"value\",\n+        |    \"type\": [\"null\", \"string\"],\n+        |    \"default\": null\n+        |  }]\n+        |}\n+      \"\"\".stripMargin\n+    sql(\n+      s\"\"\"\n+         |CREATE EXTERNAL TABLE $table (key INT, value STRING)\n+         |PARTITIONED BY (dt STRING)\n+         |STORED AS\n+         |  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n+         |  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n+         |LOCATION '$location'\n+         |TBLPROPERTIES ('avro.schema.literal' = '$avroSchema')\n+      \"\"\".stripMargin\n+    )\n+  }\n+\n+  private def addPartitions(table: String, partitionDefs: List[PartitionDefinition]): DataFrame = {\n+    val partitions = partitionDefs\n+      .map(definition => s\"PARTITION ${definition.toSpec} LOCATION '${definition.location}'\")\n+      .mkString(\"\\n\")\n+\n+    sql(\n+      s\"\"\"\n+         |ALTER TABLE $table ADD\n+         |$partitions\n+      \"\"\".stripMargin\n+    )\n+\n+  }\n+\n+  private def setPartitionFormat(\n+                                  table: String,"
  }],
  "prId": 21893
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Maybe these three functions (createTableWithPartitions\\ createParquetPartitionDefinitions\\ createAvroPartitionDefinitions) can be combined into one with format parameters.",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T15:13:21Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val avroCheckTableSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${avroPartitionTable}\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        // Selecting data from the partition currently fails because it tries to\n+        // read avro data with parquet reader\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+      }\n+      //      }\n+    }\n+  }\n+\n+  private def createMultiformatPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {"
  }, {
    "author": {
      "login": "krisgeus"
    },
    "body": "Good suggestion. Done",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T16:04:12Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val avroCheckTableSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${avroPartitionTable}\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        // Selecting data from the partition currently fails because it tries to\n+        // read avro data with parquet reader\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+      }\n+      //      }\n+    }\n+  }\n+\n+  private def createMultiformatPartitionDefinitions(baseDir: File): List[PartitionDefinition] = {"
  }],
  "prId": 21893
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "nit: indent.",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T15:13:33Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)"
  }, {
    "author": {
      "login": "krisgeus"
    },
    "body": "Fixed",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T16:03:48Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)"
  }],
  "prId": 21893
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "?",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T15:13:40Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {"
  }, {
    "author": {
      "login": "krisgeus"
    },
    "body": "Old try with setting this configuration which changes the execution plan. Not needed anymore so removed it.",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T16:03:41Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)\n+\n+        val parquetData = spark.read.parquet(partitions.head.location.toString)\n+        checkAnswer(parquetData, Row(1, \"a\"))\n+\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $avroPartitionTable\n+             |SELECT 2, 'b'\n+           \"\"\".stripMargin\n+        )\n+\n+        // Directly reading from the avro table should yield correct results\n+        val avroData = spark.read.table(avroPartitionTable)\n+        checkAnswer(avroData, Row(2, \"b\"))\n+\n+        val parquetPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal1}'\n+           \"\"\".stripMargin\n+\n+        val avroPartitionSelectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+             |WHERE ${partitionCol}='${partitionVal2}'\n+           \"\"\".stripMargin\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val avroPartitionData = sql(avroPartitionSelectQuery)\n+        checkAnswer(avroPartitionData, Row(2, \"b\"))\n+\n+        val parquetPartitionData = sql(parquetPartitionSelectQuery)\n+        checkAnswer(parquetPartitionData, Row(1, \"a\"))\n+\n+        val allData = sql(selectQuery)\n+        checkAnswer(allData, Seq(Row(1, \"a\"), Row(2, \"b\")))\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      //      withSQLConf(HiveUtils.CONVERT_METASTORE_PARQUET.key -> \"false\") {"
  }],
  "prId": 21893
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "nit: indent.",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T15:14:04Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)"
  }, {
    "author": {
      "login": "krisgeus"
    },
    "body": "Fixed",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T16:02:50Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {\n+\n+    def toSpec: String = {\n+      s\"($column='$value')\"\n+    }\n+    def toSpecAsMap: Map[String, String] = {\n+      Map(column -> value)\n+    }\n+  }\n+\n+  test(\"create hive table with multi format partitions\") {\n+    val catalog = spark.sessionState.catalog\n+    withTempDir { baseDir =>\n+\n+      val partitionedTable = \"ext_multiformat_partition_table\"\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        // Check table storage type is PARQUET\n+        val hiveResultTable =\n+          catalog.getTableMetadata(TableIdentifier(partitionedTable, Some(\"default\")))\n+        assert(DDLUtils.isHiveTable(hiveResultTable))\n+        assert(hiveResultTable.tableType == CatalogTableType.EXTERNAL)\n+        assert(hiveResultTable.storage.inputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\")\n+        )\n+        assert(hiveResultTable.storage.outputFormat\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\")\n+        )\n+        assert(hiveResultTable.storage.serde\n+          .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check table has correct partititons\n+        assert(\n+          catalog.listPartitions(TableIdentifier(partitionedTable,\n+            Some(\"default\"))).map(_.spec).toSet == partitions.map(_.toSpecAsMap).toSet\n+        )\n+\n+        // Check first table partition storage type is PARQUET\n+        val parquetPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.head.toSpecAsMap\n+        )\n+        assert(\n+          parquetPartition.storage.serde\n+            .contains(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\")\n+        )\n+\n+        // Check second table partition storage type is AVRO\n+        val avroPartition = catalog.getPartition(\n+          TableIdentifier(partitionedTable, Some(\"default\")),\n+          partitions.last.toSpecAsMap\n+        )\n+        assert(\n+          avroPartition.storage.serde.contains(\"org.apache.hadoop.hive.serde2.avro.AvroSerDe\")\n+        )\n+\n+        assert(\n+          avroPartition.storage.inputFormat\n+            .contains(\"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat\")\n+        )\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only parquet partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_parquet_partition_table\"\n+\n+      val partitions = createParquetPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive table with only avro partitions - test plan\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_avro_partition_table\"\n+\n+      val partitions = createAvroPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, avro = true)\n+\n+        val selectQuery =\n+          s\"\"\"\n+             |SELECT key, value FROM ${partitionedTable}\n+           \"\"\".stripMargin\n+\n+        val plan = parser.parsePlan(selectQuery)\n+\n+        plan.queryExecution.sparkPlan.find(_.isInstanceOf[HiveTableScanExec]) shouldNot equal(None)\n+\n+      }\n+    }\n+  }\n+\n+  test(\"create hive avro table with multi format partitions containing correct data\") {\n+    withTempDir { baseDir =>\n+      val partitionedTable = \"ext_multiformat_partition_table_with_data\"\n+      val avroPartitionTable = \"ext_avro_partition_table\"\n+      val pqPartitionTable = \"ext_pq_partition_table\"\n+\n+      val partitions = createMultiformatPartitionDefinitions(baseDir)\n+\n+      withTable(partitionedTable, avroPartitionTable, pqPartitionTable) {\n+        assert(baseDir.listFiles.isEmpty)\n+\n+        createTableWithPartitions(partitionedTable, baseDir, partitions, true)\n+        createAvroCheckTable(avroPartitionTable, partitions.last)\n+        createPqCheckTable(pqPartitionTable, partitions.head)\n+\n+        // INSERT OVERWRITE TABLE only works for the default table format.\n+        // So we can use it here to insert data into the parquet partition\n+        sql(\n+          s\"\"\"\n+             |INSERT OVERWRITE TABLE $pqPartitionTable\n+             |SELECT 1 as id, 'a' as value\n+                  \"\"\".stripMargin)"
  }],
  "prId": 21893
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Do not have to start a new line.",
    "commit": "e9d77f00bad884b19ca397e17ec7514de4588b26",
    "createdAt": "2018-07-30T15:15:48Z",
    "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.net.URI\n+\n+import org.scalatest.BeforeAndAfterEach\n+import org.scalatest.Matchers\n+\n+import org.apache.spark.sql.{DataFrame, QueryTest, Row}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.execution._\n+import org.apache.spark.sql.execution.command.DDLUtils\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+\n+class MultiFormatTableSuite\n+  extends QueryTest with SQLTestUtils with TestHiveSingleton with BeforeAndAfterEach with Matchers {\n+  import testImplicits._\n+\n+  val parser = new SparkSqlParser(new SQLConf())\n+\n+  override def afterEach(): Unit = {\n+    try {\n+      // drop all databases, tables and functions after each test\n+      spark.sessionState.catalog.reset()\n+    } finally {\n+      super.afterEach()\n+    }\n+  }\n+\n+  val partitionCol = \"dt\"\n+  val partitionVal1 = \"2018-01-26\"\n+  val partitionVal2 = \"2018-01-27\"\n+\n+  private case class PartitionDefinition(\n+      column: String,\n+      value: String,\n+      location: URI,\n+      format: Option[String] = None\n+  ) {",
    "line": 59
  }],
  "prId": 21893
}]