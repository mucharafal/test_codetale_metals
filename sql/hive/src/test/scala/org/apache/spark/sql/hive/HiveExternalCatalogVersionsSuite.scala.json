[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Since this is a backport to 2.2, do we still need to test backward compatibility on 2.2?",
    "commit": "7178ccfbd726149354ad2f9544ed978537efe70f",
    "createdAt": "2017-09-08T15:13:30Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.File\n+import java.nio.file.Files\n+\n+import org.apache.spark.TestUtils\n+import org.apache.spark.sql.{QueryTest, Row, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.test.SQLTestUtils\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Test HiveExternalCatalog backward compatibility.\n+ *\n+ * Note that, this test suite will automatically download spark binary packages of different\n+ * versions to a local directory `/tmp/spark-test`. If there is already a spark folder with\n+ * expected version under this local directory, e.g. `/tmp/spark-test/spark-2.0.3`, we will skip the\n+ * downloading for this spark version.\n+ */\n+class HiveExternalCatalogVersionsSuite extends SparkSubmitTestUtils {\n+  private val wareHousePath = Utils.createTempDir(namePrefix = \"warehouse\")\n+  private val tmpDataDir = Utils.createTempDir(namePrefix = \"test-data\")\n+  private val sparkTestingDir = \"/tmp/spark-test\"\n+  private val unusedJar = TestUtils.createJarWithClasses(Seq.empty)\n+\n+  override def afterAll(): Unit = {\n+    Utils.deleteRecursively(wareHousePath)\n+    Utils.deleteRecursively(tmpDataDir)\n+    super.afterAll()\n+  }\n+\n+  private def downloadSpark(version: String): Unit = {\n+    import scala.sys.process._\n+\n+    val url = s\"https://d3kbcqa49mib13.cloudfront.net/spark-$version-bin-hadoop2.7.tgz\"\n+\n+    Seq(\"wget\", url, \"-q\", \"-P\", sparkTestingDir).!\n+\n+    val downloaded = new File(sparkTestingDir, s\"spark-$version-bin-hadoop2.7.tgz\").getCanonicalPath\n+    val targetDir = new File(sparkTestingDir, s\"spark-$version\").getCanonicalPath\n+\n+    Seq(\"mkdir\", targetDir).!\n+\n+    Seq(\"tar\", \"-xzf\", downloaded, \"-C\", targetDir, \"--strip-components=1\").!\n+\n+    Seq(\"rm\", downloaded).!\n+  }\n+\n+  private def genDataDir(name: String): String = {\n+    new File(tmpDataDir, name).getCanonicalPath\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+\n+    val tempPyFile = File.createTempFile(\"test\", \".py\")\n+    Files.write(tempPyFile.toPath,\n+      s\"\"\"\n+        |from pyspark.sql import SparkSession\n+        |\n+        |spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n+        |version_index = spark.conf.get(\"spark.sql.test.version.index\", None)\n+        |\n+        |spark.sql(\"create table data_source_tbl_{} using json as select 1 i\".format(version_index))\n+        |\n+        |spark.sql(\"create table hive_compatible_data_source_tbl_\" + version_index + \\\\\n+        |          \" using parquet as select 1 i\")\n+        |\n+        |json_file = \"${genDataDir(\"json_\")}\" + str(version_index)\n+        |spark.range(1, 2).selectExpr(\"cast(id as int) as i\").write.json(json_file)\n+        |spark.sql(\"create table external_data_source_tbl_\" + version_index + \\\\\n+        |          \"(i int) using json options (path '{}')\".format(json_file))\n+        |\n+        |parquet_file = \"${genDataDir(\"parquet_\")}\" + str(version_index)\n+        |spark.range(1, 2).selectExpr(\"cast(id as int) as i\").write.parquet(parquet_file)\n+        |spark.sql(\"create table hive_compatible_external_data_source_tbl_\" + version_index + \\\\\n+        |          \"(i int) using parquet options (path '{}')\".format(parquet_file))\n+        |\n+        |json_file2 = \"${genDataDir(\"json2_\")}\" + str(version_index)\n+        |spark.range(1, 2).selectExpr(\"cast(id as int) as i\").write.json(json_file2)\n+        |spark.sql(\"create table external_table_without_schema_\" + version_index + \\\\\n+        |          \" using json options (path '{}')\".format(json_file2))\n+        |\n+        |spark.sql(\"create view v_{} as select 1 i\".format(version_index))\n+      \"\"\".stripMargin.getBytes(\"utf8\"))\n+\n+    PROCESS_TABLES.testingVersions.zipWithIndex.foreach { case (version, index) =>\n+      val sparkHome = new File(sparkTestingDir, s\"spark-$version\")\n+      if (!sparkHome.exists()) {\n+        downloadSpark(version)\n+      }\n+\n+      val args = Seq(\n+        \"--name\", \"prepare testing tables\",\n+        \"--master\", \"local[2]\",\n+        \"--conf\", \"spark.ui.enabled=false\",\n+        \"--conf\", \"spark.master.rest.enabled=false\",\n+        \"--conf\", s\"spark.sql.warehouse.dir=${wareHousePath.getCanonicalPath}\",\n+        \"--conf\", s\"spark.sql.test.version.index=$index\",\n+        \"--driver-java-options\", s\"-Dderby.system.home=${wareHousePath.getCanonicalPath}\",\n+        tempPyFile.getCanonicalPath)\n+      runSparkSubmit(args, Some(sparkHome.getCanonicalPath))\n+    }\n+\n+    tempPyFile.delete()\n+  }\n+\n+  test(\"backward compatibility\") {\n+    val args = Seq(\n+      \"--class\", PROCESS_TABLES.getClass.getName.stripSuffix(\"$\"),\n+      \"--name\", \"HiveExternalCatalog backward compatibility test\",\n+      \"--master\", \"local[2]\",\n+      \"--conf\", \"spark.ui.enabled=false\",\n+      \"--conf\", \"spark.master.rest.enabled=false\",\n+      \"--conf\", s\"spark.sql.warehouse.dir=${wareHousePath.getCanonicalPath}\",\n+      \"--driver-java-options\", s\"-Dderby.system.home=${wareHousePath.getCanonicalPath}\",\n+      unusedJar.toString)\n+    runSparkSubmit(args)\n+  }\n+}\n+\n+object PROCESS_TABLES extends QueryTest with SQLTestUtils {\n+  // Tests the latest version of every release line.\n+  val testingVersions = Seq(\"2.0.2\", \"2.1.1\", \"2.2.0\")",
    "line": 142
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "I thought it's needed against 2.2.0, here.",
    "commit": "7178ccfbd726149354ad2f9544ed978537efe70f",
    "createdAt": "2017-09-08T16:00:21Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.File\n+import java.nio.file.Files\n+\n+import org.apache.spark.TestUtils\n+import org.apache.spark.sql.{QueryTest, Row, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.test.SQLTestUtils\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Test HiveExternalCatalog backward compatibility.\n+ *\n+ * Note that, this test suite will automatically download spark binary packages of different\n+ * versions to a local directory `/tmp/spark-test`. If there is already a spark folder with\n+ * expected version under this local directory, e.g. `/tmp/spark-test/spark-2.0.3`, we will skip the\n+ * downloading for this spark version.\n+ */\n+class HiveExternalCatalogVersionsSuite extends SparkSubmitTestUtils {\n+  private val wareHousePath = Utils.createTempDir(namePrefix = \"warehouse\")\n+  private val tmpDataDir = Utils.createTempDir(namePrefix = \"test-data\")\n+  private val sparkTestingDir = \"/tmp/spark-test\"\n+  private val unusedJar = TestUtils.createJarWithClasses(Seq.empty)\n+\n+  override def afterAll(): Unit = {\n+    Utils.deleteRecursively(wareHousePath)\n+    Utils.deleteRecursively(tmpDataDir)\n+    super.afterAll()\n+  }\n+\n+  private def downloadSpark(version: String): Unit = {\n+    import scala.sys.process._\n+\n+    val url = s\"https://d3kbcqa49mib13.cloudfront.net/spark-$version-bin-hadoop2.7.tgz\"\n+\n+    Seq(\"wget\", url, \"-q\", \"-P\", sparkTestingDir).!\n+\n+    val downloaded = new File(sparkTestingDir, s\"spark-$version-bin-hadoop2.7.tgz\").getCanonicalPath\n+    val targetDir = new File(sparkTestingDir, s\"spark-$version\").getCanonicalPath\n+\n+    Seq(\"mkdir\", targetDir).!\n+\n+    Seq(\"tar\", \"-xzf\", downloaded, \"-C\", targetDir, \"--strip-components=1\").!\n+\n+    Seq(\"rm\", downloaded).!\n+  }\n+\n+  private def genDataDir(name: String): String = {\n+    new File(tmpDataDir, name).getCanonicalPath\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+\n+    val tempPyFile = File.createTempFile(\"test\", \".py\")\n+    Files.write(tempPyFile.toPath,\n+      s\"\"\"\n+        |from pyspark.sql import SparkSession\n+        |\n+        |spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n+        |version_index = spark.conf.get(\"spark.sql.test.version.index\", None)\n+        |\n+        |spark.sql(\"create table data_source_tbl_{} using json as select 1 i\".format(version_index))\n+        |\n+        |spark.sql(\"create table hive_compatible_data_source_tbl_\" + version_index + \\\\\n+        |          \" using parquet as select 1 i\")\n+        |\n+        |json_file = \"${genDataDir(\"json_\")}\" + str(version_index)\n+        |spark.range(1, 2).selectExpr(\"cast(id as int) as i\").write.json(json_file)\n+        |spark.sql(\"create table external_data_source_tbl_\" + version_index + \\\\\n+        |          \"(i int) using json options (path '{}')\".format(json_file))\n+        |\n+        |parquet_file = \"${genDataDir(\"parquet_\")}\" + str(version_index)\n+        |spark.range(1, 2).selectExpr(\"cast(id as int) as i\").write.parquet(parquet_file)\n+        |spark.sql(\"create table hive_compatible_external_data_source_tbl_\" + version_index + \\\\\n+        |          \"(i int) using parquet options (path '{}')\".format(parquet_file))\n+        |\n+        |json_file2 = \"${genDataDir(\"json2_\")}\" + str(version_index)\n+        |spark.range(1, 2).selectExpr(\"cast(id as int) as i\").write.json(json_file2)\n+        |spark.sql(\"create table external_table_without_schema_\" + version_index + \\\\\n+        |          \" using json options (path '{}')\".format(json_file2))\n+        |\n+        |spark.sql(\"create view v_{} as select 1 i\".format(version_index))\n+      \"\"\".stripMargin.getBytes(\"utf8\"))\n+\n+    PROCESS_TABLES.testingVersions.zipWithIndex.foreach { case (version, index) =>\n+      val sparkHome = new File(sparkTestingDir, s\"spark-$version\")\n+      if (!sparkHome.exists()) {\n+        downloadSpark(version)\n+      }\n+\n+      val args = Seq(\n+        \"--name\", \"prepare testing tables\",\n+        \"--master\", \"local[2]\",\n+        \"--conf\", \"spark.ui.enabled=false\",\n+        \"--conf\", \"spark.master.rest.enabled=false\",\n+        \"--conf\", s\"spark.sql.warehouse.dir=${wareHousePath.getCanonicalPath}\",\n+        \"--conf\", s\"spark.sql.test.version.index=$index\",\n+        \"--driver-java-options\", s\"-Dderby.system.home=${wareHousePath.getCanonicalPath}\",\n+        tempPyFile.getCanonicalPath)\n+      runSparkSubmit(args, Some(sparkHome.getCanonicalPath))\n+    }\n+\n+    tempPyFile.delete()\n+  }\n+\n+  test(\"backward compatibility\") {\n+    val args = Seq(\n+      \"--class\", PROCESS_TABLES.getClass.getName.stripSuffix(\"$\"),\n+      \"--name\", \"HiveExternalCatalog backward compatibility test\",\n+      \"--master\", \"local[2]\",\n+      \"--conf\", \"spark.ui.enabled=false\",\n+      \"--conf\", \"spark.master.rest.enabled=false\",\n+      \"--conf\", s\"spark.sql.warehouse.dir=${wareHousePath.getCanonicalPath}\",\n+      \"--driver-java-options\", s\"-Dderby.system.home=${wareHousePath.getCanonicalPath}\",\n+      unusedJar.toString)\n+    runSparkSubmit(args)\n+  }\n+}\n+\n+object PROCESS_TABLES extends QueryTest with SQLTestUtils {\n+  // Tests the latest version of every release line.\n+  val testingVersions = Seq(\"2.0.2\", \"2.1.1\", \"2.2.0\")",
    "line": 142
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "This is needed",
    "commit": "7178ccfbd726149354ad2f9544ed978537efe70f",
    "createdAt": "2017-09-08T16:34:24Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.File\n+import java.nio.file.Files\n+\n+import org.apache.spark.TestUtils\n+import org.apache.spark.sql.{QueryTest, Row, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.CatalogTableType\n+import org.apache.spark.sql.test.SQLTestUtils\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Test HiveExternalCatalog backward compatibility.\n+ *\n+ * Note that, this test suite will automatically download spark binary packages of different\n+ * versions to a local directory `/tmp/spark-test`. If there is already a spark folder with\n+ * expected version under this local directory, e.g. `/tmp/spark-test/spark-2.0.3`, we will skip the\n+ * downloading for this spark version.\n+ */\n+class HiveExternalCatalogVersionsSuite extends SparkSubmitTestUtils {\n+  private val wareHousePath = Utils.createTempDir(namePrefix = \"warehouse\")\n+  private val tmpDataDir = Utils.createTempDir(namePrefix = \"test-data\")\n+  private val sparkTestingDir = \"/tmp/spark-test\"\n+  private val unusedJar = TestUtils.createJarWithClasses(Seq.empty)\n+\n+  override def afterAll(): Unit = {\n+    Utils.deleteRecursively(wareHousePath)\n+    Utils.deleteRecursively(tmpDataDir)\n+    super.afterAll()\n+  }\n+\n+  private def downloadSpark(version: String): Unit = {\n+    import scala.sys.process._\n+\n+    val url = s\"https://d3kbcqa49mib13.cloudfront.net/spark-$version-bin-hadoop2.7.tgz\"\n+\n+    Seq(\"wget\", url, \"-q\", \"-P\", sparkTestingDir).!\n+\n+    val downloaded = new File(sparkTestingDir, s\"spark-$version-bin-hadoop2.7.tgz\").getCanonicalPath\n+    val targetDir = new File(sparkTestingDir, s\"spark-$version\").getCanonicalPath\n+\n+    Seq(\"mkdir\", targetDir).!\n+\n+    Seq(\"tar\", \"-xzf\", downloaded, \"-C\", targetDir, \"--strip-components=1\").!\n+\n+    Seq(\"rm\", downloaded).!\n+  }\n+\n+  private def genDataDir(name: String): String = {\n+    new File(tmpDataDir, name).getCanonicalPath\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+\n+    val tempPyFile = File.createTempFile(\"test\", \".py\")\n+    Files.write(tempPyFile.toPath,\n+      s\"\"\"\n+        |from pyspark.sql import SparkSession\n+        |\n+        |spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n+        |version_index = spark.conf.get(\"spark.sql.test.version.index\", None)\n+        |\n+        |spark.sql(\"create table data_source_tbl_{} using json as select 1 i\".format(version_index))\n+        |\n+        |spark.sql(\"create table hive_compatible_data_source_tbl_\" + version_index + \\\\\n+        |          \" using parquet as select 1 i\")\n+        |\n+        |json_file = \"${genDataDir(\"json_\")}\" + str(version_index)\n+        |spark.range(1, 2).selectExpr(\"cast(id as int) as i\").write.json(json_file)\n+        |spark.sql(\"create table external_data_source_tbl_\" + version_index + \\\\\n+        |          \"(i int) using json options (path '{}')\".format(json_file))\n+        |\n+        |parquet_file = \"${genDataDir(\"parquet_\")}\" + str(version_index)\n+        |spark.range(1, 2).selectExpr(\"cast(id as int) as i\").write.parquet(parquet_file)\n+        |spark.sql(\"create table hive_compatible_external_data_source_tbl_\" + version_index + \\\\\n+        |          \"(i int) using parquet options (path '{}')\".format(parquet_file))\n+        |\n+        |json_file2 = \"${genDataDir(\"json2_\")}\" + str(version_index)\n+        |spark.range(1, 2).selectExpr(\"cast(id as int) as i\").write.json(json_file2)\n+        |spark.sql(\"create table external_table_without_schema_\" + version_index + \\\\\n+        |          \" using json options (path '{}')\".format(json_file2))\n+        |\n+        |spark.sql(\"create view v_{} as select 1 i\".format(version_index))\n+      \"\"\".stripMargin.getBytes(\"utf8\"))\n+\n+    PROCESS_TABLES.testingVersions.zipWithIndex.foreach { case (version, index) =>\n+      val sparkHome = new File(sparkTestingDir, s\"spark-$version\")\n+      if (!sparkHome.exists()) {\n+        downloadSpark(version)\n+      }\n+\n+      val args = Seq(\n+        \"--name\", \"prepare testing tables\",\n+        \"--master\", \"local[2]\",\n+        \"--conf\", \"spark.ui.enabled=false\",\n+        \"--conf\", \"spark.master.rest.enabled=false\",\n+        \"--conf\", s\"spark.sql.warehouse.dir=${wareHousePath.getCanonicalPath}\",\n+        \"--conf\", s\"spark.sql.test.version.index=$index\",\n+        \"--driver-java-options\", s\"-Dderby.system.home=${wareHousePath.getCanonicalPath}\",\n+        tempPyFile.getCanonicalPath)\n+      runSparkSubmit(args, Some(sparkHome.getCanonicalPath))\n+    }\n+\n+    tempPyFile.delete()\n+  }\n+\n+  test(\"backward compatibility\") {\n+    val args = Seq(\n+      \"--class\", PROCESS_TABLES.getClass.getName.stripSuffix(\"$\"),\n+      \"--name\", \"HiveExternalCatalog backward compatibility test\",\n+      \"--master\", \"local[2]\",\n+      \"--conf\", \"spark.ui.enabled=false\",\n+      \"--conf\", \"spark.master.rest.enabled=false\",\n+      \"--conf\", s\"spark.sql.warehouse.dir=${wareHousePath.getCanonicalPath}\",\n+      \"--driver-java-options\", s\"-Dderby.system.home=${wareHousePath.getCanonicalPath}\",\n+      unusedJar.toString)\n+    runSparkSubmit(args)\n+  }\n+}\n+\n+object PROCESS_TABLES extends QueryTest with SQLTestUtils {\n+  // Tests the latest version of every release line.\n+  val testingVersions = Seq(\"2.0.2\", \"2.1.1\", \"2.2.0\")",
    "line": 142
  }],
  "prId": 19163
}]