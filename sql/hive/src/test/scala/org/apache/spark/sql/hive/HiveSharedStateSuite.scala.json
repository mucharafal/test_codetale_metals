[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I think you can just `contains(\"HiveExternalCatalog\")` for the possibility that we move the class to somewhere in the future.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-01T11:48:45Z",
    "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"the catalog should be determined at the very first\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.getClass.getName ===\n+      \"org.apache.spark.sql.hive.HiveExternalCatalog\", \"The catalog should be hive \")"
  }],
  "prId": 23709
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: use `isInstanceOf[HiveExternalCatalog]`",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T02:37:14Z",
    "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"the catalog should be determined at the very first\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.getClass.getName\n+      .contains(\"HiveExternalCatalog\"), \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.getClass.getName"
  }],
  "prId": 23709
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "IIUC these 2 tests are positive and negative cases for setting initial session configs. Can you make it more explicit in the test name, and move all the configs that can take effect during session creation to one test?",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T02:39:07Z",
    "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"the catalog should be determined at the very first\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.getClass.getName\n+      .contains(\"HiveExternalCatalog\"), \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.getClass.getName\n+      .contains(\"HiveExternalCatalog\"), \"The catalog should be shared across sessions\")\n+\n+  }\n+\n+  test(\"using initial configs to generate SharedState\") {"
  }],
  "prId": 23709
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Can we be more real-world and create SparkSession here?",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T04:24:59Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "In hive module test, there is an long cached TestHiveSession, so I use this SharedState explicitly",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T05:27:00Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "why can we build SparkSession in the above test case?",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T05:31:01Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "Individually the above test runs correctly, but will use a existing SparkSession with hive support while runs under the whole hive module. so I add the second test to insure  all configurations passed correctly.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T05:36:30Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "shall we add `beforeEach` and call `SparkSession.clearActiveSession` and `SparkSession.clearDefaultSession`?",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T06:02:07Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "if we clear session here then dozens of existing tests will fail. Or we cached that active session here and set it back in `afterEach` by calling `SparkSession.setActiveSession`?",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T06:08:36Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "Em.. It can't be done since `spark.driver.allowMultiContext` no longer exists any more.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T06:10:48Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "then shall we move the test to sql core module? We can add try-catch and make sure ClassNotFound exception is thrown.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T06:21:29Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "yes, we can move the first test to sql core module, and may leave the second in hive module to ensure all confs correct.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T06:26:48Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "I tried to move test 1 to sql module but it will fail fast with `IllegalArgumentException` in enableHiveSupport without hive class present, classnotfound can't be captured.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T06:43:58Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "can we change test 1 to create `SharedState` as well?",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T07:15:00Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "the intent of test 1 is used to verify enableHiveSupport. I think `SharedState` is already covered in test 2. Or maybe I can just delete test 1 ? ",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T07:40:03Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "test 1 can pass without your fix, right?",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T12:16:46Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "without this fix ï¼Œtest 1 fails individually but pass fully",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T13:28:55Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I'd like to move test 1 to sql core module, we can catch the `IllegalArgumentException` to check it. Then the test is reliable, it doesn't matter we run it individually or with other tests.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T13:38:26Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "IllegalArgumentException does not mean that the catalog is hive or not. It will also pass anyway with or without this fixï¼Œand the fact is that it doesn't have the chance to get to where the catalog is initialized.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-12T15:14:21Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "then let's just remove test 1, but post the code in PR description to let people know how to reproduce the bug.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-13T04:06:37Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }, {
    "author": {
      "login": "yaooqinn"
    },
    "body": "ok. both done.",
    "commit": "748821c86d063e4b57c8297ae3e259934db544c2",
    "createdAt": "2019-02-13T06:14:06Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.internal.SharedState\n+import org.apache.spark.sql.internal.StaticSQLConf._\n+import org.apache.spark.util.Utils\n+\n+class HiveSharedStateSuite extends SparkFunSuite {\n+\n+  test(\"enableHiveSupport has right to determine the catalog while using an existing sc\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val ss = SparkSession.builder().enableHiveSupport().getOrCreate()\n+    assert(ss.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be hive \")\n+\n+    val ss2 = SparkSession.builder().getOrCreate()\n+    assert(ss2.sharedState.externalCatalog.unwrapped.isInstanceOf[HiveExternalCatalog],\n+      \"The catalog should be shared across sessions\")\n+  }\n+\n+  test(\"initial configs should be passed to SharedState but not SparkContext\") {\n+    val conf = new SparkConf().setMaster(\"local\").setAppName(\"SharedState Test\")\n+    val sc = SparkContext.getOrCreate(conf)\n+    val invalidPath = \"invalid/path\"\n+    val metastorePath = Utils.createTempDir()\n+    val tmpDb = \"tmp_db\"\n+\n+    // The initial configs used to generate SharedState, none of these should affect the global\n+    // shared SparkContext's configurations. Especially, all these configs are passed to the cloned\n+    // confs inside SharedState except metastore warehouse dir.\n+    val initialConfigs = Map(\"spark.foo\" -> \"bar\",\n+      WAREHOUSE_PATH.key -> invalidPath,\n+      ConfVars.METASTOREWAREHOUSE.varname -> invalidPath,\n+      CATALOG_IMPLEMENTATION.key -> \"hive\",\n+      ConfVars.METASTORECONNECTURLKEY.varname ->\n+        s\"jdbc:derby:;databaseName=$metastorePath/metastore_db;create=true\",\n+      GLOBAL_TEMP_DATABASE.key -> tmpDb)\n+\n+    val state = new SharedState(sc, initialConfigs)",
    "line": 47
  }],
  "prId": 23709
}]