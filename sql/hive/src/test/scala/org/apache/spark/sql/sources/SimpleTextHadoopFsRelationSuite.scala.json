[{
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "btw, what does `markup` do?\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-04T21:22:35Z",
    "diffHunk": "@@ -70,43 +76,258 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")",
    "line": 141
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "print the string out?\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-04T21:23:31Z",
    "diffHunk": "@@ -70,43 +76,258 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")",
    "line": 141
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Sort of, it's for annotating test execution, so that when a test failure occur, we know which step we are executing.\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-05T00:25:29Z",
    "diffHunk": "@@ -70,43 +76,258 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")",
    "line": 141
  }],
  "prId": 9468
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Seems it is not very obvious why `expectedPartitioningFilters.intersect(nonPushedFilters).isEmpty`.\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-04T21:33:05Z",
    "diffHunk": "@@ -70,43 +76,258 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")\n+      checkAnswer(\n+        DataFrame(sqlContext, LogicalRDD(rawScan.output, rawScan.rdd)(sqlContext)),\n+        expectedRawScanAnswer)\n+\n+      markup(\"Checking full query answer\")\n+      checkAnswer(df, expectedAnswer)\n+\n+      markup(\"Checking required columns\")\n+      assert(requiredColumns === SimpleTextRelation.requiredColumns)\n \n-      assertResult(Set((2, 1), (3, 1))) {\n-        rawScan.execute().collect()\n-          .map { CatalystTypeConverters.convertToScala(_, outputSchema) }\n-          .map { case Row(a, p) => (a, p) }.toSet\n+      val nonPushedFilters = {\n+        val boundFilters = executedPlan.collect {\n+          case f: execution.Filter => f\n+        } match {\n+          case Nil => Nil\n+          case Seq(f) => splitConjunctivePredicates(f.condition)\n+          case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n+        }\n+\n+        boundFilters.map {\n+          _.transform { case a: AttributeReference => UnresolvedAttribute(a.name) }\n+        }.toSet\n       }\n \n-      checkAnswer(df, Row(3, 1))\n+      markup(\"Checking pushed filters\")\n+      assert(SimpleTextRelation.pushedFilters === pushedFilters.toSet)\n+\n+      val expectedInconvertibleFilters = inconvertibleFilters.map(_.expr).toSet\n+      val expectedUnhandledFilters = unhandledFilters.map(_.expr).toSet\n+      val expectedPartitioningFilters = partitioningFilters.map(_.expr).toSet\n+\n+      markup(\"Checking unhandled, inconvertible, and partitioning filters\")\n+      assert(expectedInconvertibleFilters ++ expectedUnhandledFilters === nonPushedFilters)\n+      assert(expectedPartitioningFilters.intersect(nonPushedFilters).isEmpty)"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Because filters referencing partition columns are not even converted to data source `Filter`s. Adding comments here.\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-05T00:28:38Z",
    "diffHunk": "@@ -70,43 +76,258 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")\n+      checkAnswer(\n+        DataFrame(sqlContext, LogicalRDD(rawScan.output, rawScan.rdd)(sqlContext)),\n+        expectedRawScanAnswer)\n+\n+      markup(\"Checking full query answer\")\n+      checkAnswer(df, expectedAnswer)\n+\n+      markup(\"Checking required columns\")\n+      assert(requiredColumns === SimpleTextRelation.requiredColumns)\n \n-      assertResult(Set((2, 1), (3, 1))) {\n-        rawScan.execute().collect()\n-          .map { CatalystTypeConverters.convertToScala(_, outputSchema) }\n-          .map { case Row(a, p) => (a, p) }.toSet\n+      val nonPushedFilters = {\n+        val boundFilters = executedPlan.collect {\n+          case f: execution.Filter => f\n+        } match {\n+          case Nil => Nil\n+          case Seq(f) => splitConjunctivePredicates(f.condition)\n+          case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n+        }\n+\n+        boundFilters.map {\n+          _.transform { case a: AttributeReference => UnresolvedAttribute(a.name) }\n+        }.toSet\n       }\n \n-      checkAnswer(df, Row(3, 1))\n+      markup(\"Checking pushed filters\")\n+      assert(SimpleTextRelation.pushedFilters === pushedFilters.toSet)\n+\n+      val expectedInconvertibleFilters = inconvertibleFilters.map(_.expr).toSet\n+      val expectedUnhandledFilters = unhandledFilters.map(_.expr).toSet\n+      val expectedPartitioningFilters = partitioningFilters.map(_.expr).toSet\n+\n+      markup(\"Checking unhandled, inconvertible, and partitioning filters\")\n+      assert(expectedInconvertibleFilters ++ expectedUnhandledFilters === nonPushedFilters)\n+      assert(expectedPartitioningFilters.intersect(nonPushedFilters).isEmpty)"
  }],
  "prId": 9468
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "btw, where do we define what can be pushed and what cannot?\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-04T21:35:39Z",
    "diffHunk": "@@ -70,43 +76,258 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")\n+      checkAnswer(\n+        DataFrame(sqlContext, LogicalRDD(rawScan.output, rawScan.rdd)(sqlContext)),\n+        expectedRawScanAnswer)\n+\n+      markup(\"Checking full query answer\")\n+      checkAnswer(df, expectedAnswer)\n+\n+      markup(\"Checking required columns\")\n+      assert(requiredColumns === SimpleTextRelation.requiredColumns)\n \n-      assertResult(Set((2, 1), (3, 1))) {\n-        rawScan.execute().collect()\n-          .map { CatalystTypeConverters.convertToScala(_, outputSchema) }\n-          .map { case Row(a, p) => (a, p) }.toSet\n+      val nonPushedFilters = {\n+        val boundFilters = executedPlan.collect {\n+          case f: execution.Filter => f\n+        } match {\n+          case Nil => Nil\n+          case Seq(f) => splitConjunctivePredicates(f.condition)\n+          case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n+        }\n+\n+        boundFilters.map {\n+          _.transform { case a: AttributeReference => UnresolvedAttribute(a.name) }\n+        }.toSet\n       }\n \n-      checkAnswer(df, Row(3, 1))\n+      markup(\"Checking pushed filters\")\n+      assert(SimpleTextRelation.pushedFilters === pushedFilters.toSet)\n+\n+      val expectedInconvertibleFilters = inconvertibleFilters.map(_.expr).toSet\n+      val expectedUnhandledFilters = unhandledFilters.map(_.expr).toSet\n+      val expectedPartitioningFilters = partitioningFilters.map(_.expr).toSet\n+\n+      markup(\"Checking unhandled, inconvertible, and partitioning filters\")\n+      assert(expectedInconvertibleFilters ++ expectedUnhandledFilters === nonPushedFilters)\n+      assert(expectedPartitioningFilters.intersect(nonPushedFilters).isEmpty)\n     }\n   }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'p > 0,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('c, 'p),\n+    filter = 'a < 3 && 'p > 0,\n+    requiredColumns = Seq(\"c\", \"a\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('a < 3),\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(\"val_0\", 1, 0),\n+      Row(\"val_1\", 1, 1),\n+      Row(\"val_2\", 1, 2),\n+      Row(\"val_3\", 1, 3),\n+      Row(\"val_4\", 1, 4),\n+      Row(\"val_5\", 1, 5),\n+      Row(\"val_6\", 1, 6),\n+      Row(\"val_7\", 1, 7),\n+      Row(\"val_8\", 1, 8),\n+      Row(\"val_9\", 1, 9))\n+  } {\n+    Seq(\n+      Row(\"val_0\", 1),\n+      Row(\"val_1\", 1),\n+      Row(\"val_2\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'a > 8,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(9, 18, \"val_9\", 0),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(9, 18, \"val_9\", 0),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a > 8,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(18, 0),\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(18, 0),\n+      Row(18, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a > 8 && 'p > 0,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Seq('p > 0)\n+  ) {\n+    Seq(\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(18, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'c > \"val_7\" && 'b < 18 && 'p > 0,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"c\", \"val_7\")),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('b < 18),\n+    partitioningFilters = Seq('p > 0)\n+  ) {\n+    Seq(\n+      Row(16, 1),\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(16, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a % 2 === 0 && 'c > \"val_7\" && 'b < 18 && 'p > 0,",
    "line": 326
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "1. Partitioning filters don't participate filter push-down, they are handled separately in `DataSourceStrategy`\n2. Catalyst filter `Expression`s that cannot be converted to data source `Filter`s are not pushed down (e.g. UDF and filters referencing multiple columns). This behavior is defined in `DataSourceStrategy`.\n3. Catalyst filter `Expression`s that can be converted to data source `Filter`s but cannot be handled by the underlying data source are not pushed down (e.g. returned from `BaseRelation.unhandledFilters()`).\n\nAdding the above comments to the test.\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-05T00:43:38Z",
    "diffHunk": "@@ -70,43 +76,258 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")\n+      checkAnswer(\n+        DataFrame(sqlContext, LogicalRDD(rawScan.output, rawScan.rdd)(sqlContext)),\n+        expectedRawScanAnswer)\n+\n+      markup(\"Checking full query answer\")\n+      checkAnswer(df, expectedAnswer)\n+\n+      markup(\"Checking required columns\")\n+      assert(requiredColumns === SimpleTextRelation.requiredColumns)\n \n-      assertResult(Set((2, 1), (3, 1))) {\n-        rawScan.execute().collect()\n-          .map { CatalystTypeConverters.convertToScala(_, outputSchema) }\n-          .map { case Row(a, p) => (a, p) }.toSet\n+      val nonPushedFilters = {\n+        val boundFilters = executedPlan.collect {\n+          case f: execution.Filter => f\n+        } match {\n+          case Nil => Nil\n+          case Seq(f) => splitConjunctivePredicates(f.condition)\n+          case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n+        }\n+\n+        boundFilters.map {\n+          _.transform { case a: AttributeReference => UnresolvedAttribute(a.name) }\n+        }.toSet\n       }\n \n-      checkAnswer(df, Row(3, 1))\n+      markup(\"Checking pushed filters\")\n+      assert(SimpleTextRelation.pushedFilters === pushedFilters.toSet)\n+\n+      val expectedInconvertibleFilters = inconvertibleFilters.map(_.expr).toSet\n+      val expectedUnhandledFilters = unhandledFilters.map(_.expr).toSet\n+      val expectedPartitioningFilters = partitioningFilters.map(_.expr).toSet\n+\n+      markup(\"Checking unhandled, inconvertible, and partitioning filters\")\n+      assert(expectedInconvertibleFilters ++ expectedUnhandledFilters === nonPushedFilters)\n+      assert(expectedPartitioningFilters.intersect(nonPushedFilters).isEmpty)\n     }\n   }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'p > 0,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('c, 'p),\n+    filter = 'a < 3 && 'p > 0,\n+    requiredColumns = Seq(\"c\", \"a\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('a < 3),\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(\"val_0\", 1, 0),\n+      Row(\"val_1\", 1, 1),\n+      Row(\"val_2\", 1, 2),\n+      Row(\"val_3\", 1, 3),\n+      Row(\"val_4\", 1, 4),\n+      Row(\"val_5\", 1, 5),\n+      Row(\"val_6\", 1, 6),\n+      Row(\"val_7\", 1, 7),\n+      Row(\"val_8\", 1, 8),\n+      Row(\"val_9\", 1, 9))\n+  } {\n+    Seq(\n+      Row(\"val_0\", 1),\n+      Row(\"val_1\", 1),\n+      Row(\"val_2\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'a > 8,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(9, 18, \"val_9\", 0),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(9, 18, \"val_9\", 0),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a > 8,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(18, 0),\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(18, 0),\n+      Row(18, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a > 8 && 'p > 0,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Seq('p > 0)\n+  ) {\n+    Seq(\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(18, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'c > \"val_7\" && 'b < 18 && 'p > 0,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"c\", \"val_7\")),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('b < 18),\n+    partitioningFilters = Seq('p > 0)\n+  ) {\n+    Seq(\n+      Row(16, 1),\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(16, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a % 2 === 0 && 'c > \"val_7\" && 'b < 18 && 'p > 0,",
    "line": 326
  }],
  "prId": 9468
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Can we have a test to have a column that appear in both handled and unhandled filters?\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-04T21:36:56Z",
    "diffHunk": "@@ -70,43 +76,258 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")\n+      checkAnswer(\n+        DataFrame(sqlContext, LogicalRDD(rawScan.output, rawScan.rdd)(sqlContext)),\n+        expectedRawScanAnswer)\n+\n+      markup(\"Checking full query answer\")\n+      checkAnswer(df, expectedAnswer)\n+\n+      markup(\"Checking required columns\")\n+      assert(requiredColumns === SimpleTextRelation.requiredColumns)\n \n-      assertResult(Set((2, 1), (3, 1))) {\n-        rawScan.execute().collect()\n-          .map { CatalystTypeConverters.convertToScala(_, outputSchema) }\n-          .map { case Row(a, p) => (a, p) }.toSet\n+      val nonPushedFilters = {\n+        val boundFilters = executedPlan.collect {\n+          case f: execution.Filter => f\n+        } match {\n+          case Nil => Nil\n+          case Seq(f) => splitConjunctivePredicates(f.condition)\n+          case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n+        }\n+\n+        boundFilters.map {\n+          _.transform { case a: AttributeReference => UnresolvedAttribute(a.name) }\n+        }.toSet\n       }\n \n-      checkAnswer(df, Row(3, 1))\n+      markup(\"Checking pushed filters\")\n+      assert(SimpleTextRelation.pushedFilters === pushedFilters.toSet)\n+\n+      val expectedInconvertibleFilters = inconvertibleFilters.map(_.expr).toSet\n+      val expectedUnhandledFilters = unhandledFilters.map(_.expr).toSet\n+      val expectedPartitioningFilters = partitioningFilters.map(_.expr).toSet\n+\n+      markup(\"Checking unhandled, inconvertible, and partitioning filters\")\n+      assert(expectedInconvertibleFilters ++ expectedUnhandledFilters === nonPushedFilters)\n+      assert(expectedPartitioningFilters.intersect(nonPushedFilters).isEmpty)\n     }\n   }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'p > 0,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('c, 'p),\n+    filter = 'a < 3 && 'p > 0,\n+    requiredColumns = Seq(\"c\", \"a\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('a < 3),\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(\"val_0\", 1, 0),\n+      Row(\"val_1\", 1, 1),\n+      Row(\"val_2\", 1, 2),\n+      Row(\"val_3\", 1, 3),\n+      Row(\"val_4\", 1, 4),\n+      Row(\"val_5\", 1, 5),\n+      Row(\"val_6\", 1, 6),\n+      Row(\"val_7\", 1, 7),\n+      Row(\"val_8\", 1, 8),\n+      Row(\"val_9\", 1, 9))\n+  } {\n+    Seq(\n+      Row(\"val_0\", 1),\n+      Row(\"val_1\", 1),\n+      Row(\"val_2\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'a > 8,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(9, 18, \"val_9\", 0),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(9, 18, \"val_9\", 0),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a > 8,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(18, 0),\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(18, 0),\n+      Row(18, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a > 8 && 'p > 0,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Seq('p > 0)\n+  ) {\n+    Seq(\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(18, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'c > \"val_7\" && 'b < 18 && 'p > 0,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"c\", \"val_7\")),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('b < 18),\n+    partitioningFilters = Seq('p > 0)\n+  ) {\n+    Seq(\n+      Row(16, 1),\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(16, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a % 2 === 0 && 'c > \"val_7\" && 'b < 18 && 'p > 0,\n+    requiredColumns = Seq(\"b\", \"a\"),\n+    pushedFilters = Seq(GreaterThan(\"c\", \"val_7\")),",
    "line": 328
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Added.\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-05T00:56:57Z",
    "diffHunk": "@@ -70,43 +76,258 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")\n+      checkAnswer(\n+        DataFrame(sqlContext, LogicalRDD(rawScan.output, rawScan.rdd)(sqlContext)),\n+        expectedRawScanAnswer)\n+\n+      markup(\"Checking full query answer\")\n+      checkAnswer(df, expectedAnswer)\n+\n+      markup(\"Checking required columns\")\n+      assert(requiredColumns === SimpleTextRelation.requiredColumns)\n \n-      assertResult(Set((2, 1), (3, 1))) {\n-        rawScan.execute().collect()\n-          .map { CatalystTypeConverters.convertToScala(_, outputSchema) }\n-          .map { case Row(a, p) => (a, p) }.toSet\n+      val nonPushedFilters = {\n+        val boundFilters = executedPlan.collect {\n+          case f: execution.Filter => f\n+        } match {\n+          case Nil => Nil\n+          case Seq(f) => splitConjunctivePredicates(f.condition)\n+          case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n+        }\n+\n+        boundFilters.map {\n+          _.transform { case a: AttributeReference => UnresolvedAttribute(a.name) }\n+        }.toSet\n       }\n \n-      checkAnswer(df, Row(3, 1))\n+      markup(\"Checking pushed filters\")\n+      assert(SimpleTextRelation.pushedFilters === pushedFilters.toSet)\n+\n+      val expectedInconvertibleFilters = inconvertibleFilters.map(_.expr).toSet\n+      val expectedUnhandledFilters = unhandledFilters.map(_.expr).toSet\n+      val expectedPartitioningFilters = partitioningFilters.map(_.expr).toSet\n+\n+      markup(\"Checking unhandled, inconvertible, and partitioning filters\")\n+      assert(expectedInconvertibleFilters ++ expectedUnhandledFilters === nonPushedFilters)\n+      assert(expectedPartitioningFilters.intersect(nonPushedFilters).isEmpty)\n     }\n   }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'p > 0,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('c, 'p),\n+    filter = 'a < 3 && 'p > 0,\n+    requiredColumns = Seq(\"c\", \"a\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('a < 3),\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(\"val_0\", 1, 0),\n+      Row(\"val_1\", 1, 1),\n+      Row(\"val_2\", 1, 2),\n+      Row(\"val_3\", 1, 3),\n+      Row(\"val_4\", 1, 4),\n+      Row(\"val_5\", 1, 5),\n+      Row(\"val_6\", 1, 6),\n+      Row(\"val_7\", 1, 7),\n+      Row(\"val_8\", 1, 8),\n+      Row(\"val_9\", 1, 9))\n+  } {\n+    Seq(\n+      Row(\"val_0\", 1),\n+      Row(\"val_1\", 1),\n+      Row(\"val_2\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'a > 8,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(9, 18, \"val_9\", 0),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(9, 18, \"val_9\", 0),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a > 8,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(18, 0),\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(18, 0),\n+      Row(18, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a > 8 && 'p > 0,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"a\", 8)),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Seq('p > 0)\n+  ) {\n+    Seq(\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(18, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'c > \"val_7\" && 'b < 18 && 'p > 0,\n+    requiredColumns = Seq(\"b\"),\n+    pushedFilters = Seq(GreaterThan(\"c\", \"val_7\")),\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('b < 18),\n+    partitioningFilters = Seq('p > 0)\n+  ) {\n+    Seq(\n+      Row(16, 1),\n+      Row(18, 1))\n+  } {\n+    Seq(\n+      Row(16, 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('b, 'p),\n+    filter = 'a % 2 === 0 && 'c > \"val_7\" && 'b < 18 && 'p > 0,\n+    requiredColumns = Seq(\"b\", \"a\"),\n+    pushedFilters = Seq(GreaterThan(\"c\", \"val_7\")),",
    "line": 328
  }],
  "prId": 9468
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Should we have `'p > 0`?\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-06T05:21:24Z",
    "diffHunk": "@@ -70,43 +76,297 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * For filter push-down, the following filters are not pushed-down.\n+   *\n+   * 1. Partitioning filters don't participate filter push-down, they are handled separately in\n+   *    `DataSourceStrategy`\n+   *\n+   * 2. Catalyst filter `Expression`s that cannot be converted to data source `Filter`s are not\n+   *    pushed down (e.g. UDF and filters referencing multiple columns).\n+   *\n+   * 3. Catalyst filter `Expression`s that can be converted to data source `Filter`s but cannot be\n+   *    handled by the underlying data source are not pushed down (e.g. returned from\n+   *    `BaseRelation.unhandledFilters()`).\n+   *\n+   *    Note that for [[SimpleTextRelation]], all data source [[Filter]]s other than [[GreaterThan]]\n+   *    are unhandled.  We made this assumption in [[SimpleTextRelation.unhandledFilters()]] only\n+   *    for testing purposes.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")\n+      checkAnswer(\n+        DataFrame(sqlContext, LogicalRDD(rawScan.output, rawScan.rdd)(sqlContext)),\n+        expectedRawScanAnswer)\n+\n+      markup(\"Checking full query answer\")\n+      checkAnswer(df, expectedAnswer)\n+\n+      markup(\"Checking required columns\")\n+      assert(requiredColumns === SimpleTextRelation.requiredColumns)\n+\n+      val nonPushedFilters = {\n+        val boundFilters = executedPlan.collect {\n+          case f: execution.Filter => f\n+        } match {\n+          case Nil => Nil\n+          case Seq(f) => splitConjunctivePredicates(f.condition)\n+          case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n+        }\n \n-      assertResult(Set((2, 1), (3, 1))) {\n-        rawScan.execute().collect()\n-          .map { CatalystTypeConverters.convertToScala(_, outputSchema) }\n-          .map { case Row(a, p) => (a, p) }.toSet\n+        // Unbound these bound filters so that we can easily compare them with expected results.\n+        boundFilters.map {\n+          _.transform { case a: AttributeReference => UnresolvedAttribute(a.name) }\n+        }.toSet\n       }\n \n-      checkAnswer(df, Row(3, 1))\n+      markup(\"Checking pushed filters\")\n+      assert(SimpleTextRelation.pushedFilters === pushedFilters.toSet)\n+\n+      val expectedInconvertibleFilters = inconvertibleFilters.map(_.expr).toSet\n+      val expectedUnhandledFilters = unhandledFilters.map(_.expr).toSet\n+      val expectedPartitioningFilters = partitioningFilters.map(_.expr).toSet\n+\n+      markup(\"Checking unhandled, inconvertible, and partitioning filters\")\n+      assert(expectedInconvertibleFilters ++ expectedUnhandledFilters === nonPushedFilters)\n+      // Partitioning filters are handled separately and don't participate filter push-down. So they\n+      // shouldn't be part of non-pushed filters.\n+      assert(expectedPartitioningFilters.intersect(nonPushedFilters).isEmpty)\n     }\n   }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'p > 0,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('c, 'p),\n+    filter = 'a < 3 && 'p > 0,\n+    requiredColumns = Seq(\"c\", \"a\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('a < 3),\n+    partitioningFilters = Nil"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Good catch... Hardened partitioning filters check in my last commit.\n",
    "commit": "433632ad21b42cd9a5a636febbcabe6a9d9eb56a",
    "createdAt": "2015-11-06T08:48:19Z",
    "diffHunk": "@@ -70,43 +76,297 @@ class SimpleTextHadoopFsRelationSuite extends HadoopFsRelationTest {\n     }\n   }\n \n-  private val writer = testDF.write.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-  private val reader = sqlContext.read.option(\"dataSchema\", dataSchema.json).format(dataSourceName)\n-\n-  test(\"unhandledFilters\") {\n-    withTempPath { dir =>\n-\n-      val path = dir.getCanonicalPath\n-      writer.save(s\"$path/p=0\")\n-      writer.save(s\"$path/p=1\")\n-\n-      val isOdd = udf((_: Int) % 2 == 1)\n-      val df = reader.load(path)\n-        .filter(\n-          // This filter is inconvertible\n-          isOdd('a) &&\n-            // This filter is convertible but unhandled\n-            'a > 1 &&\n-            // This filter is convertible and handled\n-            'b > \"val_1\" &&\n-            // This filter references a partiiton column, won't be pushed down\n-            'p === 1\n-        ).select('a, 'p)\n-      val rawScan = df.queryExecution.executedPlan collect {\n+  private var tempPath: File = _\n+\n+  private var partitionedDF: DataFrame = _\n+\n+  private val partitionedDataSchema: StructType = StructType('a.int :: 'b.int :: 'c.string :: Nil)\n+\n+  protected override def beforeAll(): Unit = {\n+    this.tempPath = Utils.createTempDir()\n+\n+    val df = sqlContext.range(10).select(\n+      'id cast IntegerType as 'a,\n+      ('id cast IntegerType) * 2 as 'b,\n+      concat(lit(\"val_\"), 'id) as 'c\n+    )\n+\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=0\")\n+    partitionedWriter(df).save(s\"${tempPath.getCanonicalPath}/p=1\")\n+\n+    partitionedDF = partitionedReader.load(tempPath.getCanonicalPath)\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    Utils.deleteRecursively(tempPath)\n+  }\n+\n+  private def partitionedWriter(df: DataFrame) =\n+    df.write.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  private def partitionedReader =\n+    sqlContext.read.option(\"dataSchema\", partitionedDataSchema.json).format(dataSourceName)\n+\n+  /**\n+   * Constructs test cases that test column pruning and filter push-down.\n+   *\n+   * For filter push-down, the following filters are not pushed-down.\n+   *\n+   * 1. Partitioning filters don't participate filter push-down, they are handled separately in\n+   *    `DataSourceStrategy`\n+   *\n+   * 2. Catalyst filter `Expression`s that cannot be converted to data source `Filter`s are not\n+   *    pushed down (e.g. UDF and filters referencing multiple columns).\n+   *\n+   * 3. Catalyst filter `Expression`s that can be converted to data source `Filter`s but cannot be\n+   *    handled by the underlying data source are not pushed down (e.g. returned from\n+   *    `BaseRelation.unhandledFilters()`).\n+   *\n+   *    Note that for [[SimpleTextRelation]], all data source [[Filter]]s other than [[GreaterThan]]\n+   *    are unhandled.  We made this assumption in [[SimpleTextRelation.unhandledFilters()]] only\n+   *    for testing purposes.\n+   *\n+   * @param projections Projection list of the query\n+   * @param filter Filter condition of the query\n+   * @param requiredColumns Expected names of required columns\n+   * @param pushedFilters Expected data source [[Filter]]s that are pushed down\n+   * @param inconvertibleFilters Expected Catalyst filter [[Expression]]s that cannot be converted\n+   *        to data source [[Filter]]s\n+   * @param unhandledFilters Expected Catalyst flter [[Expression]]s that can be converted to data\n+   *        source [[Filter]]s but cannot be handled by the data source relation\n+   * @param partitioningFilters Expected Catalyst filter [[Expression]]s that reference partition\n+   *        columns\n+   * @param expectedRawScanAnswer Expected query result of the raw table scan returned by the data\n+   *        source relation\n+   * @param expectedAnswer Expected query result of the full query\n+   */\n+  def testPruningAndFiltering(\n+      projections: Seq[Column],\n+      filter: Column,\n+      requiredColumns: Seq[String],\n+      pushedFilters: Seq[Filter],\n+      inconvertibleFilters: Seq[Column],\n+      unhandledFilters: Seq[Column],\n+      partitioningFilters: Seq[Column])(\n+      expectedRawScanAnswer: => Seq[Row])(\n+      expectedAnswer: => Seq[Row]): Unit = {\n+    test(s\"pruning and filtering: df.select(${projections.mkString(\", \")}).where($filter)\") {\n+      val df = partitionedDF.where(filter).select(projections: _*)\n+      val queryExecution = df.queryExecution\n+      val executedPlan = queryExecution.executedPlan\n+\n+      val rawScan = executedPlan.collect {\n         case p: PhysicalRDD => p\n       } match {\n-        case Seq(p) => p\n+        case Seq(scan) => scan\n+        case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n       }\n \n-      val outputSchema = new StructType().add(\"a\", IntegerType).add(\"p\", IntegerType)\n+      markup(\"Checking raw scan answer\")\n+      checkAnswer(\n+        DataFrame(sqlContext, LogicalRDD(rawScan.output, rawScan.rdd)(sqlContext)),\n+        expectedRawScanAnswer)\n+\n+      markup(\"Checking full query answer\")\n+      checkAnswer(df, expectedAnswer)\n+\n+      markup(\"Checking required columns\")\n+      assert(requiredColumns === SimpleTextRelation.requiredColumns)\n+\n+      val nonPushedFilters = {\n+        val boundFilters = executedPlan.collect {\n+          case f: execution.Filter => f\n+        } match {\n+          case Nil => Nil\n+          case Seq(f) => splitConjunctivePredicates(f.condition)\n+          case _ => fail(s\"More than one PhysicalRDD found\\n$queryExecution\")\n+        }\n \n-      assertResult(Set((2, 1), (3, 1))) {\n-        rawScan.execute().collect()\n-          .map { CatalystTypeConverters.convertToScala(_, outputSchema) }\n-          .map { case Row(a, p) => (a, p) }.toSet\n+        // Unbound these bound filters so that we can easily compare them with expected results.\n+        boundFilters.map {\n+          _.transform { case a: AttributeReference => UnresolvedAttribute(a.name) }\n+        }.toSet\n       }\n \n-      checkAnswer(df, Row(3, 1))\n+      markup(\"Checking pushed filters\")\n+      assert(SimpleTextRelation.pushedFilters === pushedFilters.toSet)\n+\n+      val expectedInconvertibleFilters = inconvertibleFilters.map(_.expr).toSet\n+      val expectedUnhandledFilters = unhandledFilters.map(_.expr).toSet\n+      val expectedPartitioningFilters = partitioningFilters.map(_.expr).toSet\n+\n+      markup(\"Checking unhandled, inconvertible, and partitioning filters\")\n+      assert(expectedInconvertibleFilters ++ expectedUnhandledFilters === nonPushedFilters)\n+      // Partitioning filters are handled separately and don't participate filter push-down. So they\n+      // shouldn't be part of non-pushed filters.\n+      assert(expectedPartitioningFilters.intersect(nonPushedFilters).isEmpty)\n     }\n   }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('*),\n+    filter = 'p > 0,\n+    requiredColumns = Seq(\"a\", \"b\", \"c\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Nil,\n+    partitioningFilters = Nil\n+  ) {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  } {\n+    Seq(\n+      Row(0, 0, \"val_0\", 1),\n+      Row(1, 2, \"val_1\", 1),\n+      Row(2, 4, \"val_2\", 1),\n+      Row(3, 6, \"val_3\", 1),\n+      Row(4, 8, \"val_4\", 1),\n+      Row(5, 10, \"val_5\", 1),\n+      Row(6, 12, \"val_6\", 1),\n+      Row(7, 14, \"val_7\", 1),\n+      Row(8, 16, \"val_8\", 1),\n+      Row(9, 18, \"val_9\", 1))\n+  }\n+\n+  testPruningAndFiltering(\n+    projections = Seq('c, 'p),\n+    filter = 'a < 3 && 'p > 0,\n+    requiredColumns = Seq(\"c\", \"a\"),\n+    pushedFilters = Nil,\n+    inconvertibleFilters = Nil,\n+    unhandledFilters = Seq('a < 3),\n+    partitioningFilters = Nil"
  }],
  "prId": 9468
}]