[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Any reason why this is Hive only?",
    "commit": "aa0cf0a80dfb00b7d367b2a58ecf0ae6ddeee16d",
    "createdAt": "2017-02-09T05:56:36Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\n+import org.apache.spark.sql.execution.SQLViewSuite\n+import org.apache.spark.sql.hive.test.{TestHive, TestHiveSingleton}\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A test suite for Hive view related functionality.\n+ */\n+class HiveSQLViewSuite extends SQLViewSuite with TestHiveSingleton {\n+  protected override val spark: SparkSession = TestHive.sparkSession\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    // Create a simple table with two columns: id and id1\n+    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      spark.sql(s\"DROP TABLE IF EXISTS jt\")\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  import testImplicits._\n+\n+  test(\"create a permanent/temp view using a hive, built-in, and permanent user function\") {\n+    val permanentFuncName = \"myUpper\"\n+    val permanentFuncClass =\n+      classOf[org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper].getCanonicalName\n+    val builtInFuncNameInLowerCase = \"abs\"\n+    val builtInFuncNameInMixedCase = \"aBs\"\n+    val hiveFuncName = \"histogram_numeric\"\n+\n+    withUserDefinedFunction(permanentFuncName -> false) {\n+      sql(s\"CREATE FUNCTION $permanentFuncName AS '$permanentFuncClass'\")\n+      withTable(\"tab1\") {\n+        (1 to 10).map(i => (s\"$i\", i)).toDF(\"str\", \"id\").write.saveAsTable(\"tab1\")\n+        Seq(\"VIEW\", \"TEMPORARY VIEW\").foreach { viewMode =>\n+          withView(\"view1\") {\n+            sql(\n+              s\"\"\"\n+                 |CREATE $viewMode view1\n+                 |AS SELECT\n+                 |$permanentFuncName(str),\n+                 |$builtInFuncNameInLowerCase(id),\n+                 |$builtInFuncNameInMixedCase(id) as aBs,\n+                 |$hiveFuncName(id, 5) over()\n+                 |FROM tab1\n+               \"\"\".stripMargin)\n+            checkAnswer(sql(\"select count(*) FROM view1\"), Row(10))\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"create a permanent/temp view using a temporary function\") {\n+    val tempFunctionName = \"temp\"\n+    val functionClass =\n+      classOf[org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper].getCanonicalName\n+    withUserDefinedFunction(tempFunctionName -> true) {\n+      sql(s\"CREATE TEMPORARY FUNCTION $tempFunctionName AS '$functionClass'\")\n+      withView(\"view1\", \"tempView1\") {\n+        withTable(\"tab1\") {\n+          (1 to 10).map(i => s\"$i\").toDF(\"id\").write.saveAsTable(\"tab1\")\n+\n+          // temporary view\n+          sql(s\"CREATE TEMPORARY VIEW tempView1 AS SELECT $tempFunctionName(id) from tab1\")\n+          checkAnswer(sql(\"select count(*) FROM tempView1\"), Row(10))\n+\n+          // permanent view\n+          val e = intercept[AnalysisException] {\n+            sql(s\"CREATE VIEW view1 AS SELECT $tempFunctionName(id) from tab1\")\n+          }.getMessage\n+          assert(e.contains(\"Not allowed to create a permanent view `view1` by referencing \" +\n+            s\"a temporary function `$tempFunctionName`\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"create hive view for json table\") {\n+    // json table is not hive-compatible, make sure the new flag fix it.\n+    withView(\"testView\") {\n+      sql(\"CREATE VIEW testView AS SELECT id FROM jt\")\n+      checkAnswer(sql(\"SELECT * FROM testView ORDER BY id\"), (1 to 9).map(i => Row(i)))\n+    }\n+  }\n+\n+  test(\"create hive view for partitioned parquet table\") {\n+    // partitioned parquet table is not hive-compatible, make sure the new flag fix it.\n+    withTable(\"parTable\") {\n+      withView(\"testView\") {\n+        val df = Seq(1 -> \"a\").toDF(\"i\", \"j\")\n+        df.write.format(\"parquet\").partitionBy(\"i\").saveAsTable(\"parTable\")\n+        sql(\"CREATE VIEW testView AS SELECT i, j FROM parTable\")\n+        checkAnswer(sql(\"SELECT * FROM testView\"), Row(1, \"a\"))\n+      }\n+    }\n+  }\n+\n+  test(\"create hive view for joined tables\") {"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "I moved this to `HiveSQLViewSuite` just because the test name describes itself to be hive specific.",
    "commit": "aa0cf0a80dfb00b7d367b2a58ecf0ae6ddeee16d",
    "createdAt": "2017-02-09T06:37:45Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\n+import org.apache.spark.sql.execution.SQLViewSuite\n+import org.apache.spark.sql.hive.test.{TestHive, TestHiveSingleton}\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A test suite for Hive view related functionality.\n+ */\n+class HiveSQLViewSuite extends SQLViewSuite with TestHiveSingleton {\n+  protected override val spark: SparkSession = TestHive.sparkSession\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    // Create a simple table with two columns: id and id1\n+    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      spark.sql(s\"DROP TABLE IF EXISTS jt\")\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  import testImplicits._\n+\n+  test(\"create a permanent/temp view using a hive, built-in, and permanent user function\") {\n+    val permanentFuncName = \"myUpper\"\n+    val permanentFuncClass =\n+      classOf[org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper].getCanonicalName\n+    val builtInFuncNameInLowerCase = \"abs\"\n+    val builtInFuncNameInMixedCase = \"aBs\"\n+    val hiveFuncName = \"histogram_numeric\"\n+\n+    withUserDefinedFunction(permanentFuncName -> false) {\n+      sql(s\"CREATE FUNCTION $permanentFuncName AS '$permanentFuncClass'\")\n+      withTable(\"tab1\") {\n+        (1 to 10).map(i => (s\"$i\", i)).toDF(\"str\", \"id\").write.saveAsTable(\"tab1\")\n+        Seq(\"VIEW\", \"TEMPORARY VIEW\").foreach { viewMode =>\n+          withView(\"view1\") {\n+            sql(\n+              s\"\"\"\n+                 |CREATE $viewMode view1\n+                 |AS SELECT\n+                 |$permanentFuncName(str),\n+                 |$builtInFuncNameInLowerCase(id),\n+                 |$builtInFuncNameInMixedCase(id) as aBs,\n+                 |$hiveFuncName(id, 5) over()\n+                 |FROM tab1\n+               \"\"\".stripMargin)\n+            checkAnswer(sql(\"select count(*) FROM view1\"), Row(10))\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"create a permanent/temp view using a temporary function\") {\n+    val tempFunctionName = \"temp\"\n+    val functionClass =\n+      classOf[org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper].getCanonicalName\n+    withUserDefinedFunction(tempFunctionName -> true) {\n+      sql(s\"CREATE TEMPORARY FUNCTION $tempFunctionName AS '$functionClass'\")\n+      withView(\"view1\", \"tempView1\") {\n+        withTable(\"tab1\") {\n+          (1 to 10).map(i => s\"$i\").toDF(\"id\").write.saveAsTable(\"tab1\")\n+\n+          // temporary view\n+          sql(s\"CREATE TEMPORARY VIEW tempView1 AS SELECT $tempFunctionName(id) from tab1\")\n+          checkAnswer(sql(\"select count(*) FROM tempView1\"), Row(10))\n+\n+          // permanent view\n+          val e = intercept[AnalysisException] {\n+            sql(s\"CREATE VIEW view1 AS SELECT $tempFunctionName(id) from tab1\")\n+          }.getMessage\n+          assert(e.contains(\"Not allowed to create a permanent view `view1` by referencing \" +\n+            s\"a temporary function `$tempFunctionName`\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"create hive view for json table\") {\n+    // json table is not hive-compatible, make sure the new flag fix it.\n+    withView(\"testView\") {\n+      sql(\"CREATE VIEW testView AS SELECT id FROM jt\")\n+      checkAnswer(sql(\"SELECT * FROM testView ORDER BY id\"), (1 to 9).map(i => Row(i)))\n+    }\n+  }\n+\n+  test(\"create hive view for partitioned parquet table\") {\n+    // partitioned parquet table is not hive-compatible, make sure the new flag fix it.\n+    withTable(\"parTable\") {\n+      withView(\"testView\") {\n+        val df = Seq(1 -> \"a\").toDF(\"i\", \"j\")\n+        df.write.format(\"parquet\").partitionBy(\"i\").saveAsTable(\"parTable\")\n+        sql(\"CREATE VIEW testView AS SELECT i, j FROM parTable\")\n+        checkAnswer(sql(\"SELECT * FROM testView\"), Row(1, \"a\"))\n+      }\n+    }\n+  }\n+\n+  test(\"create hive view for joined tables\") {"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": ": ) Could you ignore the test case names and make `HiveSQLViewSuite` as small as possible?\r\n\r\nIf we move them to `SQLViewSuite`, they will also be executed in `HiveSQLViewSuite`.",
    "commit": "aa0cf0a80dfb00b7d367b2a58ecf0ae6ddeee16d",
    "createdAt": "2017-02-09T19:58:43Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\n+import org.apache.spark.sql.execution.SQLViewSuite\n+import org.apache.spark.sql.hive.test.{TestHive, TestHiveSingleton}\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A test suite for Hive view related functionality.\n+ */\n+class HiveSQLViewSuite extends SQLViewSuite with TestHiveSingleton {\n+  protected override val spark: SparkSession = TestHive.sparkSession\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    // Create a simple table with two columns: id and id1\n+    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      spark.sql(s\"DROP TABLE IF EXISTS jt\")\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  import testImplicits._\n+\n+  test(\"create a permanent/temp view using a hive, built-in, and permanent user function\") {\n+    val permanentFuncName = \"myUpper\"\n+    val permanentFuncClass =\n+      classOf[org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper].getCanonicalName\n+    val builtInFuncNameInLowerCase = \"abs\"\n+    val builtInFuncNameInMixedCase = \"aBs\"\n+    val hiveFuncName = \"histogram_numeric\"\n+\n+    withUserDefinedFunction(permanentFuncName -> false) {\n+      sql(s\"CREATE FUNCTION $permanentFuncName AS '$permanentFuncClass'\")\n+      withTable(\"tab1\") {\n+        (1 to 10).map(i => (s\"$i\", i)).toDF(\"str\", \"id\").write.saveAsTable(\"tab1\")\n+        Seq(\"VIEW\", \"TEMPORARY VIEW\").foreach { viewMode =>\n+          withView(\"view1\") {\n+            sql(\n+              s\"\"\"\n+                 |CREATE $viewMode view1\n+                 |AS SELECT\n+                 |$permanentFuncName(str),\n+                 |$builtInFuncNameInLowerCase(id),\n+                 |$builtInFuncNameInMixedCase(id) as aBs,\n+                 |$hiveFuncName(id, 5) over()\n+                 |FROM tab1\n+               \"\"\".stripMargin)\n+            checkAnswer(sql(\"select count(*) FROM view1\"), Row(10))\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"create a permanent/temp view using a temporary function\") {\n+    val tempFunctionName = \"temp\"\n+    val functionClass =\n+      classOf[org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper].getCanonicalName\n+    withUserDefinedFunction(tempFunctionName -> true) {\n+      sql(s\"CREATE TEMPORARY FUNCTION $tempFunctionName AS '$functionClass'\")\n+      withView(\"view1\", \"tempView1\") {\n+        withTable(\"tab1\") {\n+          (1 to 10).map(i => s\"$i\").toDF(\"id\").write.saveAsTable(\"tab1\")\n+\n+          // temporary view\n+          sql(s\"CREATE TEMPORARY VIEW tempView1 AS SELECT $tempFunctionName(id) from tab1\")\n+          checkAnswer(sql(\"select count(*) FROM tempView1\"), Row(10))\n+\n+          // permanent view\n+          val e = intercept[AnalysisException] {\n+            sql(s\"CREATE VIEW view1 AS SELECT $tempFunctionName(id) from tab1\")\n+          }.getMessage\n+          assert(e.contains(\"Not allowed to create a permanent view `view1` by referencing \" +\n+            s\"a temporary function `$tempFunctionName`\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"create hive view for json table\") {\n+    // json table is not hive-compatible, make sure the new flag fix it.\n+    withView(\"testView\") {\n+      sql(\"CREATE VIEW testView AS SELECT id FROM jt\")\n+      checkAnswer(sql(\"SELECT * FROM testView ORDER BY id\"), (1 to 9).map(i => Row(i)))\n+    }\n+  }\n+\n+  test(\"create hive view for partitioned parquet table\") {\n+    // partitioned parquet table is not hive-compatible, make sure the new flag fix it.\n+    withTable(\"parTable\") {\n+      withView(\"testView\") {\n+        val df = Seq(1 -> \"a\").toDF(\"i\", \"j\")\n+        df.write.format(\"parquet\").partitionBy(\"i\").saveAsTable(\"parTable\")\n+        sql(\"CREATE VIEW testView AS SELECT i, j FROM parTable\")\n+        checkAnswer(sql(\"SELECT * FROM testView\"), Row(1, \"a\"))\n+      }\n+    }\n+  }\n+\n+  test(\"create hive view for joined tables\") {"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "Sure - let me do this.",
    "commit": "aa0cf0a80dfb00b7d367b2a58ecf0ae6ddeee16d",
    "createdAt": "2017-02-10T01:30:11Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\n+import org.apache.spark.sql.execution.SQLViewSuite\n+import org.apache.spark.sql.hive.test.{TestHive, TestHiveSingleton}\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A test suite for Hive view related functionality.\n+ */\n+class HiveSQLViewSuite extends SQLViewSuite with TestHiveSingleton {\n+  protected override val spark: SparkSession = TestHive.sparkSession\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    // Create a simple table with two columns: id and id1\n+    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")\n+  }\n+\n+  override def afterAll(): Unit = {\n+    try {\n+      spark.sql(s\"DROP TABLE IF EXISTS jt\")\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  import testImplicits._\n+\n+  test(\"create a permanent/temp view using a hive, built-in, and permanent user function\") {\n+    val permanentFuncName = \"myUpper\"\n+    val permanentFuncClass =\n+      classOf[org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper].getCanonicalName\n+    val builtInFuncNameInLowerCase = \"abs\"\n+    val builtInFuncNameInMixedCase = \"aBs\"\n+    val hiveFuncName = \"histogram_numeric\"\n+\n+    withUserDefinedFunction(permanentFuncName -> false) {\n+      sql(s\"CREATE FUNCTION $permanentFuncName AS '$permanentFuncClass'\")\n+      withTable(\"tab1\") {\n+        (1 to 10).map(i => (s\"$i\", i)).toDF(\"str\", \"id\").write.saveAsTable(\"tab1\")\n+        Seq(\"VIEW\", \"TEMPORARY VIEW\").foreach { viewMode =>\n+          withView(\"view1\") {\n+            sql(\n+              s\"\"\"\n+                 |CREATE $viewMode view1\n+                 |AS SELECT\n+                 |$permanentFuncName(str),\n+                 |$builtInFuncNameInLowerCase(id),\n+                 |$builtInFuncNameInMixedCase(id) as aBs,\n+                 |$hiveFuncName(id, 5) over()\n+                 |FROM tab1\n+               \"\"\".stripMargin)\n+            checkAnswer(sql(\"select count(*) FROM view1\"), Row(10))\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"create a permanent/temp view using a temporary function\") {\n+    val tempFunctionName = \"temp\"\n+    val functionClass =\n+      classOf[org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper].getCanonicalName\n+    withUserDefinedFunction(tempFunctionName -> true) {\n+      sql(s\"CREATE TEMPORARY FUNCTION $tempFunctionName AS '$functionClass'\")\n+      withView(\"view1\", \"tempView1\") {\n+        withTable(\"tab1\") {\n+          (1 to 10).map(i => s\"$i\").toDF(\"id\").write.saveAsTable(\"tab1\")\n+\n+          // temporary view\n+          sql(s\"CREATE TEMPORARY VIEW tempView1 AS SELECT $tempFunctionName(id) from tab1\")\n+          checkAnswer(sql(\"select count(*) FROM tempView1\"), Row(10))\n+\n+          // permanent view\n+          val e = intercept[AnalysisException] {\n+            sql(s\"CREATE VIEW view1 AS SELECT $tempFunctionName(id) from tab1\")\n+          }.getMessage\n+          assert(e.contains(\"Not allowed to create a permanent view `view1` by referencing \" +\n+            s\"a temporary function `$tempFunctionName`\"))\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"create hive view for json table\") {\n+    // json table is not hive-compatible, make sure the new flag fix it.\n+    withView(\"testView\") {\n+      sql(\"CREATE VIEW testView AS SELECT id FROM jt\")\n+      checkAnswer(sql(\"SELECT * FROM testView ORDER BY id\"), (1 to 9).map(i => Row(i)))\n+    }\n+  }\n+\n+  test(\"create hive view for partitioned parquet table\") {\n+    // partitioned parquet table is not hive-compatible, make sure the new flag fix it.\n+    withTable(\"parTable\") {\n+      withView(\"testView\") {\n+        val df = Seq(1 -> \"a\").toDF(\"i\", \"j\")\n+        df.write.format(\"parquet\").partitionBy(\"i\").saveAsTable(\"parTable\")\n+        sql(\"CREATE VIEW testView AS SELECT i, j FROM parTable\")\n+        checkAnswer(sql(\"SELECT * FROM testView\"), Row(1, \"a\"))\n+      }\n+    }\n+  }\n+\n+  test(\"create hive view for joined tables\") {"
  }],
  "prId": 16674
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "can we put this in `SQLViewSuite`? then we don't need to duplicate this logic in `SimpleSQLViewSuite` and `HiveSQLViewSuite`",
    "commit": "aa0cf0a80dfb00b7d367b2a58ecf0ae6ddeee16d",
    "createdAt": "2017-02-10T18:37:04Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\n+import org.apache.spark.sql.execution.SQLViewSuite\n+import org.apache.spark.sql.hive.test.{TestHive, TestHiveSingleton}\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A test suite for Hive view related functionality.\n+ */\n+class HiveSQLViewSuite extends SQLViewSuite with TestHiveSingleton {\n+  protected override val spark: SparkSession = TestHive.sparkSession\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    // Create a simple table with two columns: id and id1\n+    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "The problem is that `SharedSQLContext` also overrides `beforeAll()` and `afterAll()`, I haven't figure out a good way to resolve the issue, do you have any suggestions?",
    "commit": "aa0cf0a80dfb00b7d367b2a58ecf0ae6ddeee16d",
    "createdAt": "2017-02-11T17:18:12Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\n+import org.apache.spark.sql.execution.SQLViewSuite\n+import org.apache.spark.sql.hive.test.{TestHive, TestHiveSingleton}\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A test suite for Hive view related functionality.\n+ */\n+class HiveSQLViewSuite extends SQLViewSuite with TestHiveSingleton {\n+  protected override val spark: SparkSession = TestHive.sparkSession\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    // Create a simple table with two columns: id and id1\n+    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "what do you mean? In `SQLViewSuite.beforeAll`, we can still call `super.beforeAll`",
    "commit": "aa0cf0a80dfb00b7d367b2a58ecf0ae6ddeee16d",
    "createdAt": "2017-02-11T22:15:36Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\n+import org.apache.spark.sql.execution.SQLViewSuite\n+import org.apache.spark.sql.hive.test.{TestHive, TestHiveSingleton}\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A test suite for Hive view related functionality.\n+ */\n+class HiveSQLViewSuite extends SQLViewSuite with TestHiveSingleton {\n+  protected override val spark: SparkSession = TestHive.sparkSession\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    // Create a simple table with two columns: id and id1\n+    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Yes, we can remove both by doing something like \r\n```Scala\r\nabstract class SQLViewSuite extends QueryTest with SQLTestUtils {\r\n  import testImplicits._\r\n\r\n  protected override def beforeAll(): Unit = {\r\n    super.beforeAll()\r\n    // Create a simple table with two columns: id and id1\r\n    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")\r\n  }\r\n\r\n  protected override def afterAll(): Unit = {\r\n    try {\r\n      spark.sql(s\"DROP TABLE IF EXISTS jt\")\r\n    } finally {\r\n      super.afterAll()\r\n    }\r\n  }\r\n```",
    "commit": "aa0cf0a80dfb00b7d367b2a58ecf0ae6ddeee16d",
    "createdAt": "2017-02-12T07:16:51Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\n+import org.apache.spark.sql.execution.SQLViewSuite\n+import org.apache.spark.sql.hive.test.{TestHive, TestHiveSingleton}\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A test suite for Hive view related functionality.\n+ */\n+class HiveSQLViewSuite extends SQLViewSuite with TestHiveSingleton {\n+  protected override val spark: SparkSession = TestHive.sparkSession\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    // Create a simple table with two columns: id and id1\n+    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")"
  }, {
    "author": {
      "login": "jiangxb1987"
    },
    "body": "The call of `SharedSQLContext.afterAll()` is before that of `SQLViewSuite.afterAll()`, so the variable `spark` has been set to null in `SQLViewSuite.afterAll()`.",
    "commit": "aa0cf0a80dfb00b7d367b2a58ecf0ae6ddeee16d",
    "createdAt": "2017-02-12T11:45:32Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import org.apache.spark.sql.{AnalysisException, Row, SaveMode, SparkSession}\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}\n+import org.apache.spark.sql.execution.SQLViewSuite\n+import org.apache.spark.sql.hive.test.{TestHive, TestHiveSingleton}\n+import org.apache.spark.sql.types.StructType\n+\n+/**\n+ * A test suite for Hive view related functionality.\n+ */\n+class HiveSQLViewSuite extends SQLViewSuite with TestHiveSingleton {\n+  protected override val spark: SparkSession = TestHive.sparkSession\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    // Create a simple table with two columns: id and id1\n+    spark.range(1, 10).selectExpr(\"id\", \"id id1\").write.format(\"json\").saveAsTable(\"jt\")"
  }],
  "prId": 16674
}]