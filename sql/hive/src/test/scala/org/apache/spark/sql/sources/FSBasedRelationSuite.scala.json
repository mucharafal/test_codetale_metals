[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "TODO?\n",
    "commit": "5351a1b7da2bcefe310e053c280b6caa476fb148",
    "createdAt": "2015-05-09T18:25:29Z",
    "diffHunk": "@@ -0,0 +1,485 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.hive.test.TestHive\n+import org.apache.spark.sql.parquet.ParquetTest\n+import org.apache.spark.sql.types._\n+\n+// This test suite extends ParquetTest for some convenient utility methods. These methods should be"
  }],
  "prId": 5526
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Perhaps you are already planning to do this, but it seems that if you make `classOf[SimpleTextSource].getCanonicalName` an abstract member you could have a pretty good test suite for any source written with this API.\n",
    "commit": "5351a1b7da2bcefe310e053c280b6caa476fb148",
    "createdAt": "2015-05-09T18:41:31Z",
    "diffHunk": "@@ -0,0 +1,485 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.hive.test.TestHive\n+import org.apache.spark.sql.parquet.ParquetTest\n+import org.apache.spark.sql.types._\n+\n+// This test suite extends ParquetTest for some convenient utility methods. These methods should be\n+// moved to some more general places, maybe QueryTest.\n+class FSBasedRelationSuite extends QueryTest with ParquetTest {\n+  override val sqlContext: SQLContext = TestHive\n+\n+  import sqlContext._\n+  import sqlContext.implicits._\n+\n+  val dataSchema =\n+    StructType(\n+      Seq(\n+        StructField(\"a\", IntegerType, nullable = false),\n+        StructField(\"b\", StringType, nullable = false)))\n+\n+  val testDF = (1 to 3).map(i => (i, s\"val_$i\")).toDF(\"a\", \"b\")\n+\n+  val partitionedTestDF1 = (for {\n+    i <- 1 to 3\n+    p2 <- Seq(\"foo\", \"bar\")\n+  } yield (i, s\"val_$i\", 1, p2)).toDF(\"a\", \"b\", \"p1\", \"p2\")\n+\n+  val partitionedTestDF2 = (for {\n+    i <- 1 to 3\n+    p2 <- Seq(\"foo\", \"bar\")\n+  } yield (i, s\"val_$i\", 2, p2)).toDF(\"a\", \"b\", \"p1\", \"p2\")\n+\n+  val partitionedTestDF = partitionedTestDF1.unionAll(partitionedTestDF2)\n+\n+  def checkQueries(df: DataFrame): Unit = {\n+    // Selects everything\n+    checkAnswer(\n+      df,\n+      for (i <- 1 to 3; p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", p1, p2))\n+\n+    // Simple filtering and partition pruning\n+    checkAnswer(\n+      df.filter('a > 1 && 'p1 === 2),\n+      for (i <- 2 to 3; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", 2, p2))\n+\n+    // Simple projection and filtering\n+    checkAnswer(\n+      df.filter('a > 1).select('b, 'a + 1),\n+      for (i <- 2 to 3; _ <- 1 to 2; _ <- Seq(\"foo\", \"bar\")) yield Row(s\"val_$i\", i + 1))\n+\n+    // Simple projection and partition pruning\n+    checkAnswer(\n+      df.filter('a > 1 && 'p1 < 2).select('b, 'p1),\n+      for (i <- 2 to 3; _ <- Seq(\"foo\", \"bar\")) yield Row(s\"val_$i\", 1))\n+\n+    // Self-join\n+    df.registerTempTable(\"t\")\n+    withTempTable(\"t\") {\n+      checkAnswer(\n+        sql(\n+          \"\"\"SELECT l.a, r.b, l.p1, r.p2\n+            |FROM t l JOIN t r\n+            |ON l.a = r.a AND l.p1 = r.p1 AND l.p2 = r.p2\n+          \"\"\".stripMargin),\n+        for (i <- 1 to 3; p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", p1, p2))\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Overwrite\") {\n+    withTempPath { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        testDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Append\") {\n+    withTempPath { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append)\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)).orderBy(\"a\"),\n+        testDF.unionAll(testDF).orderBy(\"a\").collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - ErrorIfExists\") {\n+    withTempDir { file =>\n+      intercept[RuntimeException] {\n+        testDF.save(\n+          path = file.getCanonicalPath,\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists)\n+      }\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Ignore\") {\n+    withTempDir { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      val path = new Path(file.getCanonicalPath)\n+      val fs = path.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+      assert(fs.listStatus(path).isEmpty)\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - simple queries\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.ErrorIfExists,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkQueries(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)))\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - simple queries - partition columns in data\") {\n+    withTempDir { file =>\n+      val basePath = new Path(file.getCanonicalPath)\n+      val fs = basePath.getFileSystem(SparkHadoopUtil.get.conf)\n+      val qualifiedBasePath = fs.makeQualified(basePath)\n+\n+      for (p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) {\n+        val partitionDir = new Path(qualifiedBasePath, s\"p1=$p1/p2=$p2\")\n+        sparkContext\n+          .parallelize(for (i <- 1 to 3) yield s\"$i,val_$i,$p1\")\n+          .saveAsTextFile(partitionDir.toString)\n+      }\n+\n+      val dataSchemaWithPartition =\n+        StructType(dataSchema.fields :+ StructField(\"p1\", IntegerType, nullable = true))\n+\n+      checkQueries(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchemaWithPartition.json)))\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Overwrite\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Append\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.unionAll(partitionedTestDF).collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Append - new partition values\") {\n+    withTempPath { file =>\n+      partitionedTestDF1.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF2.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - ErrorIfExists\") {\n+    withTempDir { file =>\n+      intercept[RuntimeException] {\n+        partitionedTestDF.save(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists,\n+          options = Map(\"path\" -> file.getCanonicalPath),\n+          partitionColumns = Seq(\"p1\", \"p2\"))\n+      }\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Ignore\") {\n+    withTempDir { file =>\n+      partitionedTestDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      val path = new Path(file.getCanonicalPath)\n+      val fs = path.getFileSystem(SparkHadoopUtil.get.conf)\n+      assert(fs.listStatus(path).isEmpty)\n+    }\n+  }\n+\n+  def withTable(tableName: String)(f: => Unit): Unit = {\n+    try f finally sql(s\"DROP TABLE $tableName\")\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Overwrite\") {\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      Map(\"dataSchema\" -> dataSchema.json))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), testDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Append\") {\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite)\n+\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append)\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), testDF.unionAll(testDF).orderBy(\"a\").collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - ErrorIfExists\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      intercept[AnalysisException] {\n+        testDF.saveAsTable(\n+          tableName = \"t\",\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists)\n+      }\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Ignore\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      testDF.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      assert(table(\"t\").collect().isEmpty)\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - simple queries\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      Map(\"dataSchema\" -> dataSchema.json))\n+\n+    withTable(\"t\") {\n+      checkQueries(table(\"t\"))\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Overwrite\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.unionAll(partitionedTestDF).collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append - new partition values\") {\n+    partitionedTestDF1.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF2.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append - mismatched partition columns\") {\n+    partitionedTestDF1.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    // Using only a subset of all partition columns\n+    intercept[Throwable] {\n+      partitionedTestDF2.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p1\"))\n+    }\n+\n+    // Using different order of partition columns\n+    intercept[Throwable] {\n+      partitionedTestDF2.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p2\", \"p1\"))\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - ErrorIfExists\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      intercept[AnalysisException] {\n+        partitionedTestDF.saveAsTable(\n+          tableName = \"t\",\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists,\n+          options = Map(\"dataSchema\" -> dataSchema.json),\n+          partitionColumns = Seq(\"p1\", \"p2\"))\n+      }\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Ignore\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      partitionedTestDF.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,",
    "line": 478
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Good point, haven't thought about generalizing this test suite yet. Let's leave it to another PR.\n",
    "commit": "5351a1b7da2bcefe310e053c280b6caa476fb148",
    "createdAt": "2015-05-10T10:02:45Z",
    "diffHunk": "@@ -0,0 +1,485 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.hive.test.TestHive\n+import org.apache.spark.sql.parquet.ParquetTest\n+import org.apache.spark.sql.types._\n+\n+// This test suite extends ParquetTest for some convenient utility methods. These methods should be\n+// moved to some more general places, maybe QueryTest.\n+class FSBasedRelationSuite extends QueryTest with ParquetTest {\n+  override val sqlContext: SQLContext = TestHive\n+\n+  import sqlContext._\n+  import sqlContext.implicits._\n+\n+  val dataSchema =\n+    StructType(\n+      Seq(\n+        StructField(\"a\", IntegerType, nullable = false),\n+        StructField(\"b\", StringType, nullable = false)))\n+\n+  val testDF = (1 to 3).map(i => (i, s\"val_$i\")).toDF(\"a\", \"b\")\n+\n+  val partitionedTestDF1 = (for {\n+    i <- 1 to 3\n+    p2 <- Seq(\"foo\", \"bar\")\n+  } yield (i, s\"val_$i\", 1, p2)).toDF(\"a\", \"b\", \"p1\", \"p2\")\n+\n+  val partitionedTestDF2 = (for {\n+    i <- 1 to 3\n+    p2 <- Seq(\"foo\", \"bar\")\n+  } yield (i, s\"val_$i\", 2, p2)).toDF(\"a\", \"b\", \"p1\", \"p2\")\n+\n+  val partitionedTestDF = partitionedTestDF1.unionAll(partitionedTestDF2)\n+\n+  def checkQueries(df: DataFrame): Unit = {\n+    // Selects everything\n+    checkAnswer(\n+      df,\n+      for (i <- 1 to 3; p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", p1, p2))\n+\n+    // Simple filtering and partition pruning\n+    checkAnswer(\n+      df.filter('a > 1 && 'p1 === 2),\n+      for (i <- 2 to 3; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", 2, p2))\n+\n+    // Simple projection and filtering\n+    checkAnswer(\n+      df.filter('a > 1).select('b, 'a + 1),\n+      for (i <- 2 to 3; _ <- 1 to 2; _ <- Seq(\"foo\", \"bar\")) yield Row(s\"val_$i\", i + 1))\n+\n+    // Simple projection and partition pruning\n+    checkAnswer(\n+      df.filter('a > 1 && 'p1 < 2).select('b, 'p1),\n+      for (i <- 2 to 3; _ <- Seq(\"foo\", \"bar\")) yield Row(s\"val_$i\", 1))\n+\n+    // Self-join\n+    df.registerTempTable(\"t\")\n+    withTempTable(\"t\") {\n+      checkAnswer(\n+        sql(\n+          \"\"\"SELECT l.a, r.b, l.p1, r.p2\n+            |FROM t l JOIN t r\n+            |ON l.a = r.a AND l.p1 = r.p1 AND l.p2 = r.p2\n+          \"\"\".stripMargin),\n+        for (i <- 1 to 3; p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", p1, p2))\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Overwrite\") {\n+    withTempPath { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        testDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Append\") {\n+    withTempPath { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append)\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)).orderBy(\"a\"),\n+        testDF.unionAll(testDF).orderBy(\"a\").collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - ErrorIfExists\") {\n+    withTempDir { file =>\n+      intercept[RuntimeException] {\n+        testDF.save(\n+          path = file.getCanonicalPath,\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists)\n+      }\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Ignore\") {\n+    withTempDir { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      val path = new Path(file.getCanonicalPath)\n+      val fs = path.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+      assert(fs.listStatus(path).isEmpty)\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - simple queries\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.ErrorIfExists,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkQueries(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)))\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - simple queries - partition columns in data\") {\n+    withTempDir { file =>\n+      val basePath = new Path(file.getCanonicalPath)\n+      val fs = basePath.getFileSystem(SparkHadoopUtil.get.conf)\n+      val qualifiedBasePath = fs.makeQualified(basePath)\n+\n+      for (p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) {\n+        val partitionDir = new Path(qualifiedBasePath, s\"p1=$p1/p2=$p2\")\n+        sparkContext\n+          .parallelize(for (i <- 1 to 3) yield s\"$i,val_$i,$p1\")\n+          .saveAsTextFile(partitionDir.toString)\n+      }\n+\n+      val dataSchemaWithPartition =\n+        StructType(dataSchema.fields :+ StructField(\"p1\", IntegerType, nullable = true))\n+\n+      checkQueries(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchemaWithPartition.json)))\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Overwrite\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Append\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.unionAll(partitionedTestDF).collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Append - new partition values\") {\n+    withTempPath { file =>\n+      partitionedTestDF1.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF2.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - ErrorIfExists\") {\n+    withTempDir { file =>\n+      intercept[RuntimeException] {\n+        partitionedTestDF.save(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists,\n+          options = Map(\"path\" -> file.getCanonicalPath),\n+          partitionColumns = Seq(\"p1\", \"p2\"))\n+      }\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Ignore\") {\n+    withTempDir { file =>\n+      partitionedTestDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      val path = new Path(file.getCanonicalPath)\n+      val fs = path.getFileSystem(SparkHadoopUtil.get.conf)\n+      assert(fs.listStatus(path).isEmpty)\n+    }\n+  }\n+\n+  def withTable(tableName: String)(f: => Unit): Unit = {\n+    try f finally sql(s\"DROP TABLE $tableName\")\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Overwrite\") {\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      Map(\"dataSchema\" -> dataSchema.json))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), testDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Append\") {\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite)\n+\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append)\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), testDF.unionAll(testDF).orderBy(\"a\").collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - ErrorIfExists\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      intercept[AnalysisException] {\n+        testDF.saveAsTable(\n+          tableName = \"t\",\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists)\n+      }\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Ignore\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      testDF.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      assert(table(\"t\").collect().isEmpty)\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - simple queries\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      Map(\"dataSchema\" -> dataSchema.json))\n+\n+    withTable(\"t\") {\n+      checkQueries(table(\"t\"))\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Overwrite\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.unionAll(partitionedTestDF).collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append - new partition values\") {\n+    partitionedTestDF1.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF2.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append - mismatched partition columns\") {\n+    partitionedTestDF1.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    // Using only a subset of all partition columns\n+    intercept[Throwable] {\n+      partitionedTestDF2.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p1\"))\n+    }\n+\n+    // Using different order of partition columns\n+    intercept[Throwable] {\n+      partitionedTestDF2.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p2\", \"p1\"))\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - ErrorIfExists\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      intercept[AnalysisException] {\n+        partitionedTestDF.saveAsTable(\n+          tableName = \"t\",\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists,\n+          options = Map(\"dataSchema\" -> dataSchema.json),\n+          partitionColumns = Seq(\"p1\", \"p2\"))\n+      }\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Ignore\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      partitionedTestDF.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,",
    "line": 478
  }],
  "prId": 5526
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "remove this println\n",
    "commit": "5351a1b7da2bcefe310e053c280b6caa476fb148",
    "createdAt": "2015-05-13T05:19:05Z",
    "diffHunk": "@@ -0,0 +1,525 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.hive.test.TestHive\n+import org.apache.spark.sql.parquet.ParquetTest\n+import org.apache.spark.sql.types._\n+\n+// TODO Don't extend ParquetTest\n+// This test suite extends ParquetTest for some convenient utility methods. These methods should be\n+// moved to some more general places, maybe QueryTest.\n+class FSBasedRelationSuite extends QueryTest with ParquetTest {\n+  override val sqlContext: SQLContext = TestHive\n+\n+  import sqlContext._\n+  import sqlContext.implicits._\n+\n+  val dataSchema =\n+    StructType(\n+      Seq(\n+        StructField(\"a\", IntegerType, nullable = false),\n+        StructField(\"b\", StringType, nullable = false)))\n+\n+  val testDF = (1 to 3).map(i => (i, s\"val_$i\")).toDF(\"a\", \"b\")\n+\n+  val partitionedTestDF1 = (for {\n+    i <- 1 to 3\n+    p2 <- Seq(\"foo\", \"bar\")\n+  } yield (i, s\"val_$i\", 1, p2)).toDF(\"a\", \"b\", \"p1\", \"p2\")\n+\n+  val partitionedTestDF2 = (for {\n+    i <- 1 to 3\n+    p2 <- Seq(\"foo\", \"bar\")\n+  } yield (i, s\"val_$i\", 2, p2)).toDF(\"a\", \"b\", \"p1\", \"p2\")\n+\n+  val partitionedTestDF = partitionedTestDF1.unionAll(partitionedTestDF2)\n+\n+  def checkQueries(df: DataFrame): Unit = {\n+    // Selects everything\n+    checkAnswer(\n+      df,\n+      for (i <- 1 to 3; p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", p1, p2))\n+\n+    // Simple filtering and partition pruning\n+    checkAnswer(\n+      df.filter('a > 1 && 'p1 === 2),\n+      for (i <- 2 to 3; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", 2, p2))\n+\n+    // Simple projection and filtering\n+    checkAnswer(\n+      df.filter('a > 1).select('b, 'a + 1),\n+      for (i <- 2 to 3; _ <- 1 to 2; _ <- Seq(\"foo\", \"bar\")) yield Row(s\"val_$i\", i + 1))\n+\n+    // Simple projection and partition pruning\n+    checkAnswer(\n+      df.filter('a > 1 && 'p1 < 2).select('b, 'p1),\n+      for (i <- 2 to 3; _ <- Seq(\"foo\", \"bar\")) yield Row(s\"val_$i\", 1))\n+\n+    // Self-join\n+    df.registerTempTable(\"t\")\n+    withTempTable(\"t\") {\n+      checkAnswer(\n+        sql(\n+          \"\"\"SELECT l.a, r.b, l.p1, r.p2\n+            |FROM t l JOIN t r\n+            |ON l.a = r.a AND l.p1 = r.p1 AND l.p2 = r.p2\n+          \"\"\".stripMargin),\n+        for (i <- 1 to 3; p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", p1, p2))\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Overwrite\") {\n+    withTempPath { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        testDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Append\") {\n+    withTempPath { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append)\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)).orderBy(\"a\"),\n+        testDF.unionAll(testDF).orderBy(\"a\").collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - ErrorIfExists\") {\n+    withTempDir { file =>\n+      intercept[RuntimeException] {\n+        testDF.save(\n+          path = file.getCanonicalPath,\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists)\n+      }\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Ignore\") {\n+    withTempDir { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      val path = new Path(file.getCanonicalPath)\n+      val fs = path.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+      assert(fs.listStatus(path).isEmpty)\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - simple queries\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.ErrorIfExists,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkQueries(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)))\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - simple queries - partition columns in data\") {\n+    withTempDir { file =>\n+      val basePath = new Path(file.getCanonicalPath)\n+      val fs = basePath.getFileSystem(SparkHadoopUtil.get.conf)\n+      val qualifiedBasePath = fs.makeQualified(basePath)\n+\n+      for (p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) {\n+        val partitionDir = new Path(qualifiedBasePath, s\"p1=$p1/p2=$p2\")\n+        sparkContext\n+          .parallelize(for (i <- 1 to 3) yield s\"$i,val_$i,$p1\")\n+          .saveAsTextFile(partitionDir.toString)\n+      }\n+\n+      val dataSchemaWithPartition =\n+        StructType(dataSchema.fields :+ StructField(\"p1\", IntegerType, nullable = true))\n+\n+      checkQueries(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchemaWithPartition.json)))\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Overwrite\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Append\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.unionAll(partitionedTestDF).collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Append - new partition values\") {\n+    withTempPath { file =>\n+      partitionedTestDF1.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF2.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - ErrorIfExists\") {\n+    withTempDir { file =>\n+      intercept[RuntimeException] {\n+        partitionedTestDF.save(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists,\n+          options = Map(\"path\" -> file.getCanonicalPath),\n+          partitionColumns = Seq(\"p1\", \"p2\"))\n+      }\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Ignore\") {\n+    withTempDir { file =>\n+      partitionedTestDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      val path = new Path(file.getCanonicalPath)\n+      val fs = path.getFileSystem(SparkHadoopUtil.get.conf)\n+      assert(fs.listStatus(path).isEmpty)\n+    }\n+  }\n+\n+  def withTable(tableName: String)(f: => Unit): Unit = {\n+    try f finally sql(s\"DROP TABLE $tableName\")\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Overwrite\") {\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      Map(\"dataSchema\" -> dataSchema.json))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), testDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Append\") {\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite)\n+\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append)\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), testDF.unionAll(testDF).orderBy(\"a\").collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - ErrorIfExists\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      intercept[AnalysisException] {\n+        testDF.saveAsTable(\n+          tableName = \"t\",\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists)\n+      }\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Ignore\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      testDF.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      assert(table(\"t\").collect().isEmpty)\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - simple queries\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      Map(\"dataSchema\" -> dataSchema.json))\n+\n+    withTable(\"t\") {\n+      checkQueries(table(\"t\"))\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Overwrite\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.unionAll(partitionedTestDF).collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append - new partition values\") {\n+    partitionedTestDF1.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF2.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append - mismatched partition columns\") {\n+    partitionedTestDF1.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    // Using only a subset of all partition columns\n+    intercept[Throwable] {\n+      partitionedTestDF2.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p1\"))\n+    }\n+\n+    // Using different order of partition columns\n+    intercept[Throwable] {\n+      partitionedTestDF2.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p2\", \"p1\"))\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - ErrorIfExists\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      intercept[AnalysisException] {\n+        partitionedTestDF.saveAsTable(\n+          tableName = \"t\",\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists,\n+          options = Map(\"dataSchema\" -> dataSchema.json),\n+          partitionColumns = Seq(\"p1\", \"p2\"))\n+      }\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Ignore\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      partitionedTestDF.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      assert(table(\"t\").collect().isEmpty)\n+    }\n+  }\n+\n+  test(\"Hadoop style globbing\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      val df = load(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        options = Map(\n+          \"path\" -> s\"${file.getCanonicalPath}/p1=*/p2=???\",\n+          \"dataSchema\" -> dataSchema.json))\n+\n+      val expectedPaths = Set(\n+        s\"${file.getCanonicalFile}/p1=1/p2=foo\",\n+        s\"${file.getCanonicalFile}/p1=2/p2=foo\",\n+        s\"${file.getCanonicalFile}/p1=1/p2=bar\",\n+        s\"${file.getCanonicalFile}/p1=2/p2=bar\"\n+      ).map { p =>\n+        val path = new Path(p)\n+        val fs = path.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+        path.makeQualified(fs.getUri, fs.getWorkingDirectory).toString\n+      }\n+\n+      println(df.queryExecution)",
    "line": 512
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Oops, removed in #6123\n",
    "commit": "5351a1b7da2bcefe310e053c280b6caa476fb148",
    "createdAt": "2015-05-13T15:41:22Z",
    "diffHunk": "@@ -0,0 +1,525 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sources\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.hive.test.TestHive\n+import org.apache.spark.sql.parquet.ParquetTest\n+import org.apache.spark.sql.types._\n+\n+// TODO Don't extend ParquetTest\n+// This test suite extends ParquetTest for some convenient utility methods. These methods should be\n+// moved to some more general places, maybe QueryTest.\n+class FSBasedRelationSuite extends QueryTest with ParquetTest {\n+  override val sqlContext: SQLContext = TestHive\n+\n+  import sqlContext._\n+  import sqlContext.implicits._\n+\n+  val dataSchema =\n+    StructType(\n+      Seq(\n+        StructField(\"a\", IntegerType, nullable = false),\n+        StructField(\"b\", StringType, nullable = false)))\n+\n+  val testDF = (1 to 3).map(i => (i, s\"val_$i\")).toDF(\"a\", \"b\")\n+\n+  val partitionedTestDF1 = (for {\n+    i <- 1 to 3\n+    p2 <- Seq(\"foo\", \"bar\")\n+  } yield (i, s\"val_$i\", 1, p2)).toDF(\"a\", \"b\", \"p1\", \"p2\")\n+\n+  val partitionedTestDF2 = (for {\n+    i <- 1 to 3\n+    p2 <- Seq(\"foo\", \"bar\")\n+  } yield (i, s\"val_$i\", 2, p2)).toDF(\"a\", \"b\", \"p1\", \"p2\")\n+\n+  val partitionedTestDF = partitionedTestDF1.unionAll(partitionedTestDF2)\n+\n+  def checkQueries(df: DataFrame): Unit = {\n+    // Selects everything\n+    checkAnswer(\n+      df,\n+      for (i <- 1 to 3; p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", p1, p2))\n+\n+    // Simple filtering and partition pruning\n+    checkAnswer(\n+      df.filter('a > 1 && 'p1 === 2),\n+      for (i <- 2 to 3; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", 2, p2))\n+\n+    // Simple projection and filtering\n+    checkAnswer(\n+      df.filter('a > 1).select('b, 'a + 1),\n+      for (i <- 2 to 3; _ <- 1 to 2; _ <- Seq(\"foo\", \"bar\")) yield Row(s\"val_$i\", i + 1))\n+\n+    // Simple projection and partition pruning\n+    checkAnswer(\n+      df.filter('a > 1 && 'p1 < 2).select('b, 'p1),\n+      for (i <- 2 to 3; _ <- Seq(\"foo\", \"bar\")) yield Row(s\"val_$i\", 1))\n+\n+    // Self-join\n+    df.registerTempTable(\"t\")\n+    withTempTable(\"t\") {\n+      checkAnswer(\n+        sql(\n+          \"\"\"SELECT l.a, r.b, l.p1, r.p2\n+            |FROM t l JOIN t r\n+            |ON l.a = r.a AND l.p1 = r.p1 AND l.p2 = r.p2\n+          \"\"\".stripMargin),\n+        for (i <- 1 to 3; p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) yield Row(i, s\"val_$i\", p1, p2))\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Overwrite\") {\n+    withTempPath { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        testDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Append\") {\n+    withTempPath { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite)\n+\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append)\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)).orderBy(\"a\"),\n+        testDF.unionAll(testDF).orderBy(\"a\").collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - ErrorIfExists\") {\n+    withTempDir { file =>\n+      intercept[RuntimeException] {\n+        testDF.save(\n+          path = file.getCanonicalPath,\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists)\n+      }\n+    }\n+  }\n+\n+  test(\"save()/load() - non-partitioned table - Ignore\") {\n+    withTempDir { file =>\n+      testDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      val path = new Path(file.getCanonicalPath)\n+      val fs = path.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+      assert(fs.listStatus(path).isEmpty)\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - simple queries\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.ErrorIfExists,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkQueries(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)))\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - simple queries - partition columns in data\") {\n+    withTempDir { file =>\n+      val basePath = new Path(file.getCanonicalPath)\n+      val fs = basePath.getFileSystem(SparkHadoopUtil.get.conf)\n+      val qualifiedBasePath = fs.makeQualified(basePath)\n+\n+      for (p1 <- 1 to 2; p2 <- Seq(\"foo\", \"bar\")) {\n+        val partitionDir = new Path(qualifiedBasePath, s\"p1=$p1/p2=$p2\")\n+        sparkContext\n+          .parallelize(for (i <- 1 to 3) yield s\"$i,val_$i,$p1\")\n+          .saveAsTextFile(partitionDir.toString)\n+      }\n+\n+      val dataSchemaWithPartition =\n+        StructType(dataSchema.fields :+ StructField(\"p1\", IntegerType, nullable = true))\n+\n+      checkQueries(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchemaWithPartition.json)))\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Overwrite\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Append\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.unionAll(partitionedTestDF).collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Append - new partition values\") {\n+    withTempPath { file =>\n+      partitionedTestDF1.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      partitionedTestDF2.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      checkAnswer(\n+        load(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          options = Map(\n+            \"path\" -> file.getCanonicalPath,\n+            \"dataSchema\" -> dataSchema.json)),\n+        partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - ErrorIfExists\") {\n+    withTempDir { file =>\n+      intercept[RuntimeException] {\n+        partitionedTestDF.save(\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists,\n+          options = Map(\"path\" -> file.getCanonicalPath),\n+          partitionColumns = Seq(\"p1\", \"p2\"))\n+      }\n+    }\n+  }\n+\n+  test(\"save()/load() - partitioned table - Ignore\") {\n+    withTempDir { file =>\n+      partitionedTestDF.save(\n+        path = file.getCanonicalPath,\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      val path = new Path(file.getCanonicalPath)\n+      val fs = path.getFileSystem(SparkHadoopUtil.get.conf)\n+      assert(fs.listStatus(path).isEmpty)\n+    }\n+  }\n+\n+  def withTable(tableName: String)(f: => Unit): Unit = {\n+    try f finally sql(s\"DROP TABLE $tableName\")\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Overwrite\") {\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      Map(\"dataSchema\" -> dataSchema.json))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), testDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Append\") {\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite)\n+\n+    testDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append)\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), testDF.unionAll(testDF).orderBy(\"a\").collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - ErrorIfExists\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      intercept[AnalysisException] {\n+        testDF.saveAsTable(\n+          tableName = \"t\",\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists)\n+      }\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - non-partitioned table - Ignore\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      testDF.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore)\n+\n+      assert(table(\"t\").collect().isEmpty)\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - simple queries\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      Map(\"dataSchema\" -> dataSchema.json))\n+\n+    withTable(\"t\") {\n+      checkQueries(table(\"t\"))\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Overwrite\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append\") {\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.unionAll(partitionedTestDF).collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append - new partition values\") {\n+    partitionedTestDF1.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    partitionedTestDF2.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Append,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    withTable(\"t\") {\n+      checkAnswer(table(\"t\"), partitionedTestDF.collect())\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Append - mismatched partition columns\") {\n+    partitionedTestDF1.saveAsTable(\n+      tableName = \"t\",\n+      source = classOf[SimpleTextSource].getCanonicalName,\n+      mode = SaveMode.Overwrite,\n+      options = Map(\"dataSchema\" -> dataSchema.json),\n+      partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+    // Using only a subset of all partition columns\n+    intercept[Throwable] {\n+      partitionedTestDF2.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p1\"))\n+    }\n+\n+    // Using different order of partition columns\n+    intercept[Throwable] {\n+      partitionedTestDF2.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Append,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p2\", \"p1\"))\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - ErrorIfExists\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      intercept[AnalysisException] {\n+        partitionedTestDF.saveAsTable(\n+          tableName = \"t\",\n+          source = classOf[SimpleTextSource].getCanonicalName,\n+          mode = SaveMode.ErrorIfExists,\n+          options = Map(\"dataSchema\" -> dataSchema.json),\n+          partitionColumns = Seq(\"p1\", \"p2\"))\n+      }\n+    }\n+  }\n+\n+  test(\"saveAsTable()/load() - partitioned table - Ignore\") {\n+    Seq.empty[(Int, String)].toDF().registerTempTable(\"t\")\n+\n+    withTempTable(\"t\") {\n+      partitionedTestDF.saveAsTable(\n+        tableName = \"t\",\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Ignore,\n+        options = Map(\"dataSchema\" -> dataSchema.json),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      assert(table(\"t\").collect().isEmpty)\n+    }\n+  }\n+\n+  test(\"Hadoop style globbing\") {\n+    withTempPath { file =>\n+      partitionedTestDF.save(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        mode = SaveMode.Overwrite,\n+        options = Map(\"path\" -> file.getCanonicalPath),\n+        partitionColumns = Seq(\"p1\", \"p2\"))\n+\n+      val df = load(\n+        source = classOf[SimpleTextSource].getCanonicalName,\n+        options = Map(\n+          \"path\" -> s\"${file.getCanonicalPath}/p1=*/p2=???\",\n+          \"dataSchema\" -> dataSchema.json))\n+\n+      val expectedPaths = Set(\n+        s\"${file.getCanonicalFile}/p1=1/p2=foo\",\n+        s\"${file.getCanonicalFile}/p1=2/p2=foo\",\n+        s\"${file.getCanonicalFile}/p1=1/p2=bar\",\n+        s\"${file.getCanonicalFile}/p1=2/p2=bar\"\n+      ).map { p =>\n+        val path = new Path(p)\n+        val fs = path.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)\n+        path.makeQualified(fs.getUri, fs.getWorkingDirectory).toString\n+      }\n+\n+      println(df.queryExecution)",
    "line": 512
  }],
  "prId": 5526
}]