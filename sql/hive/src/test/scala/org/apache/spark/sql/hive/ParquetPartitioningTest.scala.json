[{
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "extra line.",
    "commit": "813d19c63477b82a76bdd0d1da73cf3cb1d38564",
    "createdAt": "2018-09-19T14:49:55Z",
    "diffHunk": "@@ -0,0 +1,253 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.File\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.test.SQLTestUtils\n+import org.apache.spark.util.Utils\n+\n+// The data where the partitioning key exists only in the directory structure.\n+case class ParquetData(intField: Int, stringField: String)\n+// The data that also includes the partitioning key\n+case class ParquetDataWithKey(p: Int, intField: Int, stringField: String)\n+\n+case class StructContainer(intStructField: Int, stringStructField: String)\n+\n+case class ParquetDataWithComplexTypes(\n+    intField: Int,\n+    stringField: String,\n+    structField: StructContainer,\n+    arrayField: Seq[Int])\n+\n+case class ParquetDataWithKeyAndComplexTypes(\n+    p: Int,\n+    intField: Int,\n+    stringField: String,\n+    structField: StructContainer,\n+    arrayField: Seq[Int])\n+\n+/**\n+ * A collection of tests for parquet data with various forms of partitioning.\n+ */\n+abstract class ParquetPartitioningTest extends QueryTest with SQLTestUtils with TestHiveSingleton {\n+  import testImplicits._\n+\n+  var partitionedTableDir: File = null\n+  var normalTableDir: File = null\n+  var partitionedTableDirWithKey: File = null\n+  var partitionedTableDirWithComplexTypes: File = null\n+  var partitionedTableDirWithKeyAndComplexTypes: File = null\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    partitionedTableDir = Utils.createTempDir()\n+    normalTableDir = Utils.createTempDir()\n+\n+    (1 to 10).foreach { p =>\n+      val partDir = new File(partitionedTableDir, s\"p=$p\")\n+      sparkContext.makeRDD(1 to 10)\n+        .map(i => ParquetData(i, s\"part-$p\"))\n+        .toDF()\n+        .write.parquet(partDir.getCanonicalPath)\n+    }\n+\n+    sparkContext\n+      .makeRDD(1 to 10)\n+      .map(i => ParquetData(i, s\"part-1\"))\n+      .toDF()\n+      .write.parquet(new File(normalTableDir, \"normal\").getCanonicalPath)\n+\n+    partitionedTableDirWithKey = Utils.createTempDir()\n+\n+    (1 to 10).foreach { p =>\n+      val partDir = new File(partitionedTableDirWithKey, s\"p=$p\")\n+      sparkContext.makeRDD(1 to 10)\n+        .map(i => ParquetDataWithKey(p, i, s\"part-$p\"))\n+        .toDF()\n+        .write.parquet(partDir.getCanonicalPath)\n+    }\n+\n+    partitionedTableDirWithKeyAndComplexTypes = Utils.createTempDir()\n+\n+    (1 to 10).foreach { p =>\n+      val partDir = new File(partitionedTableDirWithKeyAndComplexTypes, s\"p=$p\")\n+      sparkContext.makeRDD(1 to 10).map { i =>\n+        ParquetDataWithKeyAndComplexTypes(\n+          p, i, s\"part-$p\", StructContainer(i, f\"${i}_string\"), 1 to i)\n+      }.toDF().write.parquet(partDir.getCanonicalPath)\n+    }\n+\n+    partitionedTableDirWithComplexTypes = Utils.createTempDir()\n+\n+    (1 to 10).foreach { p =>\n+      val partDir = new File(partitionedTableDirWithComplexTypes, s\"p=$p\")\n+      sparkContext.makeRDD(1 to 10).map { i =>\n+        ParquetDataWithComplexTypes(i, s\"part-$p\", StructContainer(i, f\"${i}_string\"), 1 to i)\n+      }.toDF().write.parquet(partDir.getCanonicalPath)\n+    }\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    try {\n+      partitionedTableDir.delete()\n+      normalTableDir.delete()\n+      partitionedTableDirWithKey.delete()\n+      partitionedTableDirWithComplexTypes.delete()\n+      partitionedTableDirWithKeyAndComplexTypes.delete()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  /**\n+   * Drop named tables if they exist\n+ *\n+   * @param tableNames tables to drop\n+   */\n+  def dropTables(tableNames: String*): Unit = {\n+    tableNames.foreach { name =>\n+      sql(s\"DROP TABLE IF EXISTS $name\")\n+    }\n+  }\n+\n+  Seq(\n+    \"partitioned_parquet\",\n+    \"partitioned_parquet_with_key\",\n+    \"partitioned_parquet_with_complextypes\",\n+    \"partitioned_parquet_with_key_and_complextypes\").foreach { table =>\n+\n+    test(s\"ordering of the partitioning columns $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT p, stringField FROM $table WHERE p = 1\"),\n+        Seq.fill(10)(Row(1, \"part-1\"))\n+      )\n+\n+      checkAnswer(\n+        sql(s\"SELECT stringField, p FROM $table WHERE p = 1\"),\n+        Seq.fill(10)(Row(\"part-1\", 1))\n+      )\n+    }\n+\n+    test(s\"project the partitioning column $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT p, count(*) FROM $table group by p\"),\n+        Row(1, 10) ::\n+          Row(2, 10) ::\n+          Row(3, 10) ::\n+          Row(4, 10) ::\n+          Row(5, 10) ::\n+          Row(6, 10) ::\n+          Row(7, 10) ::\n+          Row(8, 10) ::\n+          Row(9, 10) ::\n+          Row(10, 10) :: Nil\n+      )\n+    }\n+\n+    test(s\"project partitioning and non-partitioning columns $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT stringField, p, count(intField) FROM $table GROUP BY p, stringField\"),\n+        Row(\"part-1\", 1, 10) ::\n+          Row(\"part-2\", 2, 10) ::\n+          Row(\"part-3\", 3, 10) ::\n+          Row(\"part-4\", 4, 10) ::\n+          Row(\"part-5\", 5, 10) ::\n+          Row(\"part-6\", 6, 10) ::\n+          Row(\"part-7\", 7, 10) ::\n+          Row(\"part-8\", 8, 10) ::\n+          Row(\"part-9\", 9, 10) ::\n+          Row(\"part-10\", 10, 10) :: Nil\n+      )\n+    }\n+\n+    test(s\"simple count $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT COUNT(*) FROM $table\"),\n+        Row(100))\n+    }\n+\n+    test(s\"pruned count $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT COUNT(*) FROM $table WHERE p = 1\"),\n+        Row(10))\n+    }\n+\n+    test(s\"non-existent partition $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT COUNT(*) FROM $table WHERE p = 1000\"),\n+        Row(0))\n+    }\n+\n+    test(s\"multi-partition pruned count $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT COUNT(*) FROM $table WHERE p IN (1,2,3)\"),\n+        Row(30))\n+    }\n+\n+    test(s\"non-partition predicates $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT COUNT(*) FROM $table WHERE intField IN (1,2,3)\"),\n+        Row(30))\n+    }\n+\n+    test(s\"sum $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT SUM(intField) FROM $table WHERE intField IN (1,2,3) AND p = 1\"),\n+        Row(1 + 2 + 3))\n+    }\n+\n+    test(s\"hive udfs $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT concat(stringField, stringField) FROM $table\"),\n+        sql(s\"SELECT stringField FROM $table\").rdd.map {\n+          case Row(s: String) => Row(s + s)\n+        }.collect().toSeq)\n+    }\n+  }\n+\n+  Seq(\n+    \"partitioned_parquet_with_key_and_complextypes\",\n+    \"partitioned_parquet_with_complextypes\").foreach { table =>\n+\n+    test(s\"SPARK-5775 read struct from $table\") {\n+      checkAnswer(\n+        sql(\n+          s\"\"\"\n+             |SELECT p, structField.intStructField, structField.stringStructField\n+             |FROM $table WHERE p = 1\n+           \"\"\".stripMargin),\n+        (1 to 10).map(i => Row(1, i, f\"${i}_string\")))\n+    }\n+\n+    test(s\"SPARK-5775 read array from $table\") {\n+      checkAnswer(\n+        sql(s\"SELECT arrayField, p FROM $table WHERE p = 1\"),\n+        (1 to 10).map(i => Row((1 to i).toArray, 1)))\n+    }\n+  }\n+",
    "line": 246
  }],
  "prId": 22467
}, {
  "comments": [{
    "author": {
      "login": "xuanyuanking"
    },
    "body": "nit for the indent.",
    "commit": "813d19c63477b82a76bdd0d1da73cf3cb1d38564",
    "createdAt": "2018-09-19T14:50:04Z",
    "diffHunk": "@@ -0,0 +1,253 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.File\n+\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.test.SQLTestUtils\n+import org.apache.spark.util.Utils\n+\n+// The data where the partitioning key exists only in the directory structure.\n+case class ParquetData(intField: Int, stringField: String)\n+// The data that also includes the partitioning key\n+case class ParquetDataWithKey(p: Int, intField: Int, stringField: String)\n+\n+case class StructContainer(intStructField: Int, stringStructField: String)\n+\n+case class ParquetDataWithComplexTypes(\n+    intField: Int,\n+    stringField: String,\n+    structField: StructContainer,\n+    arrayField: Seq[Int])\n+\n+case class ParquetDataWithKeyAndComplexTypes(\n+    p: Int,\n+    intField: Int,\n+    stringField: String,\n+    structField: StructContainer,\n+    arrayField: Seq[Int])\n+\n+/**\n+ * A collection of tests for parquet data with various forms of partitioning.\n+ */\n+abstract class ParquetPartitioningTest extends QueryTest with SQLTestUtils with TestHiveSingleton {\n+  import testImplicits._\n+\n+  var partitionedTableDir: File = null\n+  var normalTableDir: File = null\n+  var partitionedTableDirWithKey: File = null\n+  var partitionedTableDirWithComplexTypes: File = null\n+  var partitionedTableDirWithKeyAndComplexTypes: File = null\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    partitionedTableDir = Utils.createTempDir()\n+    normalTableDir = Utils.createTempDir()\n+\n+    (1 to 10).foreach { p =>\n+      val partDir = new File(partitionedTableDir, s\"p=$p\")\n+      sparkContext.makeRDD(1 to 10)\n+        .map(i => ParquetData(i, s\"part-$p\"))\n+        .toDF()\n+        .write.parquet(partDir.getCanonicalPath)\n+    }\n+\n+    sparkContext\n+      .makeRDD(1 to 10)\n+      .map(i => ParquetData(i, s\"part-1\"))\n+      .toDF()\n+      .write.parquet(new File(normalTableDir, \"normal\").getCanonicalPath)\n+\n+    partitionedTableDirWithKey = Utils.createTempDir()\n+\n+    (1 to 10).foreach { p =>\n+      val partDir = new File(partitionedTableDirWithKey, s\"p=$p\")\n+      sparkContext.makeRDD(1 to 10)\n+        .map(i => ParquetDataWithKey(p, i, s\"part-$p\"))\n+        .toDF()\n+        .write.parquet(partDir.getCanonicalPath)\n+    }\n+\n+    partitionedTableDirWithKeyAndComplexTypes = Utils.createTempDir()\n+\n+    (1 to 10).foreach { p =>\n+      val partDir = new File(partitionedTableDirWithKeyAndComplexTypes, s\"p=$p\")\n+      sparkContext.makeRDD(1 to 10).map { i =>\n+        ParquetDataWithKeyAndComplexTypes(\n+          p, i, s\"part-$p\", StructContainer(i, f\"${i}_string\"), 1 to i)\n+      }.toDF().write.parquet(partDir.getCanonicalPath)\n+    }\n+\n+    partitionedTableDirWithComplexTypes = Utils.createTempDir()\n+\n+    (1 to 10).foreach { p =>\n+      val partDir = new File(partitionedTableDirWithComplexTypes, s\"p=$p\")\n+      sparkContext.makeRDD(1 to 10).map { i =>\n+        ParquetDataWithComplexTypes(i, s\"part-$p\", StructContainer(i, f\"${i}_string\"), 1 to i)\n+      }.toDF().write.parquet(partDir.getCanonicalPath)\n+    }\n+  }\n+\n+  override protected def afterAll(): Unit = {\n+    try {\n+      partitionedTableDir.delete()\n+      normalTableDir.delete()\n+      partitionedTableDirWithKey.delete()\n+      partitionedTableDirWithComplexTypes.delete()\n+      partitionedTableDirWithKeyAndComplexTypes.delete()\n+    } finally {\n+      super.afterAll()\n+    }\n+  }\n+\n+  /**\n+   * Drop named tables if they exist\n+ *"
  }],
  "prId": 22467
}]