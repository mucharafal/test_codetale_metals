[{
  "comments": [{
    "author": {
      "login": "budde"
    },
    "body": "Will change this to reference the key via the constant in SQLConf rather than ```\"spark.sql.hive.schemaInferenceMode\"```. ",
    "commit": "ced9c4d8363fb4e10e027da5a793ceabed11cfb7",
    "createdAt": "2017-02-15T19:46:27Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.File\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import org.scalatest.BeforeAndAfterEach\n+\n+import org.apache.spark.metrics.source.HiveCatalogMetrics\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog._\n+import org.apache.spark.sql.execution.datasources.FileStatusCache\n+import org.apache.spark.sql.QueryTest\n+import org.apache.spark.sql.hive.client.HiveClient\n+import org.apache.spark.sql.hive.test.TestHiveSingleton\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.test.SQLTestUtils\n+import org.apache.spark.sql.types._\n+\n+class HiveSchemaInferenceSuite\n+  extends QueryTest with TestHiveSingleton with SQLTestUtils with BeforeAndAfterEach {\n+\n+  import HiveSchemaInferenceSuite._\n+\n+  // Create a CatalogTable instance modeling an external Hive table in a metastore that isn't\n+  // controlled by Spark (i.e. has no Spark-specific table properties set).\n+  private def hiveExternalCatalogTable(\n+      tableName: String,\n+      location: String,\n+      schema: StructType,\n+      partitionColumns: Seq[String],\n+      properties: Map[String, String] = Map.empty): CatalogTable = {\n+    CatalogTable(\n+      identifier = TableIdentifier(table = tableName, database = Option(\"default\")),\n+      tableType = CatalogTableType.EXTERNAL,\n+      storage = CatalogStorageFormat(\n+        locationUri = Option(location),\n+        inputFormat = Option(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\"),\n+        outputFormat = Option(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\"),\n+        serde = Option(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"),\n+        compressed = false,\n+        properties = Map(\"serialization.format\" -> \"1\")),\n+      schema = schema,\n+      provider = Option(\"hive\"),\n+      partitionColumnNames = partitionColumns,\n+      properties = properties)\n+  }\n+\n+  // Creates CatalogTablePartition instances for adding partitions of data to our test table.\n+  private def hiveCatalogPartition(location: String, index: Int): CatalogTablePartition\n+    = CatalogTablePartition(\n+      spec = Map(\"partcol1\" -> index.toString, \"partcol2\" -> index.toString),\n+      storage = CatalogStorageFormat(\n+        locationUri = Option(s\"${location}/partCol1=$index/partCol2=$index/\"),\n+        inputFormat = Option(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\"),\n+        outputFormat = Option(\"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\"),\n+        serde = Option(\"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"),\n+        compressed = false,\n+        properties = Map(\"serialization.format\" -> \"1\")))\n+\n+  // Creates a case-sensitive external Hive table for testing schema inference options. Table\n+  // will not have Spark-specific table properties set.\n+  private def setupCaseSensitiveTable(\n+      tableName: String,\n+      dir: File): Unit = {\n+    spark.range(NUM_RECORDS)\n+      .selectExpr(\"id as fieldOne\", \"id as partCol1\", \"id as partCol2\")\n+      .write\n+      .partitionBy(\"partCol1\", \"partCol2\")\n+      .mode(\"overwrite\")\n+      .parquet(dir.getAbsolutePath)\n+\n+    val lowercaseSchema = StructType(Seq(\n+      StructField(\"fieldone\", LongType),\n+      StructField(\"partcol1\", IntegerType),\n+      StructField(\"partcol2\", IntegerType)))\n+\n+    val client = spark.sharedState.externalCatalog.asInstanceOf[HiveExternalCatalog].client\n+\n+    val catalogTable = hiveExternalCatalogTable(\n+      tableName,\n+      dir.getAbsolutePath,\n+      lowercaseSchema,\n+      Seq(\"partcol1\", \"partcol2\"))\n+    client.createTable(catalogTable, true)\n+\n+    val partitions = (0 until NUM_RECORDS).map(hiveCatalogPartition(dir.getAbsolutePath, _)).toSeq\n+    client.createPartitions(\"default\", tableName, partitions, true)\n+  }\n+\n+  // Create a test table used for a single unit test, with data stored in the specified directory.\n+  private def withTestTable(dir: File)(f: File => Unit): Unit = {\n+    setupCaseSensitiveTable(TEST_TABLE_NAME, dir)\n+    try f(dir) finally spark.sql(s\"DROP TABLE IF EXISTS $TEST_TABLE_NAME\")\n+  }\n+\n+  override def beforeEach(): Unit = {\n+    super.beforeEach()\n+    FileStatusCache.resetForTesting()\n+  }\n+\n+  override def afterEach(): Unit = {\n+    super.afterEach()\n+    FileStatusCache.resetForTesting()\n+  }\n+\n+  test(\"Queries against case-sensitive tables with no schema in table properties should work \" +\n+    \"when schema inference is enabled\") {\n+    withSQLConf(\"spark.sql.hive.schemaInferenceMode\" -> \"INFER_AND_SAVE\") {",
    "line": 125
  }],
  "prId": 16942
}]