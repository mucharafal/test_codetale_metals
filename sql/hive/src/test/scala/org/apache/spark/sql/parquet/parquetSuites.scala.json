[{
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Also add a test for `CREATE TABLE ... STORED AS PARQUET AS ...`?\n",
    "commit": "36978d1835ab6e0266ad3787b33056b573fd59e8",
    "createdAt": "2015-02-17T04:33:40Z",
    "diffHunk": "@@ -121,13 +121,50 @@ class ParquetDataSourceOnMetastoreSuite extends ParquetMetastoreSuiteBase {\n \n   override def beforeAll(): Unit = {\n     super.beforeAll()\n+\n+    sql(s\"\"\"\n+      create table test_parquet\n+      (\n+        intField INT,\n+        stringField STRING\n+      )\n+      ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n+       STORED AS\n+       INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n+       OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n+    \"\"\")\n+\n+    val rdd = sparkContext.parallelize((1 to 10).map(i => s\"\"\"{\"a\":$i, \"b\":\"str${i}\"}\"\"\"))\n+    jsonRDD(rdd).registerTempTable(\"jt\")\n+    sql(\"\"\"\n+      create table test ROW FORMAT\n+          |  SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n+          |  STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n+          |  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n+          |  AS select * from jt\"\"\".stripMargin)\n+",
    "line": 25
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "`STORED AS PARQUET` is supported since Hive 0.13, the unit test may failed in Hive 0.12 if we do that.\n",
    "commit": "36978d1835ab6e0266ad3787b33056b573fd59e8",
    "createdAt": "2015-02-17T05:01:02Z",
    "diffHunk": "@@ -121,13 +121,50 @@ class ParquetDataSourceOnMetastoreSuite extends ParquetMetastoreSuiteBase {\n \n   override def beforeAll(): Unit = {\n     super.beforeAll()\n+\n+    sql(s\"\"\"\n+      create table test_parquet\n+      (\n+        intField INT,\n+        stringField STRING\n+      )\n+      ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n+       STORED AS\n+       INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n+       OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n+    \"\"\")\n+\n+    val rdd = sparkContext.parallelize((1 to 10).map(i => s\"\"\"{\"a\":$i, \"b\":\"str${i}\"}\"\"\"))\n+    jsonRDD(rdd).registerTempTable(\"jt\")\n+    sql(\"\"\"\n+      create table test ROW FORMAT\n+          |  SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n+          |  STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n+          |  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n+          |  AS select * from jt\"\"\".stripMargin)\n+",
    "line": 25
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "How about we use `if (HiveShim.version ==\"0.13.1\")` to check the Hive version like what we did in https://github.com/apache/spark/commit/e0490e271d078aa55d7c7583e2ba80337ed1b0c4.\n",
    "commit": "36978d1835ab6e0266ad3787b33056b573fd59e8",
    "createdAt": "2015-02-17T05:07:01Z",
    "diffHunk": "@@ -121,13 +121,50 @@ class ParquetDataSourceOnMetastoreSuite extends ParquetMetastoreSuiteBase {\n \n   override def beforeAll(): Unit = {\n     super.beforeAll()\n+\n+    sql(s\"\"\"\n+      create table test_parquet\n+      (\n+        intField INT,\n+        stringField STRING\n+      )\n+      ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n+       STORED AS\n+       INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n+       OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n+    \"\"\")\n+\n+    val rdd = sparkContext.parallelize((1 to 10).map(i => s\"\"\"{\"a\":$i, \"b\":\"str${i}\"}\"\"\"))\n+    jsonRDD(rdd).registerTempTable(\"jt\")\n+    sql(\"\"\"\n+      create table test ROW FORMAT\n+          |  SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n+          |  STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n+          |  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n+          |  AS select * from jt\"\"\".stripMargin)\n+",
    "line": 25
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Oh, i thought `STORED AS PARQUERT AS ..` is just the syntactic sugar. Unfortunately, all of the test suite are implemented in the sub project `sql`, but the `HiveShim` is in the subproject `hive` with `hive` package accessing visibility.\n\nLet's put this test in another PR?\n",
    "commit": "36978d1835ab6e0266ad3787b33056b573fd59e8",
    "createdAt": "2015-02-17T07:30:56Z",
    "diffHunk": "@@ -121,13 +121,50 @@ class ParquetDataSourceOnMetastoreSuite extends ParquetMetastoreSuiteBase {\n \n   override def beforeAll(): Unit = {\n     super.beforeAll()\n+\n+    sql(s\"\"\"\n+      create table test_parquet\n+      (\n+        intField INT,\n+        stringField STRING\n+      )\n+      ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n+       STORED AS\n+       INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n+       OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n+    \"\"\")\n+\n+    val rdd = sparkContext.parallelize((1 to 10).map(i => s\"\"\"{\"a\":$i, \"b\":\"str${i}\"}\"\"\"))\n+    jsonRDD(rdd).registerTempTable(\"jt\")\n+    sql(\"\"\"\n+      create table test ROW FORMAT\n+          |  SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n+          |  STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\n+          |  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n+          |  AS select * from jt\"\"\".stripMargin)\n+",
    "line": 25
  }],
  "prId": 4562
}]