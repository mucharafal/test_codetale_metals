[{
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "All the changes made above in this file are used to resolve a logical conflict with PR #15703. We don't really have any aggregate functions that don't support partial aggregation now after merging #15703, must update the tests to reflect that.",
    "commit": "6db5af95e456d6529a37c243f41a4632a69f40d0",
    "createdAt": "2016-11-22T18:22:51Z",
    "diffHunk": "@@ -325,70 +320,67 @@ class ObjectHashAggregateSuite\n \n             // Currently Spark SQL doesn't support evaluating distinct aggregate function together\n             // with aggregate functions without partial aggregation support.\n-            if (!(aggs.contains(withoutPartial) && aggs.contains(withDistinct))) {\n-              // TODO Re-enables them after fixing SPARK-18403\n-              ignore(\n-                s\"randomized aggregation test - \" +\n-                  s\"${names.mkString(\"[\", \", \", \"]\")} - \" +\n-                  s\"${if (withGroupingKeys) \"with\" else \"without\"} grouping keys - \" +\n-                  s\"with ${if (emptyInput) \"empty\" else \"non-empty\"} input\"\n-              ) {\n-                var expected: Seq[Row] = null\n-                var actual1: Seq[Row] = null\n-                var actual2: Seq[Row] = null\n-\n-                // Disables `ObjectHashAggregateExec` to obtain a standard answer\n-                withSQLConf(SQLConf.USE_OBJECT_HASH_AGG.key -> \"false\") {\n-                  val aggDf = doAggregation(df)\n-\n-                  if (aggs.intersect(Seq(withoutPartial, withPartialSafe, typed)).nonEmpty) {\n-                    assert(containsSortAggregateExec(aggDf))\n-                    assert(!containsObjectHashAggregateExec(aggDf))\n-                    assert(!containsHashAggregateExec(aggDf))\n-                  } else {\n-                    assert(!containsSortAggregateExec(aggDf))\n-                    assert(!containsObjectHashAggregateExec(aggDf))\n-                    assert(containsHashAggregateExec(aggDf))\n-                  }\n-\n-                  expected = aggDf.collect().toSeq\n+            test(\n+              s\"randomized aggregation test - \" +\n+                s\"${names.mkString(\"[\", \", \", \"]\")} - \" +\n+                s\"${if (withGroupingKeys) \"with\" else \"without\"} grouping keys - \" +\n+                s\"with ${if (emptyInput) \"empty\" else \"non-empty\"} input\"\n+            ) {\n+              var expected: Seq[Row] = null\n+              var actual1: Seq[Row] = null\n+              var actual2: Seq[Row] = null\n+\n+              // Disables `ObjectHashAggregateExec` to obtain a standard answer\n+              withSQLConf(SQLConf.USE_OBJECT_HASH_AGG.key -> \"false\") {\n+                val aggDf = doAggregation(df)\n+\n+                if (aggs.intersect(Seq(withPartialSafe, typed)).nonEmpty) {\n+                  assert(containsSortAggregateExec(aggDf))\n+                  assert(!containsObjectHashAggregateExec(aggDf))\n+                  assert(!containsHashAggregateExec(aggDf))\n+                } else {\n+                  assert(!containsSortAggregateExec(aggDf))\n+                  assert(!containsObjectHashAggregateExec(aggDf))\n+                  assert(containsHashAggregateExec(aggDf))\n                 }\n \n-                // Enables `ObjectHashAggregateExec`\n-                withSQLConf(SQLConf.USE_OBJECT_HASH_AGG.key -> \"true\") {\n-                  val aggDf = doAggregation(df)\n-\n-                  if (aggs.contains(typed) && !aggs.contains(withoutPartial)) {\n-                    assert(!containsSortAggregateExec(aggDf))\n-                    assert(containsObjectHashAggregateExec(aggDf))\n-                    assert(!containsHashAggregateExec(aggDf))\n-                  } else if (aggs.intersect(Seq(withoutPartial, withPartialSafe)).nonEmpty) {\n-                    assert(containsSortAggregateExec(aggDf))\n-                    assert(!containsObjectHashAggregateExec(aggDf))\n-                    assert(!containsHashAggregateExec(aggDf))\n-                  } else {\n-                    assert(!containsSortAggregateExec(aggDf))\n-                    assert(!containsObjectHashAggregateExec(aggDf))\n-                    assert(containsHashAggregateExec(aggDf))\n-                  }\n-\n-                  // Disables sort-based aggregation fallback (we only generate 50 rows, so 100 is\n-                  // big enough) to obtain a result to be checked.\n-                  withSQLConf(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key -> \"100\") {\n-                    actual1 = aggDf.collect().toSeq\n-                  }\n-\n-                  // Enables sort-based aggregation fallback to obtain another result to be checked.\n-                  withSQLConf(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key -> \"3\") {\n-                    // Here we are not reusing `aggDf` because the physical plan in `aggDf` is\n-                    // cached and won't be re-planned using the new fallback threshold.\n-                    actual2 = doAggregation(df).collect().toSeq\n-                  }\n+                expected = aggDf.collect().toSeq\n+              }\n+\n+              // Enables `ObjectHashAggregateExec`\n+              withSQLConf(SQLConf.USE_OBJECT_HASH_AGG.key -> \"true\") {\n+                val aggDf = doAggregation(df)\n+\n+                if (aggs.contains(typed)) {\n+                  assert(!containsSortAggregateExec(aggDf))\n+                  assert(containsObjectHashAggregateExec(aggDf))\n+                  assert(!containsHashAggregateExec(aggDf))\n+                } else if (aggs.contains(withPartialSafe)) {\n+                  assert(containsSortAggregateExec(aggDf))\n+                  assert(!containsObjectHashAggregateExec(aggDf))\n+                  assert(!containsHashAggregateExec(aggDf))\n+                } else {\n+                  assert(!containsSortAggregateExec(aggDf))\n+                  assert(!containsObjectHashAggregateExec(aggDf))\n+                  assert(containsHashAggregateExec(aggDf))\n                 }\n \n-                doubleSafeCheckRows(actual1, expected, 1e-4)\n-                doubleSafeCheckRows(actual2, expected, 1e-4)\n+                // Disables sort-based aggregation fallback (we only generate 50 rows, so 100 is\n+                // big enough) to obtain a result to be checked.\n+                withSQLConf(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key -> \"100\") {\n+                  actual1 = aggDf.collect().toSeq\n+                }\n+\n+                // Enables sort-based aggregation fallback to obtain another result to be checked.\n+                withSQLConf(SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key -> \"3\") {\n+                  // Here we are not reusing `aggDf` because the physical plan in `aggDf` is\n+                  // cached and won't be re-planned using the new fallback threshold.\n+                  actual2 = doAggregation(df).collect().toSeq\n+                }\n               }\n+\n+              doubleSafeCheckRows(actual1, expected, 1e-4)\n+              doubleSafeCheckRows(actual2, expected, 1e-4)",
    "line": 164
  }],
  "prId": 15976
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "not related to this PR, but the config name looks weird, how about `OBJECT_AGG_FALLBACK_TO_SORT_THRESHOLD`",
    "commit": "6db5af95e456d6529a37c243f41a4632a69f40d0",
    "createdAt": "2016-11-23T15:43:21Z",
    "diffHunk": "@@ -425,7 +417,35 @@ class ObjectHashAggregateSuite\n     }\n   }\n \n-  private def function(name: String, args: Column*): Column = {\n-    Column(UnresolvedFunction(FunctionIdentifier(name), args.map(_.expr), isDistinct = false))\n+  test(\"SPARK-18403 Fix unsafe data false sharing issue in ObjectHashAggregateExec\") {\n+    // SPARK-18403: An unsafe data false sharing issue may trigger OOM / SIGSEGV when evaluating\n+    // certain aggregate functions. To reproduce this issue, the following conditions must be\n+    // met:\n+    //\n+    //  1. The aggregation must be evaluated using `ObjectHashAggregateExec`;\n+    //  2. There must be an input column whose data type involves `ArrayType` or `MapType`;\n+    //  3. Sort-based aggregation fallback must be triggered during evaluation.\n+    withSQLConf(\n+      SQLConf.USE_OBJECT_HASH_AGG.key -> \"true\",\n+      SQLConf.OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD.key -> \"1\"",
    "line": 184
  }],
  "prId": 15976
}]