[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Maybe instead of #1, #2, etc name them \"Maps\", \"Arrays\", etc\n",
    "commit": "9f0aff33e862746d3d295a9dbf2629665d80cc22",
    "createdAt": "2014-12-02T00:06:08Z",
    "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.Date\n+import java.util\n+\n+import org.apache.hadoop.hive.serde2.io.DoubleWritable\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.types.decimal.Decimal\n+import org.scalatest.FunSuite\n+\n+import org.apache.hadoop.hive.ql.udf.UDAFPercentile\n+import org.apache.hadoop.hive.serde2.objectinspector.{ObjectInspector, StructObjectInspector, ObjectInspectorFactory}\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.ObjectInspectorOptions\n+import org.apache.hadoop.io.LongWritable\n+\n+import org.apache.spark.sql.catalyst.expressions.{Literal, Row}\n+\n+class HiveInspectorSuite extends FunSuite with HiveInspectors {\n+  test(\"Test wrap SettableStructObjectInspector\") {\n+    val udaf = new UDAFPercentile.PercentileLongEvaluator()\n+    udaf.init()\n+\n+    udaf.iterate(new LongWritable(1), 0.1)\n+    udaf.iterate(new LongWritable(1), 0.1)\n+\n+    val state = udaf.terminatePartial()\n+\n+    val soi = ObjectInspectorFactory.getReflectionObjectInspector(\n+      classOf[UDAFPercentile.State],\n+      ObjectInspectorOptions.JAVA).asInstanceOf[StructObjectInspector]\n+\n+    val a = unwrap(state, soi).asInstanceOf[Row]\n+    val b = wrap(a, soi).asInstanceOf[UDAFPercentile.State]\n+\n+    val sfCounts = soi.getStructFieldRef(\"counts\")\n+    val sfPercentiles = soi.getStructFieldRef(\"percentiles\")\n+\n+    assert(2 == soi.getStructFieldData(b, sfCounts)\n+      .asInstanceOf[util.Map[LongWritable, LongWritable]]\n+      .get(new LongWritable(1L))\n+      .get())\n+    assert(0.1 == soi.getStructFieldData(b, sfPercentiles)\n+      .asInstanceOf[util.ArrayList[DoubleWritable]]\n+      .get(0)\n+      .get())\n+  }\n+\n+  val data =\n+    Literal(true) ::\n+    Literal(0.asInstanceOf[Byte]) ::\n+    Literal(0.asInstanceOf[Short]) ::\n+    Literal(0) ::\n+    Literal(0.asInstanceOf[Long]) ::\n+    Literal(0.asInstanceOf[Float]) ::\n+    Literal(0.asInstanceOf[Double]) ::\n+    Literal(\"0\") ::\n+    Literal(new Date(2014, 9, 23)) ::\n+    Literal(Decimal(BigDecimal(123.123))) ::\n+    Literal(new java.sql.Timestamp(123123)) ::\n+    Literal(Array[Byte](1,2,3)) ::\n+    Literal(Seq[Int](1,2,3), ArrayType(IntegerType)) ::\n+    Literal(Map[Int, Int](1->2, 2->1), MapType(IntegerType, IntegerType)) ::\n+    Literal(Row(1,2.0d,3.0f),\n+      StructType(StructField(\"c1\", IntegerType) ::\n+      StructField(\"c2\", DoubleType) ::\n+      StructField(\"c3\", FloatType) :: Nil)) ::\n+    Nil\n+\n+  val row = data.map(_.eval(null))\n+  val dataTypes = data.map(_.dataType)\n+\n+  import scala.collection.JavaConversions._\n+  def toWritableInspector(dataType: DataType): ObjectInspector = dataType match {\n+    case ArrayType(tpe, _) =>\n+      ObjectInspectorFactory.getStandardListObjectInspector(toWritableInspector(tpe))\n+    case MapType(keyType, valueType, _) =>\n+      ObjectInspectorFactory.getStandardMapObjectInspector(\n+        toWritableInspector(keyType), toWritableInspector(valueType))\n+    case StringType => PrimitiveObjectInspectorFactory.writableStringObjectInspector\n+    case IntegerType => PrimitiveObjectInspectorFactory.writableIntObjectInspector\n+    case DoubleType => PrimitiveObjectInspectorFactory.writableDoubleObjectInspector\n+    case BooleanType => PrimitiveObjectInspectorFactory.writableBooleanObjectInspector\n+    case LongType => PrimitiveObjectInspectorFactory.writableLongObjectInspector\n+    case FloatType => PrimitiveObjectInspectorFactory.writableFloatObjectInspector\n+    case ShortType => PrimitiveObjectInspectorFactory.writableShortObjectInspector\n+    case ByteType => PrimitiveObjectInspectorFactory.writableByteObjectInspector\n+    case NullType => PrimitiveObjectInspectorFactory.writableVoidObjectInspector\n+    case BinaryType => PrimitiveObjectInspectorFactory.writableBinaryObjectInspector\n+    case DateType => PrimitiveObjectInspectorFactory.writableDateObjectInspector\n+    case TimestampType => PrimitiveObjectInspectorFactory.writableTimestampObjectInspector\n+    case DecimalType() => PrimitiveObjectInspectorFactory.writableHiveDecimalObjectInspector\n+    case StructType(fields) =>\n+      ObjectInspectorFactory.getStandardStructObjectInspector(\n+        fields.map(f => f.name), fields.map(f => toWritableInspector(f.dataType)))\n+  }\n+\n+  def checkDataType(dt1: Seq[DataType], dt2: Seq[DataType]): Unit = {\n+    dt1.zip(dt2).map {\n+      case (dd1, dd2) =>\n+        assert(dd1.getClass == dd2.getClass)  // DecimalType doesn't has the default precision info\n+    }\n+  }\n+\n+  def checkValues(row1: Seq[Any], row2: Seq[Any]): Unit = {\n+    row1.zip(row2).map {\n+      case (r1, r2) => checkValues(r1, r2)\n+    }\n+  }\n+\n+  def checkValues(v1: Any, v2: Any): Unit = {\n+    (v1, v2) match {\n+      case (r1: Decimal, r2: Decimal) =>\n+        // Ignore the Decimal precision\n+        assert(r1.compare(r2) == 0)\n+      case (r1: Array[Byte], r2: Array[Byte])\n+        if r1 != null && r2 != null && r1.length == r2.length =>\n+        r1.zip(r2).map { case (b1, b2) => assert(b1 == b2) }\n+      case (r1: Date, r2: Date) => assert(r1.compareTo(r2) == 0)\n+      case (r1, r2) => assert(r1 == r2)\n+    }\n+  }\n+\n+  test(\"oi => datatype => oi\") {\n+    val ois = dataTypes.map(toInspector)\n+\n+    checkDataType(ois.map(inspectorToDataType), dataTypes)\n+    checkDataType(dataTypes.map(toWritableInspector).map(inspectorToDataType), dataTypes)\n+  }\n+\n+  test(\"wrap / unwrap #1\") {\n+    val writableOIs = dataTypes.map(toWritableInspector)\n+    val nullRow = data.map(d => null)\n+\n+    checkValues(nullRow, nullRow.zip(writableOIs).map {\n+      case (d, oi) => unwrap(wrap(d, oi), oi)\n+    })\n+\n+    // struct couldn't be constant, sweep it out\n+    val constantExprs = data.filter(!_.dataType.isInstanceOf[StructType])\n+    val constantData = constantExprs.map(_.eval())\n+    val constantNullData = constantData.map(_ => null)\n+    val constantWritableOIs = constantExprs.map(e => toWritableInspector(e.dataType))\n+    val constantNullWritableOIs = constantExprs.map(e => toInspector(Literal(null, e.dataType)))\n+\n+    checkValues(constantData, constantData.zip(constantWritableOIs).map {\n+      case (d, oi) => unwrap(wrap(d, oi), oi)\n+    })\n+\n+    checkValues(constantNullData, constantData.zip(constantNullWritableOIs).map {\n+      case (d, oi) => unwrap(wrap(d, oi), oi)\n+    })\n+\n+    checkValues(constantNullData, constantNullData.zip(constantWritableOIs).map {\n+      case (d, oi) => unwrap(wrap(d, oi), oi)\n+    })\n+  }\n+\n+  test(\"wrap / unwrap #2\") {\n+    val writableOIs = dataTypes.map(toWritableInspector)\n+\n+    checkValues(row, row.zip(writableOIs).map {\n+      case (data, oi) => unwrap(wrap(data, oi), oi)\n+    })\n+  }\n+\n+  test(\"wrap / unwrap #3\") {\n+    val ois = dataTypes.map(toInspector)\n+\n+    checkValues(row, row.zip(ois).map {\n+      case (data, oi) => unwrap(wrap(data, oi), oi)\n+    })\n+  }\n+\n+  test(\"wrap / unwrap #4\") {\n+    val dt = StructType(dataTypes.zipWithIndex.map {\n+      case (t, idx) => StructField(s\"c_$idx\", t)\n+    })\n+\n+    checkValues(row, unwrap(wrap(row, toInspector(dt)), toInspector(dt)).asInstanceOf[Row])\n+    checkValues(null, unwrap(wrap(null, toInspector(dt)), toInspector(dt)))\n+  }\n+\n+  test(\"wrap / unwrap #5\") {\n+    val dt = ArrayType(dataTypes(0))\n+\n+    val d = row(0) :: row(0) :: Nil\n+    checkValues(d, unwrap(wrap(d, toInspector(dt)), toInspector(dt)))\n+    checkValues(null, unwrap(wrap(null, toInspector(dt)), toInspector(dt)))\n+    checkValues(d, unwrap(wrap(d, toInspector(Literal(d, dt))), toInspector(Literal(d, dt))))\n+    checkValues(d, unwrap(wrap(null, toInspector(Literal(d, dt))), toInspector(Literal(d, dt))))\n+  }\n+\n+  test(\"wrap / unwrap #6\") {"
  }],
  "prId": 3429
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "A general comment about this test suite: would be better to use `===` rather than `==` in assertions to enable more friendly error messages with actual data values when tests fail.\n",
    "commit": "9f0aff33e862746d3d295a9dbf2629665d80cc22",
    "createdAt": "2014-12-18T08:19:14Z",
    "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.Date\n+import java.util\n+\n+import org.apache.hadoop.hive.serde2.io.DoubleWritable\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory\n+import org.apache.spark.sql.catalyst.types._\n+import org.apache.spark.sql.catalyst.types.decimal.Decimal\n+import org.scalatest.FunSuite\n+\n+import org.apache.hadoop.hive.ql.udf.UDAFPercentile\n+import org.apache.hadoop.hive.serde2.objectinspector.{ObjectInspector, StructObjectInspector, ObjectInspectorFactory}\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.ObjectInspectorOptions\n+import org.apache.hadoop.io.LongWritable\n+\n+import org.apache.spark.sql.catalyst.expressions.{Literal, Row}\n+\n+class HiveInspectorSuite extends FunSuite with HiveInspectors {",
    "line": 36
  }],
  "prId": 3429
}]