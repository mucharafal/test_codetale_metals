[{
  "comments": [{
    "author": {
      "login": "mallman"
    },
    "body": "I suggest wrapping this and other such calls in an `assert` to declare and validate assumptions, e.g.\n\n```\nassert(spark.sql(\"select * from test where partCol1 = 999\").count() == 0)\n```\n\nwhereas\n\n```\nassert(spark.sql(\"select * from test where partCol1 < 2\").count() == 2)\n```\n",
    "commit": "2a965377258d4d77db5a3f00d4257bbacc4a0adb",
    "createdAt": "2016-10-19T03:07:17Z",
    "diffHunk": "@@ -103,11 +92,84 @@ class HiveDataFrameSuite extends QueryTest with TestHiveSingleton with SQLTestUt\n           assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n           assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n \n-          // read all should be cached\n+          // read all should not be cached\n           HiveCatalogMetrics.reset()\n           spark.sql(\"select * from test\").count()\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+\n+          // cache should be disabled\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"lazy partition pruning with file status caching enabled\") {\n+    withSQLConf(\n+        \"spark.sql.hive.filesourcePartitionPruning\" -> \"true\",\n+        \"spark.sql.hive.filesourcePartitionFileCacheEnabled\" -> \"true\") {\n+      withTable(\"test\") {\n+        withTempDir { dir =>\n+          setupPartitionedTable(\"test\", dir)\n+          HiveCatalogMetrics.reset()\n+          spark.sql(\"select * from test where partCol1 = 999\").count()"
  }, {
    "author": {
      "login": "ericl"
    },
    "body": "Done\n",
    "commit": "2a965377258d4d77db5a3f00d4257bbacc4a0adb",
    "createdAt": "2016-10-19T19:55:58Z",
    "diffHunk": "@@ -103,11 +92,84 @@ class HiveDataFrameSuite extends QueryTest with TestHiveSingleton with SQLTestUt\n           assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n           assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n \n-          // read all should be cached\n+          // read all should not be cached\n           HiveCatalogMetrics.reset()\n           spark.sql(\"select * from test\").count()\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+\n+          // cache should be disabled\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"lazy partition pruning with file status caching enabled\") {\n+    withSQLConf(\n+        \"spark.sql.hive.filesourcePartitionPruning\" -> \"true\",\n+        \"spark.sql.hive.filesourcePartitionFileCacheEnabled\" -> \"true\") {\n+      withTable(\"test\") {\n+        withTempDir { dir =>\n+          setupPartitionedTable(\"test\", dir)\n+          HiveCatalogMetrics.reset()\n+          spark.sql(\"select * from test where partCol1 = 999\").count()"
  }],
  "prId": 15539
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "should we test the evicting behaviour?\n",
    "commit": "2a965377258d4d77db5a3f00d4257bbacc4a0adb",
    "createdAt": "2016-10-20T02:29:09Z",
    "diffHunk": "@@ -103,11 +92,103 @@ class HiveDataFrameSuite extends QueryTest with TestHiveSingleton with SQLTestUt\n           assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n           assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n \n-          // read all should be cached\n+          // read all should not be cached\n           HiveCatalogMetrics.reset()\n           spark.sql(\"select * from test\").count()\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+\n+          // cache should be disabled\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"lazy partition pruning with file status caching enabled\") {\n+    withSQLConf(\n+        \"spark.sql.hive.filesourcePartitionPruning\" -> \"true\",\n+        \"spark.sql.hive.filesourcePartitionFileCacheSize\" -> \"9999999\") {\n+      withTable(\"test\") {\n+        withTempDir { dir =>\n+          setupPartitionedTable(\"test\", dir)\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test where partCol1 = 999\").count() == 0)\n           assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 0)\n           assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 0)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test where partCol1 < 2\").count() == 2)\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 2)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 2)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test where partCol1 < 3\").count() == 3)\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 3)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 1)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 2)\n+\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 2)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 3)\n+\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 0)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 5)\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"file status caching respects refresh table and refreshByPath\") {\n+    withSQLConf(\n+        \"spark.sql.hive.filesourcePartitionPruning\" -> \"true\",\n+        \"spark.sql.hive.filesourcePartitionFileCacheSize\" -> \"9999999\") {\n+      withTable(\"test\") {\n+        withTempDir { dir =>\n+          setupPartitionedTable(\"test\", dir)\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+\n+          HiveCatalogMetrics.reset()\n+          spark.sql(\"refresh table test\")\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+\n+          spark.catalog.cacheTable(\"test\")\n+          HiveCatalogMetrics.reset()\n+          spark.catalog.refreshByPath(dir.getAbsolutePath)\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"file status cache respects size limit\") {\n+    withSQLConf(\n+        \"spark.sql.hive.filesourcePartitionPruning\" -> \"true\",\n+        \"spark.sql.hive.filesourcePartitionFileCacheSize\" -> \"1\" /* 1 byte */) {",
    "line": 140
  }, {
    "author": {
      "login": "ericl"
    },
    "body": "Yeah this test covers that\n",
    "commit": "2a965377258d4d77db5a3f00d4257bbacc4a0adb",
    "createdAt": "2016-10-20T18:59:28Z",
    "diffHunk": "@@ -103,11 +92,103 @@ class HiveDataFrameSuite extends QueryTest with TestHiveSingleton with SQLTestUt\n           assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n           assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n \n-          // read all should be cached\n+          // read all should not be cached\n           HiveCatalogMetrics.reset()\n           spark.sql(\"select * from test\").count()\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+\n+          // cache should be disabled\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"lazy partition pruning with file status caching enabled\") {\n+    withSQLConf(\n+        \"spark.sql.hive.filesourcePartitionPruning\" -> \"true\",\n+        \"spark.sql.hive.filesourcePartitionFileCacheSize\" -> \"9999999\") {\n+      withTable(\"test\") {\n+        withTempDir { dir =>\n+          setupPartitionedTable(\"test\", dir)\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test where partCol1 = 999\").count() == 0)\n           assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 0)\n           assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 0)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test where partCol1 < 2\").count() == 2)\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 2)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 2)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test where partCol1 < 3\").count() == 3)\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 3)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 1)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 2)\n+\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 2)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 3)\n+\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 0)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 5)\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"file status caching respects refresh table and refreshByPath\") {\n+    withSQLConf(\n+        \"spark.sql.hive.filesourcePartitionPruning\" -> \"true\",\n+        \"spark.sql.hive.filesourcePartitionFileCacheSize\" -> \"9999999\") {\n+      withTable(\"test\") {\n+        withTempDir { dir =>\n+          setupPartitionedTable(\"test\", dir)\n+          HiveCatalogMetrics.reset()\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+\n+          HiveCatalogMetrics.reset()\n+          spark.sql(\"refresh table test\")\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+\n+          spark.catalog.cacheTable(\"test\")\n+          HiveCatalogMetrics.reset()\n+          spark.catalog.refreshByPath(dir.getAbsolutePath)\n+          assert(spark.sql(\"select * from test\").count() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"file status cache respects size limit\") {\n+    withSQLConf(\n+        \"spark.sql.hive.filesourcePartitionPruning\" -> \"true\",\n+        \"spark.sql.hive.filesourcePartitionFileCacheSize\" -> \"1\" /* 1 byte */) {",
    "line": 140
  }],
  "prId": 15539
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: please use SQLConf.HIVE_FILESOURCE_PARTITION_PRUNING.key\n",
    "commit": "2a965377258d4d77db5a3f00d4257bbacc4a0adb",
    "createdAt": "2016-10-22T02:07:24Z",
    "diffHunk": "@@ -104,11 +107,103 @@ class HiveDataFrameSuite extends QueryTest with TestHiveSingleton with SQLTestUt\n           assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n           assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n \n-          // read all should be cached\n+          // read all should not be cached\n           HiveCatalogMetrics.reset()\n           spark.sql(\"select * from test\").count()\n+          assert(HiveCatalogMetrics.METRIC_PARTITIONS_FETCHED.getCount() == 5)\n+          assert(HiveCatalogMetrics.METRIC_FILES_DISCOVERED.getCount() == 5)\n+\n+          // cache should be disabled\n+          assert(HiveCatalogMetrics.METRIC_FILE_CACHE_HITS.getCount() == 0)\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"lazy partition pruning with file status caching enabled\") {\n+    withSQLConf(\n+        \"spark.sql.hive.filesourcePartitionPruning\" -> \"true\",",
    "line": 69
  }],
  "prId": 15539
}]