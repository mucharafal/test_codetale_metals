[{
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "It is weird that this works. The Min and the Max value should not be equal for Short and Long types.\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-16T13:28:46Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.BasicColStats\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    val noColumnError = intercept[AnalysisException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+    assert(noColumnError.message == \"Need to specify the columns to analyze. Usage: \" +\n+      \"ANALYZE TABLE tbl COMPUTE STATISTICS FOR COLUMNS key, value\")\n+\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table (key INT, value STRING)\")\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS k\")\n+      }\n+      assert(invalidColError.message == s\"Invalid column name: k\")\n+\n+      val duplicateColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, key\")\n+      }\n+      assert(duplicateColError.message == s\"Duplicate column name: key\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS keY\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: keY\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val duplicateErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, vaLue\")\n+        }\n+        assert(duplicateErr.message == s\"Duplicate column name: vaLue\")\n+      }\n+    }\n+  }\n+\n+  test(\"basic statistics for integral type columns\") {\n+    val rdd = sparkContext.parallelize(Seq(\"1\", null, \"2\", \"3\", null)).map { i =>\n+      if (i != null) Row(i.toByte, i.toShort, i.toInt, i.toLong) else Row(i, i, i, i)\n+    }\n+    val schema = StructType(\n+      StructField(name = \"c1\", dataType = ByteType, nullable = true) ::\n+        StructField(name = \"c2\", dataType = ShortType, nullable = true) ::\n+        StructField(name = \"c3\", dataType = IntegerType, nullable = true) ::\n+        StructField(name = \"c4\", dataType = LongType, nullable = true) :: Nil)\n+    val expectedBasicStats = BasicColStats(\n+      dataType = ByteType, numNulls = 2, max = Some(3), min = Some(1), ndv = Some(3))"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "Can you explain more about this?\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-18T03:26:31Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.BasicColStats\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    val noColumnError = intercept[AnalysisException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+    assert(noColumnError.message == \"Need to specify the columns to analyze. Usage: \" +\n+      \"ANALYZE TABLE tbl COMPUTE STATISTICS FOR COLUMNS key, value\")\n+\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table (key INT, value STRING)\")\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS k\")\n+      }\n+      assert(invalidColError.message == s\"Invalid column name: k\")\n+\n+      val duplicateColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, key\")\n+      }\n+      assert(duplicateColError.message == s\"Duplicate column name: key\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS keY\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: keY\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val duplicateErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, vaLue\")\n+        }\n+        assert(duplicateErr.message == s\"Duplicate column name: vaLue\")\n+      }\n+    }\n+  }\n+\n+  test(\"basic statistics for integral type columns\") {\n+    val rdd = sparkContext.parallelize(Seq(\"1\", null, \"2\", \"3\", null)).map { i =>\n+      if (i != null) Row(i.toByte, i.toShort, i.toInt, i.toLong) else Row(i, i, i, i)\n+    }\n+    val schema = StructType(\n+      StructField(name = \"c1\", dataType = ByteType, nullable = true) ::\n+        StructField(name = \"c2\", dataType = ShortType, nullable = true) ::\n+        StructField(name = \"c3\", dataType = IntegerType, nullable = true) ::\n+        StructField(name = \"c4\", dataType = LongType, nullable = true) :: Nil)\n+    val expectedBasicStats = BasicColStats(\n+      dataType = ByteType, numNulls = 2, max = Some(3), min = Some(1), ndv = Some(3))"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@wzhfy I guess @hvanhovell understood `\"1\", null, \"2\", \"3\", null` are the actual values for each row. Could we maybe make this easier to read? How about the codes below?\n\n``` scala\nval values = (0 to 5).map { i =>\n  if (i % 2 == 0) None else Some(i)\n}\nval data = values.map { i =>\n  (i.map(_.toByte), i.map(_.toShort), i.map(_.toInt), i.map(_.toLong))\n}\n\nval df = data.toDF(\"c1\", \"c2\", \"c3\", \"c4\")\nval statsSeq = df.schema.map { f =>\n  val basicStats = BasicColStats(\n    dataType = f.dataType,\n    numNulls = values.count(_.isEmpty),\n    max = values.filter(_.isDefined).max,\n    min = values.filter(_.isDefined).min,\n    ndv = Some(values.distinct.length.toLong))\n  (f.name, basicStats)\n}\n\ncheckColStats(df, statsSeq)\n```\n\nwith importing  `import testImplicits._` right below `StatisticsColumnSuite` class definition and then changing `checkColStats`\n\nas below:\n\n``` scala\ndef checkColStats(\n    df: DataFrame,\n    expectedColStatsSeq: Seq[(String, BasicColStats)]): Unit = {\n  val table = \"tbl\"\n  withTable(table) {\n    df.write.format(\"json\").saveAsTable(table)\n    val columns = expectedColStatsSeq.map(_._1).mkString(\", \")\n    sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS $columns\")\n    val readback = sql(s\"SELECT * FROM $table\")\n    val stats = readback.queryExecution.analyzed.collect {\n      case rel: LogicalRelation =>\n        expectedColStatsSeq.foreach { expected =>\n          assert(rel.catalogTable.get.stats.get.basicColStats.contains(expected._1))\n          checkColStats(colStats = rel.catalogTable.get.stats.get.basicColStats(expected._1),\n            expectedColStats = expected._2)\n        }\n    }\n    assert(stats.size == 1)\n  }\n}\n```\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-19T07:07:19Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.BasicColStats\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    val noColumnError = intercept[AnalysisException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+    assert(noColumnError.message == \"Need to specify the columns to analyze. Usage: \" +\n+      \"ANALYZE TABLE tbl COMPUTE STATISTICS FOR COLUMNS key, value\")\n+\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table (key INT, value STRING)\")\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS k\")\n+      }\n+      assert(invalidColError.message == s\"Invalid column name: k\")\n+\n+      val duplicateColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, key\")\n+      }\n+      assert(duplicateColError.message == s\"Duplicate column name: key\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS keY\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: keY\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val duplicateErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, vaLue\")\n+        }\n+        assert(duplicateErr.message == s\"Duplicate column name: vaLue\")\n+      }\n+    }\n+  }\n+\n+  test(\"basic statistics for integral type columns\") {\n+    val rdd = sparkContext.parallelize(Seq(\"1\", null, \"2\", \"3\", null)).map { i =>\n+      if (i != null) Row(i.toByte, i.toShort, i.toInt, i.toLong) else Row(i, i, i, i)"
  }, {
    "author": {
      "login": "wzhfy"
    },
    "body": "@HyukjinKwon Seems better. Let me change the code based on this. Thanks.\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-20T01:04:10Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.BasicColStats\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    val noColumnError = intercept[AnalysisException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+    assert(noColumnError.message == \"Need to specify the columns to analyze. Usage: \" +\n+      \"ANALYZE TABLE tbl COMPUTE STATISTICS FOR COLUMNS key, value\")\n+\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table (key INT, value STRING)\")\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS k\")\n+      }\n+      assert(invalidColError.message == s\"Invalid column name: k\")\n+\n+      val duplicateColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, key\")\n+      }\n+      assert(duplicateColError.message == s\"Duplicate column name: key\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS keY\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: keY\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val duplicateErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, vaLue\")\n+        }\n+        assert(duplicateErr.message == s\"Duplicate column name: vaLue\")\n+      }\n+    }\n+  }\n+\n+  test(\"basic statistics for integral type columns\") {\n+    val rdd = sparkContext.parallelize(Seq(\"1\", null, \"2\", \"3\", null)).map { i =>\n+      if (i != null) Row(i.toByte, i.toShort, i.toInt, i.toLong) else Row(i, i, i, i)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Cool, please add some salt to this when you fix (as I don't think mine is perfect anyway :)).\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-20T01:13:46Z",
    "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.{AnalysisException, Row}\n+import org.apache.spark.sql.catalyst.plans.logical.BasicColStats\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    val noColumnError = intercept[AnalysisException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+    assert(noColumnError.message == \"Need to specify the columns to analyze. Usage: \" +\n+      \"ANALYZE TABLE tbl COMPUTE STATISTICS FOR COLUMNS key, value\")\n+\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table (key INT, value STRING)\")\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS k\")\n+      }\n+      assert(invalidColError.message == s\"Invalid column name: k\")\n+\n+      val duplicateColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, key\")\n+      }\n+      assert(duplicateColError.message == s\"Duplicate column name: key\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS keY\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: keY\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        val duplicateErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value, vaLue\")\n+        }\n+        assert(duplicateErr.message == s\"Duplicate column name: vaLue\")\n+      }\n+    }\n+  }\n+\n+  test(\"basic statistics for integral type columns\") {\n+    val rdd = sparkContext.parallelize(Seq(\"1\", null, \"2\", \"3\", null)).map { i =>\n+      if (i != null) Row(i.toByte, i.toShort, i.toInt, i.toLong) else Row(i, i, i, i)"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "This is a `ParseException`, right?\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-20T06:27:17Z",
    "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStats\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    intercept[AnalysisException] {"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "A useless `s`\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-20T06:29:01Z",
    "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStats\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    intercept[AnalysisException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val quotedColumn = \"x.yz\"\n+    val quotedName = s\"`$quotedColumn`\"\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table (abc int, $quotedName string)\")\n+\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key\")\n+      }\n+      assert(invalidColError.message == s\"Invalid column name: key.\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ABC\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: ABC.\")"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "A useless `s`\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-20T06:29:06Z",
    "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStats\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    intercept[AnalysisException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val quotedColumn = \"x.yz\"\n+    val quotedName = s\"`$quotedColumn`\"\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table (abc int, $quotedName string)\")\n+\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key\")\n+      }\n+      assert(invalidColError.message == s\"Invalid column name: key.\")"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Please rename all the `statsSeq` to `expectedColStatsSeq`\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-20T06:34:00Z",
    "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStats\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    intercept[AnalysisException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val quotedColumn = \"x.yz\"\n+    val quotedName = s\"`$quotedColumn`\"\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table (abc int, $quotedName string)\")\n+\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key\")\n+      }\n+      assert(invalidColError.message == s\"Invalid column name: key.\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ABC\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: ABC.\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ${quotedName.toUpperCase}, \" +\n+          s\"ABC, $quotedName\")\n+        val df = sql(s\"SELECT * FROM $table\")\n+        val stats = df.queryExecution.analyzed.collect {\n+          case rel: MetastoreRelation =>\n+            val colStats = rel.catalogTable.stats.get.colStats\n+            // check deduplication\n+            assert(colStats.size == 2)\n+            assert(colStats.contains(quotedColumn))\n+            assert(colStats.contains(\"abc\"))\n+        }\n+        assert(stats.size == 1)\n+      }\n+    }\n+  }\n+\n+  private def getNonNullValues[T](values: Seq[Option[T]]): Seq[T] = {\n+    values.filter(_.isDefined).map(_.get)\n+  }\n+\n+  test(\"column-level statistics for integral type columns\") {\n+    val values = (0 to 5).map { i =>\n+      if (i % 2 == 0) None else Some(i)\n+    }\n+    val data = values.map { i =>\n+      (i.map(_.toByte), i.map(_.toShort), i.map(_.toInt), i.map(_.toLong))\n+    }\n+\n+    val df = data.toDF(\"c1\", \"c2\", \"c3\", \"c4\")\n+    val nonNullValues = getNonNullValues[Int](values)\n+    val statsSeq = df.schema.map { f =>"
  }],
  "prId": 15090
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Please also add an empty string.\n",
    "commit": "734abad045a5378d14489a4e956b7a8e1c95a811",
    "createdAt": "2016-09-20T06:38:27Z",
    "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.sql.{Date, Timestamp}\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.ColumnStats\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.command.AnalyzeColumnCommand\n+import org.apache.spark.sql.types._\n+\n+class StatisticsColumnSuite extends StatisticsTest {\n+  import testImplicits._\n+\n+  test(\"parse analyze column commands\") {\n+    val table = \"table\"\n+    assertAnalyzeCommand(\n+      s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key, value\",\n+      classOf[AnalyzeColumnCommand])\n+\n+    intercept[AnalysisException] {\n+      sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS\")\n+    }\n+  }\n+\n+  test(\"check correctness of columns\") {\n+    val table = \"tbl\"\n+    val quotedColumn = \"x.yz\"\n+    val quotedName = s\"`$quotedColumn`\"\n+    withTable(table) {\n+      sql(s\"CREATE TABLE $table (abc int, $quotedName string)\")\n+\n+      val invalidColError = intercept[AnalysisException] {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS key\")\n+      }\n+      assert(invalidColError.message == s\"Invalid column name: key.\")\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"true\") {\n+        val invalidErr = intercept[AnalysisException] {\n+          sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ABC\")\n+        }\n+        assert(invalidErr.message == s\"Invalid column name: ABC.\")\n+      }\n+\n+      withSQLConf(\"spark.sql.caseSensitive\" -> \"false\") {\n+        sql(s\"ANALYZE TABLE $table COMPUTE STATISTICS FOR COLUMNS ${quotedName.toUpperCase}, \" +\n+          s\"ABC, $quotedName\")\n+        val df = sql(s\"SELECT * FROM $table\")\n+        val stats = df.queryExecution.analyzed.collect {\n+          case rel: MetastoreRelation =>\n+            val colStats = rel.catalogTable.stats.get.colStats\n+            // check deduplication\n+            assert(colStats.size == 2)\n+            assert(colStats.contains(quotedColumn))\n+            assert(colStats.contains(\"abc\"))\n+        }\n+        assert(stats.size == 1)\n+      }\n+    }\n+  }\n+\n+  private def getNonNullValues[T](values: Seq[Option[T]]): Seq[T] = {\n+    values.filter(_.isDefined).map(_.get)\n+  }\n+\n+  test(\"column-level statistics for integral type columns\") {\n+    val values = (0 to 5).map { i =>\n+      if (i % 2 == 0) None else Some(i)\n+    }\n+    val data = values.map { i =>\n+      (i.map(_.toByte), i.map(_.toShort), i.map(_.toInt), i.map(_.toLong))\n+    }\n+\n+    val df = data.toDF(\"c1\", \"c2\", \"c3\", \"c4\")\n+    val nonNullValues = getNonNullValues[Int](values)\n+    val statsSeq = df.schema.map { f =>\n+      val colStats = ColumnStats(\n+        dataType = f.dataType,\n+        numNulls = values.count(_.isEmpty),\n+        max = Some(nonNullValues.max),\n+        min = Some(nonNullValues.min),\n+        ndv = Some(nonNullValues.distinct.length.toLong))\n+      (f.name, colStats)\n+    }\n+    checkColStats(df, statsSeq)\n+  }\n+\n+  test(\"column-level statistics for fractional type columns\") {\n+    val values = (0 to 5).map { i =>\n+      if (i == 0) None else Some(i + i * 0.01d)\n+    }\n+    val data = values.map { i =>\n+      (i.map(_.toFloat), i.map(_.toDouble), i.map(Decimal(_)))\n+    }\n+\n+    val df = data.toDF(\"c1\", \"c2\", \"c3\")\n+    val nonNullValues = getNonNullValues[Double](values)\n+    val statsSeq = df.schema.map { f =>\n+      val colStats = ColumnStats(\n+        dataType = f.dataType,\n+        numNulls = values.count(_.isEmpty),\n+        max = Some(nonNullValues.max),\n+        min = Some(nonNullValues.min),\n+        ndv = Some(nonNullValues.distinct.length.toLong))\n+      (f.name, colStats)\n+    }\n+    checkColStats(df, statsSeq)\n+  }\n+\n+  test(\"column-level statistics for string column\") {\n+    val values = Seq(None, Some(\"a\"), Some(\"bbbb\"), Some(\"cccc\"))"
  }],
  "prId": 15090
}]