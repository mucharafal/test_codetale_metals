[{
  "comments": [{
    "author": {
      "login": "zhzhan"
    },
    "body": "More test case to cover more complicated dataType?\n",
    "commit": "601d2420c8441b3aea1c83ab92b75e2dde43cb31",
    "createdAt": "2014-10-14T03:43:54Z",
    "diffHunk": "@@ -0,0 +1,74 @@\n+/*\r\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\r\n+ * contributor license agreements.  See the NOTICE file distributed with\r\n+ * this work for additional information regarding copyright ownership.\r\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\r\n+ * (the \"License\"); you may not use this file except in compliance with\r\n+ * the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *    http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.spark.sql.hive.orc\r\n+\r\n+import org.apache.spark.{SparkConf, SparkContext}\r\n+import org.scalatest.{BeforeAndAfterAll, FunSuiteLike}\r\n+import org.apache.hadoop.conf.Configuration\r\n+import org.apache.hadoop.fs.Path\r\n+import org.apache.spark.util.Utils\r\n+import org.apache.spark.sql.QueryTest\r\n+import org.apache.spark.sql.hive.test.TestHive\r\n+import org.apache.spark.sql.catalyst.util.getTempFilePath\r\n+import org.apache.spark.sql.hive.test.TestHive._\r\n+\r\n+\r\n+import java.io.File\r\n+\r\n+case class TestRDDEntry(key: Int, value: String)\r\n+\r\n+case class AllDataTypes(\r\n+    stringField: String,\r\n+    intField: Int,\r\n+    longField: Long,\r\n+    floatField: Float,\r\n+    doubleField: Double,\r\n+    shortField: Short,\r\n+    byteField: Byte,\r\n+    booleanField: Boolean)\r\n+\r\n+class OrcQuerySuite extends QueryTest with BeforeAndAfterAll {\r\n+\r\n+  test(\"Read/Write All Types\") {\r\n+    val tempDir = getTempFilePath(\"orcTest\").getCanonicalPath\r\n+    val range = (0 to 255)\r\n+    val data = sparkContext.parallelize(range)\r\n+      .map(x => AllDataTypes(s\"$x\", x, x.toLong, x.toFloat, x.toDouble, x.toShort, x.toByte, x % 2 == 0))\r\n+\r\n+    data.saveAsOrcFile(tempDir)\r\n+\r\n+    checkAnswer(\r\n+      TestHive.orcFile(tempDir),\r\n+      data.toSchemaRDD.collect().toSeq)\r\n+\r\n+    Utils.deleteRecursively(new File(tempDir))\r\n+  }\r\n+\r\n+  test(\"Compression options for writing to a Orcfile\") {\r\n+    val tempDir = getTempFilePath(\"orcTest\").getCanonicalPath\r\n+    val rdd = TestHive.sparkContext.parallelize((1 to 100))\r\n+      .map(i => TestRDDEntry(i, s\"val_$i\"))\r\n+\r\n+    // test default compression codec, now only support zlib\r\n+    rdd.saveAsOrcFile(tempDir)\r\n+    var actualCodec = OrcFileOperator.readMetaData(new Path(tempDir), Some(new Configuration())).getCompression.name\r\n+    assert(actualCodec == \"ZLIB\")\r\n+\r\n+    Utils.deleteRecursively(new File(tempDir))\r\n+  }\r\n+}\r"
  }],
  "prId": 2576
}]