[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I think this should be extended to make sure the hdfs and hbase providers are loaded. I'm not sure what's the `ServiceLoader` behavior when an extension fails to load.\r\n\r\nIt would be good to have that test in core, btw. e.g. have a test provider that throws an exception during initialization and make sure it doesn't affect other providers.",
    "commit": "7b8859281118f37c95f33b859e96a576e9a41b9c",
    "createdAt": "2019-01-18T00:11:05Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.security\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.util.Utils\n+\n+class HiveHadoopDelegationTokenManagerSuite extends SparkFunSuite {\n+  private val hadoopConf = new Configuration()\n+\n+  test(\"default configuration\") {\n+    val manager = new HadoopDelegationTokenManager(new SparkConf(false), hadoopConf, null)\n+    assert(manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"disable hive credential provider\") {\n+    val sparkConf = new SparkConf(false).set(\"spark.security.credentials.hive.enabled\", \"false\")\n+    val manager = new HadoopDelegationTokenManager(sparkConf, hadoopConf, null)\n+    assert(!manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"using deprecated configurations\") {\n+    val sparkConf = new SparkConf(false)\n+      .set(\"spark.yarn.security.credentials.hive.enabled\", \"false\")\n+    val manager = new HadoopDelegationTokenManager(sparkConf, hadoopConf, null)\n+    assert(!manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"SPARK-23209: obtain tokens when Hive classes are not available\") {\n+    // This test needs a custom class loader to hide Hive classes which are in the classpath.\n+    // Because the manager code loads the Hive provider directly instead of using reflection, we\n+    // need to drive the test through the custom class loader so a new copy that cannot find\n+    // Hive classes is loaded.\n+    val currentLoader = Thread.currentThread().getContextClassLoader()\n+    val noHive = new ClassLoader() {\n+      override def loadClass(name: String, resolve: Boolean): Class[_] = {\n+        if (name.startsWith(\"org.apache.hive\") || name.startsWith(\"org.apache.hadoop.hive\")) {\n+          throw new ClassNotFoundException(name)\n+        }\n+\n+        val prefixBlacklist = Seq(\"java\", \"scala\", \"com.sun.\", \"sun.\")\n+        if (prefixBlacklist.exists(name.startsWith(_))) {\n+          return currentLoader.loadClass(name)\n+        }\n+\n+        val found = findLoadedClass(name)\n+        if (found != null) {\n+          return found\n+        }\n+\n+        val classFileName = name.replaceAll(\"\\\\.\", \"/\") + \".class\"\n+        val in = currentLoader.getResourceAsStream(classFileName)\n+        if (in != null) {\n+          val bytes = IOUtils.toByteArray(in)\n+          return defineClass(name, bytes, 0, bytes.length)\n+        }\n+\n+        throw new ClassNotFoundException(name)\n+      }\n+    }\n+\n+    Utils.withContextClassLoader(noHive) {\n+      val test = noHive.loadClass(NoHiveTest.getClass.getName().stripSuffix(\"$\"))\n+      test.getMethod(\"runTest\").invoke(null)\n+    }\n+  }\n+}\n+\n+/** Test code for SPARK-23209 to avoid using too much reflection above. */\n+private object NoHiveTest {\n+\n+  def runTest(): Unit = {\n+    try {\n+      val manager = new HadoopDelegationTokenManager(new SparkConf(), new Configuration(), null)\n+      require(!manager.isProviderLoaded(\"hive\"))",
    "line": 90
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I've implemented and tested this execution path [here](https://github.com/apache/spark/pull/23499/files#diff-da6c1fd6d8b0c7538a3e77a09e06a083R250) but no unit test added. Checking how is it possible to add.\r\n\r\n> I'm not sure what's the ServiceLoader behavior when an extension fails to load.\r\n\r\nWith the mentioned implementation services loaded independently.",
    "commit": "7b8859281118f37c95f33b859e96a576e9a41b9c",
    "createdAt": "2019-01-21T11:23:09Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.security\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.util.Utils\n+\n+class HiveHadoopDelegationTokenManagerSuite extends SparkFunSuite {\n+  private val hadoopConf = new Configuration()\n+\n+  test(\"default configuration\") {\n+    val manager = new HadoopDelegationTokenManager(new SparkConf(false), hadoopConf, null)\n+    assert(manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"disable hive credential provider\") {\n+    val sparkConf = new SparkConf(false).set(\"spark.security.credentials.hive.enabled\", \"false\")\n+    val manager = new HadoopDelegationTokenManager(sparkConf, hadoopConf, null)\n+    assert(!manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"using deprecated configurations\") {\n+    val sparkConf = new SparkConf(false)\n+      .set(\"spark.yarn.security.credentials.hive.enabled\", \"false\")\n+    val manager = new HadoopDelegationTokenManager(sparkConf, hadoopConf, null)\n+    assert(!manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"SPARK-23209: obtain tokens when Hive classes are not available\") {\n+    // This test needs a custom class loader to hide Hive classes which are in the classpath.\n+    // Because the manager code loads the Hive provider directly instead of using reflection, we\n+    // need to drive the test through the custom class loader so a new copy that cannot find\n+    // Hive classes is loaded.\n+    val currentLoader = Thread.currentThread().getContextClassLoader()\n+    val noHive = new ClassLoader() {\n+      override def loadClass(name: String, resolve: Boolean): Class[_] = {\n+        if (name.startsWith(\"org.apache.hive\") || name.startsWith(\"org.apache.hadoop.hive\")) {\n+          throw new ClassNotFoundException(name)\n+        }\n+\n+        val prefixBlacklist = Seq(\"java\", \"scala\", \"com.sun.\", \"sun.\")\n+        if (prefixBlacklist.exists(name.startsWith(_))) {\n+          return currentLoader.loadClass(name)\n+        }\n+\n+        val found = findLoadedClass(name)\n+        if (found != null) {\n+          return found\n+        }\n+\n+        val classFileName = name.replaceAll(\"\\\\.\", \"/\") + \".class\"\n+        val in = currentLoader.getResourceAsStream(classFileName)\n+        if (in != null) {\n+          val bytes = IOUtils.toByteArray(in)\n+          return defineClass(name, bytes, 0, bytes.length)\n+        }\n+\n+        throw new ClassNotFoundException(name)\n+      }\n+    }\n+\n+    Utils.withContextClassLoader(noHive) {\n+      val test = noHive.loadClass(NoHiveTest.getClass.getName().stripSuffix(\"$\"))\n+      test.getMethod(\"runTest\").invoke(null)\n+    }\n+  }\n+}\n+\n+/** Test code for SPARK-23209 to avoid using too much reflection above. */\n+private object NoHiveTest {\n+\n+  def runTest(): Unit = {\n+    try {\n+      val manager = new HadoopDelegationTokenManager(new SparkConf(), new Configuration(), null)\n+      require(!manager.isProviderLoaded(\"hive\"))",
    "line": 90
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "> I think this should be extended to make sure the hdfs and hbase providers are loaded.\r\n\r\nI was also thinking about to add them but was hesitant to introduce inter-project tests. The more I'm considering the more I think you're right. Added.\r\n\r\n> It would be good to have that test in core, btw. e.g. have a test provider that throws an exception during initialization and make sure it doesn't affect other providers.\r\n\r\nAdded.",
    "commit": "7b8859281118f37c95f33b859e96a576e9a41b9c",
    "createdAt": "2019-01-21T13:52:39Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.security\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.util.Utils\n+\n+class HiveHadoopDelegationTokenManagerSuite extends SparkFunSuite {\n+  private val hadoopConf = new Configuration()\n+\n+  test(\"default configuration\") {\n+    val manager = new HadoopDelegationTokenManager(new SparkConf(false), hadoopConf, null)\n+    assert(manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"disable hive credential provider\") {\n+    val sparkConf = new SparkConf(false).set(\"spark.security.credentials.hive.enabled\", \"false\")\n+    val manager = new HadoopDelegationTokenManager(sparkConf, hadoopConf, null)\n+    assert(!manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"using deprecated configurations\") {\n+    val sparkConf = new SparkConf(false)\n+      .set(\"spark.yarn.security.credentials.hive.enabled\", \"false\")\n+    val manager = new HadoopDelegationTokenManager(sparkConf, hadoopConf, null)\n+    assert(!manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"SPARK-23209: obtain tokens when Hive classes are not available\") {\n+    // This test needs a custom class loader to hide Hive classes which are in the classpath.\n+    // Because the manager code loads the Hive provider directly instead of using reflection, we\n+    // need to drive the test through the custom class loader so a new copy that cannot find\n+    // Hive classes is loaded.\n+    val currentLoader = Thread.currentThread().getContextClassLoader()\n+    val noHive = new ClassLoader() {\n+      override def loadClass(name: String, resolve: Boolean): Class[_] = {\n+        if (name.startsWith(\"org.apache.hive\") || name.startsWith(\"org.apache.hadoop.hive\")) {\n+          throw new ClassNotFoundException(name)\n+        }\n+\n+        val prefixBlacklist = Seq(\"java\", \"scala\", \"com.sun.\", \"sun.\")\n+        if (prefixBlacklist.exists(name.startsWith(_))) {\n+          return currentLoader.loadClass(name)\n+        }\n+\n+        val found = findLoadedClass(name)\n+        if (found != null) {\n+          return found\n+        }\n+\n+        val classFileName = name.replaceAll(\"\\\\.\", \"/\") + \".class\"\n+        val in = currentLoader.getResourceAsStream(classFileName)\n+        if (in != null) {\n+          val bytes = IOUtils.toByteArray(in)\n+          return defineClass(name, bytes, 0, bytes.length)\n+        }\n+\n+        throw new ClassNotFoundException(name)\n+      }\n+    }\n+\n+    Utils.withContextClassLoader(noHive) {\n+      val test = noHive.loadClass(NoHiveTest.getClass.getName().stripSuffix(\"$\"))\n+      test.getMethod(\"runTest\").invoke(null)\n+    }\n+  }\n+}\n+\n+/** Test code for SPARK-23209 to avoid using too much reflection above. */\n+private object NoHiveTest {\n+\n+  def runTest(): Unit = {\n+    try {\n+      val manager = new HadoopDelegationTokenManager(new SparkConf(), new Configuration(), null)\n+      require(!manager.isProviderLoaded(\"hive\"))",
    "line": 90
  }],
  "prId": 23499
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This (and the next) test is more a test of the main manager code. It's not really testing anything specific to the Hive provider.",
    "commit": "7b8859281118f37c95f33b859e96a576e9a41b9c",
    "createdAt": "2019-01-23T20:16:02Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.security\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.util.Utils\n+\n+class HiveHadoopDelegationTokenManagerSuite extends SparkFunSuite {\n+  private val hadoopConf = new Configuration()\n+\n+  test(\"default configuration\") {\n+    val manager = new HadoopDelegationTokenManager(new SparkConf(false), hadoopConf, null)\n+    assert(manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"disable hive credential provider\") {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, will move it to `HadoopDelegationTokenManagerSuite` and convert it to `hadoopfs` because it's a generic feature.",
    "commit": "7b8859281118f37c95f33b859e96a576e9a41b9c",
    "createdAt": "2019-01-24T15:39:09Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.security\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.util.Utils\n+\n+class HiveHadoopDelegationTokenManagerSuite extends SparkFunSuite {\n+  private val hadoopConf = new Configuration()\n+\n+  test(\"default configuration\") {\n+    val manager = new HadoopDelegationTokenManager(new SparkConf(false), hadoopConf, null)\n+    assert(manager.isProviderLoaded(\"hive\"))\n+  }\n+\n+  test(\"disable hive credential provider\") {"
  }],
  "prId": 23499
}]