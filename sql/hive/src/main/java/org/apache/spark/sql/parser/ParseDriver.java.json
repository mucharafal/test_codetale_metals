[{
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "The lexer only checks the `HiveConf` to check if it supports quoted ids. We could move this to our own configuration.\n",
    "commit": "7e1a14582fc32fda2016072138b4a431c7ba9333",
    "createdAt": "2015-12-27T21:47:47Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.parser;\n+\n+import java.util.ArrayList;\n+import org.antlr.runtime.ANTLRStringStream;\n+import org.antlr.runtime.CharStream;\n+import org.antlr.runtime.NoViableAltException;\n+import org.antlr.runtime.RecognitionException;\n+import org.antlr.runtime.Token;\n+import org.antlr.runtime.TokenRewriteStream;\n+import org.antlr.runtime.TokenStream;\n+import org.antlr.runtime.tree.CommonTree;\n+import org.antlr.runtime.tree.CommonTreeAdaptor;\n+import org.antlr.runtime.tree.TreeAdaptor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hadoop.hive.ql.Context;\n+\n+/**\n+ * ParseDriver.\n+ *\n+ */\n+public class ParseDriver {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\"hive.ql.parse.ParseDriver\");\n+\n+  /**\n+   * ANTLRNoCaseStringStream.\n+   *\n+   */\n+  //This class provides and implementation for a case insensitive token checker\n+  //for the lexical analysis part of antlr. By converting the token stream into\n+  //upper case at the time when lexical rules are checked, this class ensures that the\n+  //lexical rules need to just match the token with upper case letters as opposed to\n+  //combination of upper case and lower case characteres. This is purely used for matching lexical\n+  //rules. The actual token text is stored in the same way as the user input without\n+  //actually converting it into an upper case. The token values are generated by the consume()\n+  //function of the super class ANTLRStringStream. The LA() function is the lookahead funtion\n+  //and is purely used for matching lexical rules. This also means that the grammar will only\n+  //accept capitalized tokens in case it is run from other tools like antlrworks which\n+  //do not have the ANTLRNoCaseStringStream implementation.\n+  public class ANTLRNoCaseStringStream extends ANTLRStringStream {\n+\n+    public ANTLRNoCaseStringStream(String input) {\n+      super(input);\n+    }\n+\n+    @Override\n+    public int LA(int i) {\n+\n+      int returnChar = super.LA(i);\n+      if (returnChar == CharStream.EOF) {\n+        return returnChar;\n+      } else if (returnChar == 0) {\n+        return returnChar;\n+      }\n+\n+      return Character.toUpperCase((char) returnChar);\n+    }\n+  }\n+\n+  /**\n+   * HiveLexerX.\n+   *\n+   */\n+  public class HiveLexerX extends SparkSqlLexer {\n+\n+    private final ArrayList<ParseError> errors;\n+\n+    public HiveLexerX() {\n+      super();\n+      errors = new ArrayList<ParseError>();\n+    }\n+\n+    public HiveLexerX(CharStream input) {\n+      super(input);\n+      errors = new ArrayList<ParseError>();\n+    }\n+\n+    @Override\n+    public void displayRecognitionError(String[] tokenNames,\n+        RecognitionException e) {\n+\n+      errors.add(new ParseError(this, e, tokenNames));\n+    }\n+\n+    @Override\n+    public String getErrorMessage(RecognitionException e, String[] tokenNames) {\n+      String msg = null;\n+\n+      if (e instanceof NoViableAltException) {\n+        @SuppressWarnings(\"unused\")\n+        NoViableAltException nvae = (NoViableAltException) e;\n+        // for development, can add\n+        // \"decision=<<\"+nvae.grammarDecisionDescription+\">>\"\n+        // and \"(decision=\"+nvae.decisionNumber+\") and\n+        // \"state \"+nvae.stateNumber\n+        msg = \"character \" + getCharErrorDisplay(e.c) + \" not supported here\";\n+      } else {\n+        msg = super.getErrorMessage(e, tokenNames);\n+      }\n+\n+      return msg;\n+    }\n+\n+    public ArrayList<ParseError> getErrors() {\n+      return errors;\n+    }\n+\n+  }\n+\n+  /**\n+   * Tree adaptor for making antlr return ASTNodes instead of CommonTree nodes\n+   * so that the graph walking algorithms and the rules framework defined in\n+   * ql.lib can be used with the AST Nodes.\n+   */\n+  public static final TreeAdaptor adaptor = new CommonTreeAdaptor() {\n+    /**\n+     * Creates an ASTNode for the given token. The ASTNode is a wrapper around\n+     * antlr's CommonTree class that implements the Node interface.\n+     *\n+     * @param payload\n+     *          The token.\n+     * @return Object (which is actually an ASTNode) for the token.\n+     */\n+    @Override\n+    public Object create(Token payload) {\n+      return new ASTNode(payload);\n+    }\n+\n+    @Override\n+    public Object dupNode(Object t) {\n+\n+      return create(((CommonTree)t).token);\n+    };\n+\n+    @Override\n+    public Object errorNode(TokenStream input, Token start, Token stop, RecognitionException e) {\n+      return new ASTErrorNode(input, start, stop, e);\n+    };\n+  };\n+\n+  public ASTNode parse(String command) throws ParseException {\n+    return parse(command, null);\n+  }\n+  \n+  public ASTNode parse(String command, Context ctx) \n+      throws ParseException {\n+    return parse(command, ctx, true);\n+  }\n+\n+  /**\n+   * Parses a command, optionally assigning the parser's token stream to the\n+   * given context.\n+   *\n+   * @param command\n+   *          command to parse\n+   *\n+   * @param ctx\n+   *          context with which to associate this parser's token stream, or\n+   *          null if either no context is available or the context already has\n+   *          an existing stream\n+   *\n+   * @return parsed AST\n+   */\n+  public ASTNode parse(String command, Context ctx, boolean setTokenRewriteStream) \n+      throws ParseException {\n+    LOG.info(\"Parsing command: \" + command);\n+\n+    HiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));\n+    TokenRewriteStream tokens = new TokenRewriteStream(lexer);\n+    if (ctx != null) {\n+      if ( setTokenRewriteStream) {\n+        ctx.setTokenRewriteStream(tokens);\n+      }\n+      lexer.setHiveConf(ctx.getConf());",
    "line": 193
  }],
  "prId": 10420
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "The parser only checks the `HiveConf` to check if we can use SQL11 as identifiers. We could move this to our own configuration.\n",
    "commit": "7e1a14582fc32fda2016072138b4a431c7ba9333",
    "createdAt": "2015-12-27T21:49:05Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.parser;\n+\n+import java.util.ArrayList;\n+import org.antlr.runtime.ANTLRStringStream;\n+import org.antlr.runtime.CharStream;\n+import org.antlr.runtime.NoViableAltException;\n+import org.antlr.runtime.RecognitionException;\n+import org.antlr.runtime.Token;\n+import org.antlr.runtime.TokenRewriteStream;\n+import org.antlr.runtime.TokenStream;\n+import org.antlr.runtime.tree.CommonTree;\n+import org.antlr.runtime.tree.CommonTreeAdaptor;\n+import org.antlr.runtime.tree.TreeAdaptor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hadoop.hive.ql.Context;\n+\n+/**\n+ * ParseDriver.\n+ *\n+ */\n+public class ParseDriver {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\"hive.ql.parse.ParseDriver\");\n+\n+  /**\n+   * ANTLRNoCaseStringStream.\n+   *\n+   */\n+  //This class provides and implementation for a case insensitive token checker\n+  //for the lexical analysis part of antlr. By converting the token stream into\n+  //upper case at the time when lexical rules are checked, this class ensures that the\n+  //lexical rules need to just match the token with upper case letters as opposed to\n+  //combination of upper case and lower case characteres. This is purely used for matching lexical\n+  //rules. The actual token text is stored in the same way as the user input without\n+  //actually converting it into an upper case. The token values are generated by the consume()\n+  //function of the super class ANTLRStringStream. The LA() function is the lookahead funtion\n+  //and is purely used for matching lexical rules. This also means that the grammar will only\n+  //accept capitalized tokens in case it is run from other tools like antlrworks which\n+  //do not have the ANTLRNoCaseStringStream implementation.\n+  public class ANTLRNoCaseStringStream extends ANTLRStringStream {\n+\n+    public ANTLRNoCaseStringStream(String input) {\n+      super(input);\n+    }\n+\n+    @Override\n+    public int LA(int i) {\n+\n+      int returnChar = super.LA(i);\n+      if (returnChar == CharStream.EOF) {\n+        return returnChar;\n+      } else if (returnChar == 0) {\n+        return returnChar;\n+      }\n+\n+      return Character.toUpperCase((char) returnChar);\n+    }\n+  }\n+\n+  /**\n+   * HiveLexerX.\n+   *\n+   */\n+  public class HiveLexerX extends SparkSqlLexer {\n+\n+    private final ArrayList<ParseError> errors;\n+\n+    public HiveLexerX() {\n+      super();\n+      errors = new ArrayList<ParseError>();\n+    }\n+\n+    public HiveLexerX(CharStream input) {\n+      super(input);\n+      errors = new ArrayList<ParseError>();\n+    }\n+\n+    @Override\n+    public void displayRecognitionError(String[] tokenNames,\n+        RecognitionException e) {\n+\n+      errors.add(new ParseError(this, e, tokenNames));\n+    }\n+\n+    @Override\n+    public String getErrorMessage(RecognitionException e, String[] tokenNames) {\n+      String msg = null;\n+\n+      if (e instanceof NoViableAltException) {\n+        @SuppressWarnings(\"unused\")\n+        NoViableAltException nvae = (NoViableAltException) e;\n+        // for development, can add\n+        // \"decision=<<\"+nvae.grammarDecisionDescription+\">>\"\n+        // and \"(decision=\"+nvae.decisionNumber+\") and\n+        // \"state \"+nvae.stateNumber\n+        msg = \"character \" + getCharErrorDisplay(e.c) + \" not supported here\";\n+      } else {\n+        msg = super.getErrorMessage(e, tokenNames);\n+      }\n+\n+      return msg;\n+    }\n+\n+    public ArrayList<ParseError> getErrors() {\n+      return errors;\n+    }\n+\n+  }\n+\n+  /**\n+   * Tree adaptor for making antlr return ASTNodes instead of CommonTree nodes\n+   * so that the graph walking algorithms and the rules framework defined in\n+   * ql.lib can be used with the AST Nodes.\n+   */\n+  public static final TreeAdaptor adaptor = new CommonTreeAdaptor() {\n+    /**\n+     * Creates an ASTNode for the given token. The ASTNode is a wrapper around\n+     * antlr's CommonTree class that implements the Node interface.\n+     *\n+     * @param payload\n+     *          The token.\n+     * @return Object (which is actually an ASTNode) for the token.\n+     */\n+    @Override\n+    public Object create(Token payload) {\n+      return new ASTNode(payload);\n+    }\n+\n+    @Override\n+    public Object dupNode(Object t) {\n+\n+      return create(((CommonTree)t).token);\n+    };\n+\n+    @Override\n+    public Object errorNode(TokenStream input, Token start, Token stop, RecognitionException e) {\n+      return new ASTErrorNode(input, start, stop, e);\n+    };\n+  };\n+\n+  public ASTNode parse(String command) throws ParseException {\n+    return parse(command, null);\n+  }\n+  \n+  public ASTNode parse(String command, Context ctx) \n+      throws ParseException {\n+    return parse(command, ctx, true);\n+  }\n+\n+  /**\n+   * Parses a command, optionally assigning the parser's token stream to the\n+   * given context.\n+   *\n+   * @param command\n+   *          command to parse\n+   *\n+   * @param ctx\n+   *          context with which to associate this parser's token stream, or\n+   *          null if either no context is available or the context already has\n+   *          an existing stream\n+   *\n+   * @return parsed AST\n+   */\n+  public ASTNode parse(String command, Context ctx, boolean setTokenRewriteStream) \n+      throws ParseException {\n+    LOG.info(\"Parsing command: \" + command);\n+\n+    HiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));\n+    TokenRewriteStream tokens = new TokenRewriteStream(lexer);\n+    if (ctx != null) {\n+      if ( setTokenRewriteStream) {\n+        ctx.setTokenRewriteStream(tokens);\n+      }\n+      lexer.setHiveConf(ctx.getConf());\n+    }\n+    SparkSqlParser parser = new SparkSqlParser(tokens);\n+    if (ctx != null) {\n+      parser.setHiveConf(ctx.getConf());",
    "line": 197
  }],
  "prId": 10420
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "The `Context.tokenRewriteStream` is used in HiveQL. If we factor out this and the `HiveQL` (see comments below) we can drop the Context entirely.\n",
    "commit": "7e1a14582fc32fda2016072138b4a431c7ba9333",
    "createdAt": "2015-12-27T21:52:38Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.parser;\n+\n+import java.util.ArrayList;\n+import org.antlr.runtime.ANTLRStringStream;\n+import org.antlr.runtime.CharStream;\n+import org.antlr.runtime.NoViableAltException;\n+import org.antlr.runtime.RecognitionException;\n+import org.antlr.runtime.Token;\n+import org.antlr.runtime.TokenRewriteStream;\n+import org.antlr.runtime.TokenStream;\n+import org.antlr.runtime.tree.CommonTree;\n+import org.antlr.runtime.tree.CommonTreeAdaptor;\n+import org.antlr.runtime.tree.TreeAdaptor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hadoop.hive.ql.Context;\n+\n+/**\n+ * ParseDriver.\n+ *\n+ */\n+public class ParseDriver {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\"hive.ql.parse.ParseDriver\");\n+\n+  /**\n+   * ANTLRNoCaseStringStream.\n+   *\n+   */\n+  //This class provides and implementation for a case insensitive token checker\n+  //for the lexical analysis part of antlr. By converting the token stream into\n+  //upper case at the time when lexical rules are checked, this class ensures that the\n+  //lexical rules need to just match the token with upper case letters as opposed to\n+  //combination of upper case and lower case characteres. This is purely used for matching lexical\n+  //rules. The actual token text is stored in the same way as the user input without\n+  //actually converting it into an upper case. The token values are generated by the consume()\n+  //function of the super class ANTLRStringStream. The LA() function is the lookahead funtion\n+  //and is purely used for matching lexical rules. This also means that the grammar will only\n+  //accept capitalized tokens in case it is run from other tools like antlrworks which\n+  //do not have the ANTLRNoCaseStringStream implementation.\n+  public class ANTLRNoCaseStringStream extends ANTLRStringStream {\n+\n+    public ANTLRNoCaseStringStream(String input) {\n+      super(input);\n+    }\n+\n+    @Override\n+    public int LA(int i) {\n+\n+      int returnChar = super.LA(i);\n+      if (returnChar == CharStream.EOF) {\n+        return returnChar;\n+      } else if (returnChar == 0) {\n+        return returnChar;\n+      }\n+\n+      return Character.toUpperCase((char) returnChar);\n+    }\n+  }\n+\n+  /**\n+   * HiveLexerX.\n+   *\n+   */\n+  public class HiveLexerX extends SparkSqlLexer {\n+\n+    private final ArrayList<ParseError> errors;\n+\n+    public HiveLexerX() {\n+      super();\n+      errors = new ArrayList<ParseError>();\n+    }\n+\n+    public HiveLexerX(CharStream input) {\n+      super(input);\n+      errors = new ArrayList<ParseError>();\n+    }\n+\n+    @Override\n+    public void displayRecognitionError(String[] tokenNames,\n+        RecognitionException e) {\n+\n+      errors.add(new ParseError(this, e, tokenNames));\n+    }\n+\n+    @Override\n+    public String getErrorMessage(RecognitionException e, String[] tokenNames) {\n+      String msg = null;\n+\n+      if (e instanceof NoViableAltException) {\n+        @SuppressWarnings(\"unused\")\n+        NoViableAltException nvae = (NoViableAltException) e;\n+        // for development, can add\n+        // \"decision=<<\"+nvae.grammarDecisionDescription+\">>\"\n+        // and \"(decision=\"+nvae.decisionNumber+\") and\n+        // \"state \"+nvae.stateNumber\n+        msg = \"character \" + getCharErrorDisplay(e.c) + \" not supported here\";\n+      } else {\n+        msg = super.getErrorMessage(e, tokenNames);\n+      }\n+\n+      return msg;\n+    }\n+\n+    public ArrayList<ParseError> getErrors() {\n+      return errors;\n+    }\n+\n+  }\n+\n+  /**\n+   * Tree adaptor for making antlr return ASTNodes instead of CommonTree nodes\n+   * so that the graph walking algorithms and the rules framework defined in\n+   * ql.lib can be used with the AST Nodes.\n+   */\n+  public static final TreeAdaptor adaptor = new CommonTreeAdaptor() {\n+    /**\n+     * Creates an ASTNode for the given token. The ASTNode is a wrapper around\n+     * antlr's CommonTree class that implements the Node interface.\n+     *\n+     * @param payload\n+     *          The token.\n+     * @return Object (which is actually an ASTNode) for the token.\n+     */\n+    @Override\n+    public Object create(Token payload) {\n+      return new ASTNode(payload);\n+    }\n+\n+    @Override\n+    public Object dupNode(Object t) {\n+\n+      return create(((CommonTree)t).token);\n+    };\n+\n+    @Override\n+    public Object errorNode(TokenStream input, Token start, Token stop, RecognitionException e) {\n+      return new ASTErrorNode(input, start, stop, e);\n+    };\n+  };\n+\n+  public ASTNode parse(String command) throws ParseException {\n+    return parse(command, null);\n+  }\n+  \n+  public ASTNode parse(String command, Context ctx) \n+      throws ParseException {\n+    return parse(command, ctx, true);\n+  }\n+\n+  /**\n+   * Parses a command, optionally assigning the parser's token stream to the\n+   * given context.\n+   *\n+   * @param command\n+   *          command to parse\n+   *\n+   * @param ctx\n+   *          context with which to associate this parser's token stream, or\n+   *          null if either no context is available or the context already has\n+   *          an existing stream\n+   *\n+   * @return parsed AST\n+   */\n+  public ASTNode parse(String command, Context ctx, boolean setTokenRewriteStream) \n+      throws ParseException {\n+    LOG.info(\"Parsing command: \" + command);\n+\n+    HiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));\n+    TokenRewriteStream tokens = new TokenRewriteStream(lexer);\n+    if (ctx != null) {\n+      if ( setTokenRewriteStream) {\n+        ctx.setTokenRewriteStream(tokens);",
    "line": 191
  }],
  "prId": 10420
}]