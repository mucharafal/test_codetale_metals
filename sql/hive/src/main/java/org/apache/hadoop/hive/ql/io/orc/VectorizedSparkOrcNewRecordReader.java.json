[{
  "comments": [{
    "author": {
      "login": "dafrista"
    },
    "body": "Can creation of a String be avoided by using `UTF8String.fromBytes`? My understanding is that the encode/decode in `new String(..)` and `UTF8String.fromString` can add up.\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-06-22T21:07:00Z",
    "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = columnIDs;\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch && progress < 1.0f) {\n+      if (reader.next(NullWritable.get(), internalValue)) {\n+        if (internalValue.endOfFile) {\n+          progress = 1.0f;\n+          numRowsOfBatch = 0;\n+          indexOfRow = 0;\n+          return false;\n+        } else {\n+          assert internalValue.numCols == numColumns : \"Incorrect number of columns in OrcBatch\";\n+          numRowsOfBatch = internalValue.count();\n+          indexOfRow = 0;\n+          progress = reader.getProgress();\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    } else {\n+      if (indexOfRow < numRowsOfBatch) {\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Adapter class to return an internal row.\n+   */\n+  public static final class Row extends InternalRow {\n+    protected int rowId;\n+    private List<Integer> columnIDs;\n+    private final ColumnVector[] columns;\n+\n+    private Row(ColumnVector[] columns, List<Integer> columnIDs) {\n+      this.columns = columns;\n+      this.columnIDs = columnIDs;\n+    }\n+\n+    @Override\n+    public int numFields() { return columnIDs.size(); }\n+\n+    @Override\n+    public boolean anyNull() {\n+      for (int i = 0; i < columns.length; i++) {\n+        if (columnIDs.contains(i)) {\n+          if (columns[i].isRepeating && columns[i].isNull[0]) {\n+            return true;\n+          } else if (!columns[i].isRepeating && columns[i].isNull[rowId]) {\n+            return true;\n+          }\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public boolean isNullAt(int ordinal) {\n+      ColumnVector col = columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return col.isNull[0];\n+      } else {\n+        return col.isNull[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return col.vector[0] > 0;\n+      } else {\n+        return col.vector[rowId] > 0;\n+      }\n+    }\n+\n+    @Override\n+    public byte getByte(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (byte)col.vector[0];\n+      } else {\n+        return (byte)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public short getShort(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (short)col.vector[0];\n+      } else {\n+        return (short)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public int getInt(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (int)col.vector[0];\n+      } else {\n+        return (int)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public long getLong(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (long)col.vector[0];\n+      } else {\n+        return (long)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public float getFloat(int ordinal) {\n+      DoubleColumnVector col = (DoubleColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (float)col.vector[0];\n+      } else {\n+        return (float)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public double getDouble(int ordinal) {\n+      DoubleColumnVector col = (DoubleColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (double)col.vector[0];\n+      } else {\n+        return (double)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public Decimal getDecimal(int ordinal, int precision, int scale) {\n+      DecimalColumnVector col = (DecimalColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return Decimal.apply(col.vector[0].getHiveDecimal().bigDecimalValue(), precision, scale);\n+      } else {\n+        return Decimal.apply(col.vector[rowId].getHiveDecimal().bigDecimalValue(),\n+          precision, scale);\n+      }\n+    }\n+\n+    @Override\n+    public UTF8String getUTF8String(int ordinal) {\n+      BytesColumnVector bv = ((BytesColumnVector)columns[columnIDs.get(ordinal)]);\n+      String str = null;\n+      if (bv.isRepeating) {\n+        str = new String(bv.vector[0], bv.start[0], bv.length[0], StandardCharsets.UTF_8);"
  }],
  "prId": 13775
}, {
  "comments": [{
    "author": {
      "login": "dafrista"
    },
    "body": "I've done some profiling of this code (repeated jstack traces on a job that scans the table), and was surprised to see this call taking around 30% of CPU time. Each time .size() is called, the scala sequence to java list conversion has to take place. Other calls that involve columnIDs also took about 9% of CPU time.\n\nI made a change to avoid this conversion, and saw the CPU time spent in these calls reduced to effectively zero. The change was to make a (java) copy of the columnIDs in the constructor of VectorizedSparkOrcNewRecordReader. I replaced `this.columnIDs = columnIDs` at L75 with `this.columnIDs = new ArrayList<>(columnIDs)` and `columnIDs` at L78 with `this.columnIDs`.\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-08-10T03:31:23Z",
    "diffHunk": "@@ -0,0 +1,317 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = columnIDs;\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch && progress < 1.0f) {\n+      if (reader.next(NullWritable.get(), internalValue)) {\n+        if (internalValue.endOfFile) {\n+          progress = 1.0f;\n+          numRowsOfBatch = 0;\n+          indexOfRow = 0;\n+          return false;\n+        } else {\n+          assert internalValue.numCols == numColumns : \"Incorrect number of columns in OrcBatch\";\n+          numRowsOfBatch = internalValue.count();\n+          indexOfRow = 0;\n+          progress = reader.getProgress();\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    } else {\n+      if (indexOfRow < numRowsOfBatch) {\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Adapter class to return an internal row.\n+   */\n+  public static final class Row extends InternalRow {\n+    protected int rowId;\n+    private List<Integer> columnIDs;\n+    private final ColumnVector[] columns;\n+\n+    private Row(ColumnVector[] columns, List<Integer> columnIDs) {\n+      this.columns = columns;\n+      this.columnIDs = columnIDs;\n+    }\n+\n+    @Override\n+    public int numFields() { return columnIDs.size(); }"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Good catch! Thanks.\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-08-10T04:16:28Z",
    "diffHunk": "@@ -0,0 +1,317 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = columnIDs;\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch && progress < 1.0f) {\n+      if (reader.next(NullWritable.get(), internalValue)) {\n+        if (internalValue.endOfFile) {\n+          progress = 1.0f;\n+          numRowsOfBatch = 0;\n+          indexOfRow = 0;\n+          return false;\n+        } else {\n+          assert internalValue.numCols == numColumns : \"Incorrect number of columns in OrcBatch\";\n+          numRowsOfBatch = internalValue.count();\n+          indexOfRow = 0;\n+          progress = reader.getProgress();\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    } else {\n+      if (indexOfRow < numRowsOfBatch) {\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Adapter class to return an internal row.\n+   */\n+  public static final class Row extends InternalRow {\n+    protected int rowId;\n+    private List<Integer> columnIDs;\n+    private final ColumnVector[] columns;\n+\n+    private Row(ColumnVector[] columns, List<Integer> columnIDs) {\n+      this.columns = columns;\n+      this.columnIDs = columnIDs;\n+    }\n+\n+    @Override\n+    public int numFields() { return columnIDs.size(); }"
  }],
  "prId": 13775
}, {
  "comments": [{
    "author": {
      "login": "dafrista"
    },
    "body": "We can't rely on `progress` to indicate the last row -- `< 1.0f` can evaluate to true even before the last row because of floating point imprecision. Anyway, this part of the check is redundant, `reader.next` on the next line will return false when no more rows remain.\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-08-11T00:37:07Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch && progress < 1.0f) {"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "reader.next will not return false if no more batch. An exception will be thrown then.\n\n```\n[info]   Cause: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1\n[info]   at java.util.ArrayList.rangeCheck(ArrayList.java:653)\n[info]   at java.util.ArrayList.get(ArrayList.java:429)\n[info]   at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.beginReadStripe(RecordReaderImpl.java:804)\n[info]   at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:776)\n[info]   at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1056)\n...\n```\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-08-11T04:35:12Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch && progress < 1.0f) {"
  }, {
    "author": {
      "login": "dafrista"
    },
    "body": "Ah, I see. The new commit should work ðŸ‘ \n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-08-11T05:02:27Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch && progress < 1.0f) {"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "oh. We can use reader.hasNext().\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-08-11T05:17:21Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch && progress < 1.0f) {"
  }],
  "prId": 13775
}, {
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "nit: move all the `InterruptedException` in the previous line. This might also apply to other places in the PR but I am not pointing out each instance.\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-10-17T23:41:01Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {"
  }],
  "prId": 13775
}, {
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "This if-else is double fetching `columns[i].isRepeating`. You could either save to a var OR add one more level of branching.\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-10-17T23:59:15Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch) {\n+      if (reader.next(NullWritable.get(), internalValue)) {\n+        if (internalValue.endOfFile) {\n+          progress = 1.0f;\n+          numRowsOfBatch = 0;\n+          indexOfRow = 0;\n+          return false;\n+        } else {\n+          assert internalValue.numCols == numColumns : \"Incorrect number of columns in OrcBatch\";\n+          numRowsOfBatch = internalValue.count();\n+          indexOfRow = 0;\n+          progress = reader.getProgress();\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    } else {\n+      if (indexOfRow < numRowsOfBatch) {\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Adapter class to return an internal row.\n+   */\n+  public static final class Row extends InternalRow {\n+    protected int rowId;\n+    private List<Integer> columnIDs;\n+    private final ColumnVector[] columns;\n+\n+    private Row(ColumnVector[] columns, List<Integer> columnIDs) {\n+      this.columns = columns;\n+      this.columnIDs = columnIDs;\n+    }\n+\n+    @Override\n+    public int numFields() { return columnIDs.size(); }\n+\n+    @Override\n+    public boolean anyNull() {\n+      for (int i = 0; i < columns.length; i++) {\n+        if (columnIDs.contains(i)) {\n+          if (columns[i].isRepeating && columns[i].isNull[0]) {\n+            return true;\n+          } else if (!columns[i].isRepeating && columns[i].isNull[rowId]) {"
  }],
  "prId": 13775
}, {
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "I see this pattern here and even below: The operation being done is inherently the same ... its just the index which changes in the if-else blocks. You could separate the operation being done from the index gathering part.\n\n```\nint index = col.isRepeating ? 0 : rowId;\nreturn col.isNull[index];\n```\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-10-18T00:03:44Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch) {\n+      if (reader.next(NullWritable.get(), internalValue)) {\n+        if (internalValue.endOfFile) {\n+          progress = 1.0f;\n+          numRowsOfBatch = 0;\n+          indexOfRow = 0;\n+          return false;\n+        } else {\n+          assert internalValue.numCols == numColumns : \"Incorrect number of columns in OrcBatch\";\n+          numRowsOfBatch = internalValue.count();\n+          indexOfRow = 0;\n+          progress = reader.getProgress();\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    } else {\n+      if (indexOfRow < numRowsOfBatch) {\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Adapter class to return an internal row.\n+   */\n+  public static final class Row extends InternalRow {\n+    protected int rowId;\n+    private List<Integer> columnIDs;\n+    private final ColumnVector[] columns;\n+\n+    private Row(ColumnVector[] columns, List<Integer> columnIDs) {\n+      this.columns = columns;\n+      this.columnIDs = columnIDs;\n+    }\n+\n+    @Override\n+    public int numFields() { return columnIDs.size(); }\n+\n+    @Override\n+    public boolean anyNull() {\n+      for (int i = 0; i < columns.length; i++) {\n+        if (columnIDs.contains(i)) {\n+          if (columns[i].isRepeating && columns[i].isNull[0]) {\n+            return true;\n+          } else if (!columns[i].isRepeating && columns[i].isNull[rowId]) {\n+            return true;\n+          }\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public boolean isNullAt(int ordinal) {\n+      ColumnVector col = columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {"
  }],
  "prId": 13775
}, {
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "nit: space after `(LongColumnVector)`. This might also apply to other places in the PR but I am not pointing out each instance.\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-10-18T00:16:41Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch) {\n+      if (reader.next(NullWritable.get(), internalValue)) {\n+        if (internalValue.endOfFile) {\n+          progress = 1.0f;\n+          numRowsOfBatch = 0;\n+          indexOfRow = 0;\n+          return false;\n+        } else {\n+          assert internalValue.numCols == numColumns : \"Incorrect number of columns in OrcBatch\";\n+          numRowsOfBatch = internalValue.count();\n+          indexOfRow = 0;\n+          progress = reader.getProgress();\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    } else {\n+      if (indexOfRow < numRowsOfBatch) {\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Adapter class to return an internal row.\n+   */\n+  public static final class Row extends InternalRow {\n+    protected int rowId;\n+    private List<Integer> columnIDs;\n+    private final ColumnVector[] columns;\n+\n+    private Row(ColumnVector[] columns, List<Integer> columnIDs) {\n+      this.columns = columns;\n+      this.columnIDs = columnIDs;\n+    }\n+\n+    @Override\n+    public int numFields() { return columnIDs.size(); }\n+\n+    @Override\n+    public boolean anyNull() {\n+      for (int i = 0; i < columns.length; i++) {\n+        if (columnIDs.contains(i)) {\n+          if (columns[i].isRepeating && columns[i].isNull[0]) {\n+            return true;\n+          } else if (!columns[i].isRepeating && columns[i].isNull[rowId]) {\n+            return true;\n+          }\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public boolean isNullAt(int ordinal) {\n+      ColumnVector col = columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return col.isNull[0];\n+      } else {\n+        return col.isNull[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];"
  }],
  "prId": 13775
}, {
  "comments": [{
    "author": {
      "login": "tejasapatil"
    },
    "body": "I think the hive artifact used by Spark does not support vectorisation for all the datatypes and thats why this is not being implemented. In future, upgrading hive in Spark might make it possible to add it. Can you add a comment about this ?\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-10-18T00:20:48Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch) {\n+      if (reader.next(NullWritable.get(), internalValue)) {\n+        if (internalValue.endOfFile) {\n+          progress = 1.0f;\n+          numRowsOfBatch = 0;\n+          indexOfRow = 0;\n+          return false;\n+        } else {\n+          assert internalValue.numCols == numColumns : \"Incorrect number of columns in OrcBatch\";\n+          numRowsOfBatch = internalValue.count();\n+          indexOfRow = 0;\n+          progress = reader.getProgress();\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    } else {\n+      if (indexOfRow < numRowsOfBatch) {\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Adapter class to return an internal row.\n+   */\n+  public static final class Row extends InternalRow {\n+    protected int rowId;\n+    private List<Integer> columnIDs;\n+    private final ColumnVector[] columns;\n+\n+    private Row(ColumnVector[] columns, List<Integer> columnIDs) {\n+      this.columns = columns;\n+      this.columnIDs = columnIDs;\n+    }\n+\n+    @Override\n+    public int numFields() { return columnIDs.size(); }\n+\n+    @Override\n+    public boolean anyNull() {\n+      for (int i = 0; i < columns.length; i++) {\n+        if (columnIDs.contains(i)) {\n+          if (columns[i].isRepeating && columns[i].isNull[0]) {\n+            return true;\n+          } else if (!columns[i].isRepeating && columns[i].isNull[rowId]) {\n+            return true;\n+          }\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public boolean isNullAt(int ordinal) {\n+      ColumnVector col = columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return col.isNull[0];\n+      } else {\n+        return col.isNull[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return col.vector[0] > 0;\n+      } else {\n+        return col.vector[rowId] > 0;\n+      }\n+    }\n+\n+    @Override\n+    public byte getByte(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (byte)col.vector[0];\n+      } else {\n+        return (byte)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public short getShort(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (short)col.vector[0];\n+      } else {\n+        return (short)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public int getInt(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (int)col.vector[0];\n+      } else {\n+        return (int)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public long getLong(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (long)col.vector[0];\n+      } else {\n+        return (long)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public float getFloat(int ordinal) {\n+      DoubleColumnVector col = (DoubleColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (float)col.vector[0];\n+      } else {\n+        return (float)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public double getDouble(int ordinal) {\n+      DoubleColumnVector col = (DoubleColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (double)col.vector[0];\n+      } else {\n+        return (double)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public Decimal getDecimal(int ordinal, int precision, int scale) {\n+      DecimalColumnVector col = (DecimalColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return Decimal.apply(col.vector[0].getHiveDecimal().bigDecimalValue(), precision, scale);\n+      } else {\n+        return Decimal.apply(col.vector[rowId].getHiveDecimal().bigDecimalValue(),\n+          precision, scale);\n+      }\n+    }\n+\n+    @Override\n+    public UTF8String getUTF8String(int ordinal) {\n+      BytesColumnVector bv = ((BytesColumnVector)columns[columnIDs.get(ordinal)]);\n+      if (bv.isRepeating) {\n+        return UTF8String.fromBytes(bv.vector[0], bv.start[0], bv.length[0]);\n+      } else {\n+        return UTF8String.fromBytes(bv.vector[rowId], bv.start[rowId], bv.length[rowId]);\n+      }\n+    }\n+\n+    @Override\n+    public byte[] getBinary(int ordinal) {\n+      BytesColumnVector col = (BytesColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        byte[] binary = new byte[col.length[0]];\n+        System.arraycopy(col.vector[0], col.start[0], binary, 0, binary.length);\n+        return binary;\n+      } else {\n+        byte[] binary = new byte[col.length[rowId]];\n+        System.arraycopy(col.vector[rowId], col.start[rowId], binary, 0, binary.length);\n+        return binary;\n+      }\n+    }\n+\n+    @Override\n+    public CalendarInterval getInterval(int ordinal) {\n+      throw new NotImplementedException();"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "OK.\n",
    "commit": "0ac61b794146634887d184076aababfd25a22ff5",
    "createdAt": "2016-10-18T07:37:30Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.commons.lang.NotImplementedException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * A RecordReader that returns InternalRow for Spark SQL execution.\n+ * This reader uses an internal reader that returns Hive's VectorizedRowBatch. An adapter\n+ * class is used to return internal row by directly accessing data in column vectors.\n+ */\n+public class VectorizedSparkOrcNewRecordReader\n+    extends org.apache.hadoop.mapreduce.RecordReader<NullWritable, InternalRow> {\n+  private final org.apache.hadoop.mapred.RecordReader<NullWritable, VectorizedRowBatch> reader;\n+  private final int numColumns;\n+  private VectorizedRowBatch internalValue;\n+  private float progress = 0.0f;\n+  private List<Integer> columnIDs;\n+\n+  private long numRowsOfBatch = 0;\n+  private int indexOfRow = 0;\n+\n+  private final Row row;\n+\n+  public VectorizedSparkOrcNewRecordReader(\n+      Reader file,\n+      JobConf conf,\n+      FileSplit fileSplit,\n+      List<Integer> columnIDs) throws IOException {\n+    List<OrcProto.Type> types = file.getTypes();\n+    numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();\n+    this.reader = new SparkVectorizedOrcRecordReader(file, conf,\n+      new org.apache.hadoop.mapred.FileSplit(fileSplit));\n+\n+    this.columnIDs = new ArrayList<>(columnIDs);\n+    this.internalValue = this.reader.createValue();\n+    this.progress = reader.getProgress();\n+    this.row = new Row(this.internalValue.cols, this.columnIDs);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    reader.close();\n+  }\n+\n+  @Override\n+  public NullWritable getCurrentKey() throws IOException,\n+      InterruptedException {\n+    return NullWritable.get();\n+  }\n+\n+  @Override\n+  public InternalRow getCurrentValue() throws IOException,\n+      InterruptedException {\n+    if (indexOfRow >= numRowsOfBatch) {\n+      return null;\n+    }\n+    row.rowId = indexOfRow;\n+    indexOfRow++;\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public float getProgress() throws IOException, InterruptedException {\n+    return progress;\n+  }\n+\n+  @Override\n+  public void initialize(InputSplit split, TaskAttemptContext context)\n+      throws IOException, InterruptedException {\n+  }\n+\n+  @Override\n+  public boolean nextKeyValue() throws IOException, InterruptedException {\n+    if (indexOfRow == numRowsOfBatch) {\n+      if (reader.next(NullWritable.get(), internalValue)) {\n+        if (internalValue.endOfFile) {\n+          progress = 1.0f;\n+          numRowsOfBatch = 0;\n+          indexOfRow = 0;\n+          return false;\n+        } else {\n+          assert internalValue.numCols == numColumns : \"Incorrect number of columns in OrcBatch\";\n+          numRowsOfBatch = internalValue.count();\n+          indexOfRow = 0;\n+          progress = reader.getProgress();\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    } else {\n+      if (indexOfRow < numRowsOfBatch) {\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Adapter class to return an internal row.\n+   */\n+  public static final class Row extends InternalRow {\n+    protected int rowId;\n+    private List<Integer> columnIDs;\n+    private final ColumnVector[] columns;\n+\n+    private Row(ColumnVector[] columns, List<Integer> columnIDs) {\n+      this.columns = columns;\n+      this.columnIDs = columnIDs;\n+    }\n+\n+    @Override\n+    public int numFields() { return columnIDs.size(); }\n+\n+    @Override\n+    public boolean anyNull() {\n+      for (int i = 0; i < columns.length; i++) {\n+        if (columnIDs.contains(i)) {\n+          if (columns[i].isRepeating && columns[i].isNull[0]) {\n+            return true;\n+          } else if (!columns[i].isRepeating && columns[i].isNull[rowId]) {\n+            return true;\n+          }\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public boolean isNullAt(int ordinal) {\n+      ColumnVector col = columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return col.isNull[0];\n+      } else {\n+        return col.isNull[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return col.vector[0] > 0;\n+      } else {\n+        return col.vector[rowId] > 0;\n+      }\n+    }\n+\n+    @Override\n+    public byte getByte(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (byte)col.vector[0];\n+      } else {\n+        return (byte)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public short getShort(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (short)col.vector[0];\n+      } else {\n+        return (short)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public int getInt(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (int)col.vector[0];\n+      } else {\n+        return (int)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public long getLong(int ordinal) {\n+      LongColumnVector col = (LongColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (long)col.vector[0];\n+      } else {\n+        return (long)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public float getFloat(int ordinal) {\n+      DoubleColumnVector col = (DoubleColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (float)col.vector[0];\n+      } else {\n+        return (float)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public double getDouble(int ordinal) {\n+      DoubleColumnVector col = (DoubleColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return (double)col.vector[0];\n+      } else {\n+        return (double)col.vector[rowId];\n+      }\n+    }\n+\n+    @Override\n+    public Decimal getDecimal(int ordinal, int precision, int scale) {\n+      DecimalColumnVector col = (DecimalColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        return Decimal.apply(col.vector[0].getHiveDecimal().bigDecimalValue(), precision, scale);\n+      } else {\n+        return Decimal.apply(col.vector[rowId].getHiveDecimal().bigDecimalValue(),\n+          precision, scale);\n+      }\n+    }\n+\n+    @Override\n+    public UTF8String getUTF8String(int ordinal) {\n+      BytesColumnVector bv = ((BytesColumnVector)columns[columnIDs.get(ordinal)]);\n+      if (bv.isRepeating) {\n+        return UTF8String.fromBytes(bv.vector[0], bv.start[0], bv.length[0]);\n+      } else {\n+        return UTF8String.fromBytes(bv.vector[rowId], bv.start[rowId], bv.length[rowId]);\n+      }\n+    }\n+\n+    @Override\n+    public byte[] getBinary(int ordinal) {\n+      BytesColumnVector col = (BytesColumnVector)columns[columnIDs.get(ordinal)];\n+      if (col.isRepeating) {\n+        byte[] binary = new byte[col.length[0]];\n+        System.arraycopy(col.vector[0], col.start[0], binary, 0, binary.length);\n+        return binary;\n+      } else {\n+        byte[] binary = new byte[col.length[rowId]];\n+        System.arraycopy(col.vector[rowId], col.start[rowId], binary, 0, binary.length);\n+        return binary;\n+      }\n+    }\n+\n+    @Override\n+    public CalendarInterval getInterval(int ordinal) {\n+      throw new NotImplementedException();"
  }],
  "prId": 13775
}]