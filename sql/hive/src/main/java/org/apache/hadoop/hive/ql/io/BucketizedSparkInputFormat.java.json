[{
  "comments": [{
    "author": {
      "login": "chrysan"
    },
    "body": "This logic depends on the files are listed in a right order, otherwise the RDD partitions to be joined cannot be zipped correctly. Logic should be fixed here to reorder the files listed. ",
    "commit": "d37eb8b3359981756c923948fe12833a56b61865",
    "createdAt": "2018-03-31T14:46:54Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.io.WritableComparable;\n+import org.apache.hadoop.mapred.*;\n+import org.apache.hadoop.util.StringUtils;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import static org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR;\n+\n+/**\n+ * A {@link InputFormat} implementation for reading bucketed tables.\n+ *\n+ * We cannot directly use {@link BucketizedHiveInputFormat} from Hive as it depends on the\n+ * map-reduce plan to get required information for split generation.\n+ */\n+public class BucketizedSparkInputFormat<K extends WritableComparable, V extends Writable>\n+        extends BucketizedHiveInputFormat<K, V> {\n+\n+  private static final String FILE_INPUT_FORMAT = \"file.inputformat\";\n+\n+  @Override\n+  public RecordReader getRecordReader(\n+      InputSplit split,\n+      JobConf job,\n+      Reporter reporter) throws IOException {\n+\n+    BucketizedHiveInputSplit hsplit = (BucketizedHiveInputSplit) split;\n+    String inputFormatClassName = null;\n+    Class inputFormatClass = null;\n+\n+    try {\n+      inputFormatClassName = hsplit.inputFormatClassName();\n+      inputFormatClass = job.getClassByName(inputFormatClassName);\n+    } catch (ClassNotFoundException e) {\n+      throw new IOException(\"Cannot find class \" + inputFormatClassName, e);\n+    }\n+\n+    InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);\n+    return new BucketizedSparkRecordReader<>(inputFormat, hsplit, job, reporter);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numBuckets) throws IOException {\n+    final String inputFormatClassName = job.get(FILE_INPUT_FORMAT);\n+    final String[] inputDirs = job.get(INPUT_DIR).split(StringUtils.COMMA_STR);\n+\n+    if (inputDirs.length != 1) {\n+      throw new IOException(this.getClass().getCanonicalName() +\n+        \" expects only one input directory. \" + inputDirs.length +\n+        \" directories detected : \" + Arrays.toString(inputDirs));\n+    }\n+\n+    final String inputDir = inputDirs[0];\n+    final Path inputPath = new Path(inputDir);\n+    final JobConf newJob = new JobConf(job);\n+    final FileStatus[] listStatus = this.listStatus(newJob, inputPath);\n+    final InputSplit[] result = new InputSplit[numBuckets];\n+\n+    if (listStatus.length != 0 && listStatus.length != numBuckets) {\n+      throw new IOException(\"Bucketed path was expected to have \" + numBuckets + \" files but \" +\n+        listStatus.length + \" files are present. Path = \" + inputPath);\n+    }\n+\n+    try {\n+      final Class<?> inputFormatClass = Class.forName(inputFormatClassName);\n+      final InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job);\n+      newJob.setInputFormat(inputFormat.getClass());\n+\n+      for (int i = 0; i < numBuckets; i++) {\n+        final FileStatus fileStatus = listStatus[i];",
    "line": 93
  }],
  "prId": 19001
}]