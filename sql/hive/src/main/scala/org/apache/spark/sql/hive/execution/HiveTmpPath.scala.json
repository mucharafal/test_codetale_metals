[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "No need to extend `RunnableCommand `",
    "commit": "f93d57a224da29ee529a2d691c0d8edea1808d0b",
    "createdAt": "2017-09-06T20:56:04Z",
    "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.{File, IOException}\n+import java.net.URI\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Locale, Random}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.common.FileUtils\n+import org.apache.hadoop.hive.ql.exec.TaskRunner\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+import org.apache.spark.sql.hive.HiveExternalCatalog\n+import org.apache.spark.sql.hive.client.HiveVersion\n+\n+// Base trait for getting a temporary location for writing data\n+private[hive] trait HiveTmpPath extends RunnableCommand {"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Instead, we should extend it in the class that extend HiveTmpPath",
    "commit": "f93d57a224da29ee529a2d691c0d8edea1808d0b",
    "createdAt": "2017-09-06T20:56:28Z",
    "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.{File, IOException}\n+import java.net.URI\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Locale, Random}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.common.FileUtils\n+import org.apache.hadoop.hive.ql.exec.TaskRunner\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+import org.apache.spark.sql.hive.HiveExternalCatalog\n+import org.apache.spark.sql.hive.client.HiveVersion\n+\n+// Base trait for getting a temporary location for writing data\n+private[hive] trait HiveTmpPath extends RunnableCommand {"
  }, {
    "author": {
      "login": "janewangfb"
    },
    "body": "removed RunnableCommand, the classes that exten HiveTmpPath already extends RunnableCommand",
    "commit": "f93d57a224da29ee529a2d691c0d8edea1808d0b",
    "createdAt": "2017-09-06T21:48:40Z",
    "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.{File, IOException}\n+import java.net.URI\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Locale, Random}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.common.FileUtils\n+import org.apache.hadoop.hive.ql.exec.TaskRunner\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+import org.apache.spark.sql.hive.HiveExternalCatalog\n+import org.apache.spark.sql.hive.client.HiveVersion\n+\n+// Base trait for getting a temporary location for writing data\n+private[hive] trait HiveTmpPath extends RunnableCommand {"
  }],
  "prId": 18975
}, {
  "comments": [{
    "author": {
      "login": "janewangfb"
    },
    "body": "added stagingDir to deleteExternalTmpPath",
    "commit": "f93d57a224da29ee529a2d691c0d8edea1808d0b",
    "createdAt": "2017-09-06T21:51:44Z",
    "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.{File, IOException}\n+import java.net.URI\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Locale, Random}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.common.FileUtils\n+import org.apache.hadoop.hive.ql.exec.TaskRunner\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.command.RunnableCommand\n+import org.apache.spark.sql.hive.HiveExternalCatalog\n+import org.apache.spark.sql.hive.client.HiveVersion\n+\n+// Base trait for getting a temporary location for writing data\n+private[hive] trait HiveTmpPath extends RunnableCommand {\n+\n+  var createdTempDir: Option[Path] = None\n+\n+  private var stagingDir: String = \"\"\n+\n+  def getExternalTmpPath(\n+      sparkSession: SparkSession,\n+      hadoopConf: Configuration,\n+      path: Path): Path = {\n+    import org.apache.spark.sql.hive.client.hive._\n+\n+    // Before Hive 1.1, when inserting into a table, Hive will create the staging directory under\n+    // a common scratch directory. After the writing is finished, Hive will simply empty the table\n+    // directory and move the staging directory to it.\n+    // After Hive 1.1, Hive will create the staging directory under the table directory, and when\n+    // moving staging directory to table directory, Hive will still empty the table directory, but\n+    // will exclude the staging directory there.\n+    // We have to follow the Hive behavior here, to avoid troubles. For example, if we create\n+    // staging directory under the table director for Hive prior to 1.1, the staging directory will\n+    // be removed by Hive when Hive is trying to empty the table directory.\n+    val hiveVersionsUsingOldExternalTempPath: Set[HiveVersion] = Set(v12, v13, v14, v1_0)\n+    val hiveVersionsUsingNewExternalTempPath: Set[HiveVersion] = Set(v1_1, v1_2, v2_0, v2_1)\n+\n+    // Ensure all the supported versions are considered here.\n+    assert(hiveVersionsUsingNewExternalTempPath ++ hiveVersionsUsingOldExternalTempPath ==\n+      allSupportedHiveVersions)\n+\n+    val externalCatalog = sparkSession.sharedState.externalCatalog\n+    val hiveVersion = externalCatalog.asInstanceOf[HiveExternalCatalog].client.version\n+    stagingDir = hadoopConf.get(\"hive.exec.stagingdir\", \".hive-staging\")\n+    val scratchDir = hadoopConf.get(\"hive.exec.scratchdir\", \"/tmp/hive\")\n+\n+    if (hiveVersionsUsingOldExternalTempPath.contains(hiveVersion)) {\n+      oldVersionExternalTempPath(path, hadoopConf, scratchDir)\n+    } else if (hiveVersionsUsingNewExternalTempPath.contains(hiveVersion)) {\n+      newVersionExternalTempPath(path, hadoopConf, stagingDir)\n+    } else {\n+      throw new IllegalStateException(\"Unsupported hive version: \" + hiveVersion.fullVersion)\n+    }\n+  }\n+\n+  def deleteExternalTmpPath(hadoopConf : Configuration) : Unit = {\n+    // Attempt to delete the staging directory and the inclusive files. If failed, the files are\n+    // expected to be dropped at the normal termination of VM since deleteOnExit is used.\n+    try {\n+      createdTempDir.foreach { path =>\n+        val fs = path.getFileSystem(hadoopConf)\n+        if (fs.delete(path, true)) {\n+          // If we successfully delete the staging directory, remove it from FileSystem's cache.\n+          fs.cancelDeleteOnExit(path)\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Unable to delete staging directory: $stagingDir.\\n\" + e)"
  }],
  "prId": 18975
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Based on the current codes, `stagingDir ` in the caller is only used for `deleteExternalTmpPath `. Let us get the value from `hadoopConf`. That is, move it into `deleteExternalTmpPath `",
    "commit": "f93d57a224da29ee529a2d691c0d8edea1808d0b",
    "createdAt": "2017-09-07T06:23:51Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.{File, IOException}\n+import java.net.URI\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Locale, Random}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.common.FileUtils\n+import org.apache.hadoop.hive.ql.exec.TaskRunner\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.hive.HiveExternalCatalog\n+import org.apache.spark.sql.hive.client.HiveVersion\n+\n+// Base trait for getting a temporary location for writing data\n+private[hive] trait HiveTmpPath extends Logging {\n+\n+  var createdTempDir: Option[Path] = None\n+\n+  def getExternalTmpPath(\n+      sparkSession: SparkSession,\n+      hadoopConf: Configuration,\n+      path: Path): Path = {\n+    import org.apache.spark.sql.hive.client.hive._\n+\n+    // Before Hive 1.1, when inserting into a table, Hive will create the staging directory under\n+    // a common scratch directory. After the writing is finished, Hive will simply empty the table\n+    // directory and move the staging directory to it.\n+    // After Hive 1.1, Hive will create the staging directory under the table directory, and when\n+    // moving staging directory to table directory, Hive will still empty the table directory, but\n+    // will exclude the staging directory there.\n+    // We have to follow the Hive behavior here, to avoid troubles. For example, if we create\n+    // staging directory under the table director for Hive prior to 1.1, the staging directory will\n+    // be removed by Hive when Hive is trying to empty the table directory.\n+    val hiveVersionsUsingOldExternalTempPath: Set[HiveVersion] = Set(v12, v13, v14, v1_0)\n+    val hiveVersionsUsingNewExternalTempPath: Set[HiveVersion] = Set(v1_1, v1_2, v2_0, v2_1)\n+\n+    // Ensure all the supported versions are considered here.\n+    assert(hiveVersionsUsingNewExternalTempPath ++ hiveVersionsUsingOldExternalTempPath ==\n+      allSupportedHiveVersions)\n+\n+    val externalCatalog = sparkSession.sharedState.externalCatalog\n+    val hiveVersion = externalCatalog.asInstanceOf[HiveExternalCatalog].client.version\n+    val stagingDir = hadoopConf.get(\"hive.exec.stagingdir\", \".hive-staging\")\n+    val scratchDir = hadoopConf.get(\"hive.exec.scratchdir\", \"/tmp/hive\")\n+\n+    if (hiveVersionsUsingOldExternalTempPath.contains(hiveVersion)) {\n+      oldVersionExternalTempPath(path, hadoopConf, scratchDir)\n+    } else if (hiveVersionsUsingNewExternalTempPath.contains(hiveVersion)) {\n+      newVersionExternalTempPath(path, hadoopConf, stagingDir)\n+    } else {\n+      throw new IllegalStateException(\"Unsupported hive version: \" + hiveVersion.fullVersion)\n+    }\n+  }\n+\n+  def deleteExternalTmpPath(hadoopConf: Configuration, stagingDir: String) : Unit = {"
  }, {
    "author": {
      "login": "janewangfb"
    },
    "body": "updated.",
    "commit": "f93d57a224da29ee529a2d691c0d8edea1808d0b",
    "createdAt": "2017-09-07T17:35:26Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.{File, IOException}\n+import java.net.URI\n+import java.text.SimpleDateFormat\n+import java.util.{Date, Locale, Random}\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.common.FileUtils\n+import org.apache.hadoop.hive.ql.exec.TaskRunner\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.hive.HiveExternalCatalog\n+import org.apache.spark.sql.hive.client.HiveVersion\n+\n+// Base trait for getting a temporary location for writing data\n+private[hive] trait HiveTmpPath extends Logging {\n+\n+  var createdTempDir: Option[Path] = None\n+\n+  def getExternalTmpPath(\n+      sparkSession: SparkSession,\n+      hadoopConf: Configuration,\n+      path: Path): Path = {\n+    import org.apache.spark.sql.hive.client.hive._\n+\n+    // Before Hive 1.1, when inserting into a table, Hive will create the staging directory under\n+    // a common scratch directory. After the writing is finished, Hive will simply empty the table\n+    // directory and move the staging directory to it.\n+    // After Hive 1.1, Hive will create the staging directory under the table directory, and when\n+    // moving staging directory to table directory, Hive will still empty the table directory, but\n+    // will exclude the staging directory there.\n+    // We have to follow the Hive behavior here, to avoid troubles. For example, if we create\n+    // staging directory under the table director for Hive prior to 1.1, the staging directory will\n+    // be removed by Hive when Hive is trying to empty the table directory.\n+    val hiveVersionsUsingOldExternalTempPath: Set[HiveVersion] = Set(v12, v13, v14, v1_0)\n+    val hiveVersionsUsingNewExternalTempPath: Set[HiveVersion] = Set(v1_1, v1_2, v2_0, v2_1)\n+\n+    // Ensure all the supported versions are considered here.\n+    assert(hiveVersionsUsingNewExternalTempPath ++ hiveVersionsUsingOldExternalTempPath ==\n+      allSupportedHiveVersions)\n+\n+    val externalCatalog = sparkSession.sharedState.externalCatalog\n+    val hiveVersion = externalCatalog.asInstanceOf[HiveExternalCatalog].client.version\n+    val stagingDir = hadoopConf.get(\"hive.exec.stagingdir\", \".hive-staging\")\n+    val scratchDir = hadoopConf.get(\"hive.exec.scratchdir\", \"/tmp/hive\")\n+\n+    if (hiveVersionsUsingOldExternalTempPath.contains(hiveVersion)) {\n+      oldVersionExternalTempPath(path, hadoopConf, scratchDir)\n+    } else if (hiveVersionsUsingNewExternalTempPath.contains(hiveVersion)) {\n+      newVersionExternalTempPath(path, hadoopConf, stagingDir)\n+    } else {\n+      throw new IllegalStateException(\"Unsupported hive version: \" + hiveVersion.fullVersion)\n+    }\n+  }\n+\n+  def deleteExternalTmpPath(hadoopConf: Configuration, stagingDir: String) : Unit = {"
  }],
  "prId": 18975
}]