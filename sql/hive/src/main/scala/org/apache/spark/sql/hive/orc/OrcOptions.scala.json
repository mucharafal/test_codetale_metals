[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "This is confusing. You can remove `By default use the value specified in SQLConf.` ",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T06:23:52Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Sure, I will. Historically, I brought this from [ParquetOptions](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala#L41). ",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T15:44:47Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf."
  }],
  "prId": 19055
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Instead, update this paragraph to explain the priority.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T06:24:25Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n-  val compressionCodec: String = {\n+  val compressionCodecClassName: String = {\n     // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n     // `compression` has higher precedence than `orc.compress`. It means if both are set,\n     // we will use `compression`."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Sure.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T15:46:54Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n-  val compressionCodec: String = {\n+  val compressionCodecClassName: String = {\n     // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n     // `compression` has higher precedence than `orc.compress`. It means if both are set,\n     // we will use `compression`."
  }],
  "prId": 19055
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I guess we should update here too:\r\n\r\nhttps://github.com/apache/spark/blob/3c0c2d09ca89c6b6247137823169db17847dfae3/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#L522 \r\n\r\nand I think we should prioritise\r\n \r\n1. `compression` \r\n2. `spark.sql.orc.compression.codec`\r\n3. `orc.compress`\r\n\r\nto be consistent.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T12:46:46Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n-  val compressionCodec: String = {\n+  val compressionCodecClassName: String = {\n     // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n     // `compression` has higher precedence than `orc.compress`. It means if both are set,\n     // we will use `compression`."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Oh, I see.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T15:57:15Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n-  val compressionCodec: String = {\n+  val compressionCodecClassName: String = {\n     // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n     // `compression` has higher precedence than `orc.compress`. It means if both are set,\n     // we will use `compression`."
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Ur, there is a technical issue.\r\n`spark.sql.orc.compression.codec` has a default value `snappy`. So, `orc.comress` cannot be used in that order.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T15:58:10Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n-  val compressionCodec: String = {\n+  val compressionCodecClassName: String = {\n     // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n     // `compression` has higher precedence than `orc.compress`. It means if both are set,\n     // we will use `compression`."
  }],
  "prId": 19055
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I guess you matched this to `ParquetOptions` but actually I think it's `ParquetOptions` to be changed. I found this minor nit but after it got merged - https://github.com/apache/spark/pull/15580#discussion_r84577126",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T13:48:37Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n-  val compressionCodec: String = {\n+  val compressionCodecClassName: String = {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Oh, may I change Parquet at this time?",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T15:51:54Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n-  val compressionCodec: String = {\n+  val compressionCodecClassName: String = {"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Anyway, I'll revert this for the other data soruces.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-28T15:52:11Z",
    "diffHunk": "@@ -20,30 +20,35 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use. By default use the value specified in SQLConf.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n-  val compressionCodec: String = {\n+  val compressionCodecClassName: String = {"
  }],
  "prId": 19055
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Could we update the default values for consistency with Parquet one? :\r\n\r\nhttps://github.com/apache/spark/blob/3c0c2d09ca89c6b6247137823169db17847dfae3/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#L520\r\n\r\nhttps://github.com/apache/spark/blob/51620e288b5e0a7fffc3899c9deadabace28e6d7/python/pyspark/sql/readwriter.py#L855\r\n\r\n\r\n",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T02:21:06Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order.\n     val orcCompressionConf = parameters.get(OrcRelation.ORC_COMPRESSION)\n     val codecName = parameters\n       .get(\"compression\")\n       .orElse(orcCompressionConf)\n-      .getOrElse(\"snappy\").toLowerCase(Locale.ROOT)\n+      .getOrElse(sqlConf.orcCompressionCodec)",
    "line": 37
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "The default value is [snappy](https://github.com/apache/spark/pull/19055/files#diff-9a6b543db706f1a90f790783d6930a13R331), isn't it?\r\n```\r\n.createWithDefault(\"snappy\")\r\n```",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T02:26:45Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order.\n     val orcCompressionConf = parameters.get(OrcRelation.ORC_COMPRESSION)\n     val codecName = parameters\n       .get(\"compression\")\n       .orElse(orcCompressionConf)\n-      .getOrElse(\"snappy\").toLowerCase(Locale.ROOT)\n+      .getOrElse(sqlConf.orcCompressionCodec)",
    "line": 37
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I was thinking like:\r\n\r\nhttps://github.com/apache/spark/blob/51620e288b5e0a7fffc3899c9deadabace28e6d7/python/pyspark/sql/readwriter.py#L751-L753\r\n\r\nWouldn't we use the value set in `spark.sql.parquet.compression.codec` by default if `compression` is unset via option API?",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T02:30:06Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order.\n     val orcCompressionConf = parameters.get(OrcRelation.ORC_COMPRESSION)\n     val codecName = parameters\n       .get(\"compression\")\n       .orElse(orcCompressionConf)\n-      .getOrElse(\"snappy\").toLowerCase(Locale.ROOT)\n+      .getOrElse(sqlConf.orcCompressionCodec)",
    "line": 37
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Actually, I thought the purpose of this configuration is rather for setting the default compression codec for ORC datasource ..",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T02:33:37Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order.\n     val orcCompressionConf = parameters.get(OrcRelation.ORC_COMPRESSION)\n     val codecName = parameters\n       .get(\"compression\")\n       .orElse(orcCompressionConf)\n-      .getOrElse(\"snappy\").toLowerCase(Locale.ROOT)\n+      .getOrElse(sqlConf.orcCompressionCodec)",
    "line": 37
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Yes. This is the priority. If `compression` and `orc.compression` is unset via option, we use SQLConf.\r\n`compression` -> `orc.compression` -> `spark.sql.orc.compression.codec`",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T02:34:49Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order.\n     val orcCompressionConf = parameters.get(OrcRelation.ORC_COMPRESSION)\n     val codecName = parameters\n       .get(\"compression\")\n       .orElse(orcCompressionConf)\n-      .getOrElse(\"snappy\").toLowerCase(Locale.ROOT)\n+      .getOrElse(sqlConf.orcCompressionCodec)",
    "line": 37
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "The main purpose of this PR is to support users to control ORC compression by using SQLConf, too.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T02:39:02Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order.\n     val orcCompressionConf = parameters.get(OrcRelation.ORC_COMPRESSION)\n     val codecName = parameters\n       .get(\"compression\")\n       .orElse(orcCompressionConf)\n-      .getOrElse(\"snappy\").toLowerCase(Locale.ROOT)\n+      .getOrElse(sqlConf.orcCompressionCodec)",
    "line": 37
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "The default codec is unchanged and the priority is the same. Also, all previous user-given options are respected.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T02:40:03Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order.\n     val orcCompressionConf = parameters.get(OrcRelation.ORC_COMPRESSION)\n     val codecName = parameters\n       .get(\"compression\")\n       .orElse(orcCompressionConf)\n-      .getOrElse(\"snappy\").toLowerCase(Locale.ROOT)\n+      .getOrElse(sqlConf.orcCompressionCodec)",
    "line": 37
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah, it looks I had to be clear. I meant fixing the comment for default value from `snappy` to `spark.sql.orc.compression.codec`.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T02:52:09Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order.\n     val orcCompressionConf = parameters.get(OrcRelation.ORC_COMPRESSION)\n     val codecName = parameters\n       .get(\"compression\")\n       .orElse(orcCompressionConf)\n-      .getOrElse(\"snappy\").toLowerCase(Locale.ROOT)\n+      .getOrElse(sqlConf.orcCompressionCodec)",
    "line": 37
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Thank you! I'll fix that.",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T02:53:45Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order.\n     val orcCompressionConf = parameters.get(OrcRelation.ORC_COMPRESSION)\n     val codecName = parameters\n       .get(\"compression\")\n       .orElse(orcCompressionConf)\n-      .getOrElse(\"snappy\").toLowerCase(Locale.ROOT)\n+      .getOrElse(sqlConf.orcCompressionCodec)",
    "line": 37
  }],
  "prId": 19055
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`is used in order` -> `are in order of precedence from highest to lowest`",
    "commit": "9620e46c9a38549f72565e606d892c84b6301ccb",
    "createdAt": "2017-08-30T17:52:43Z",
    "diffHunk": "@@ -20,30 +20,33 @@ package org.apache.spark.sql.hive.orc\n import java.util.Locale\n \n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for the ORC data source.\n  */\n-private[orc] class OrcOptions(@transient private val parameters: CaseInsensitiveMap[String])\n+private[orc] class OrcOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String],\n+    @transient private val sqlConf: SQLConf)\n   extends Serializable {\n \n   import OrcOptions._\n \n-  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+  def this(parameters: Map[String, String], sqlConf: SQLConf) =\n+    this(CaseInsensitiveMap(parameters), sqlConf)\n \n   /**\n-   * Compression codec to use. By default snappy compression.\n+   * Compression codec to use.\n    * Acceptable values are defined in [[shortOrcCompressionCodecNames]].\n    */\n   val compressionCodec: String = {\n-    // `orc.compress` is a ORC configuration. So, here we respect this as an option but\n-    // `compression` has higher precedence than `orc.compress`. It means if both are set,\n-    // we will use `compression`.\n+    // `compression`, `orc.compress`, and `spark.sql.orc.compression.codec` is used in order."
  }],
  "prId": 19055
}]