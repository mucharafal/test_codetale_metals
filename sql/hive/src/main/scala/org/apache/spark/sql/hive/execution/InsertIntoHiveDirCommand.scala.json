[{
  "comments": [{
    "author": {
      "login": "liupc"
    },
    "body": "but not a local path?",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-02-21T04:52:37Z",
    "diffHunk": "@@ -86,20 +86,21 @@ case class InsertIntoHiveDirCommand(\n     val jobConf = new JobConf(hadoopConf)\n \n     val targetPath = new Path(storage.locationUri.get)\n-    val writeToPath =\n+    val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n+    val (writeToPath : Path , fs : FileSystem) =\n       if (isLocal) {\n         val localFileSystem = FileSystem.getLocal(jobConf)\n-        localFileSystem.makeQualified(targetPath)\n+        (localFileSystem.makeQualified(targetPath), localFileSystem)\n       } else {\n-        val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n-        val dfs = qualifiedPath.getFileSystem(jobConf)\n-        if (!dfs.exists(qualifiedPath)) {\n-          dfs.mkdirs(qualifiedPath.getParent)\n-        }\n-        qualifiedPath\n+        val dfs = qualifiedPath.getFileSystem(hadoopConf)\n+        (qualifiedPath, dfs)\n       }\n+    if (!fs.exists(writeToPath)) {\n+      fs.mkdirs(writeToPath)\n+    }\n \n-    val tmpPath = getExternalTmpPath(sparkSession, hadoopConf, writeToPath)\n+    // The temporary path must be a HDFS path but a local path."
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "OK,thanks a lot!\r\nI get it and fix the incorrect syntax just now.",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-02-22T04:14:11Z",
    "diffHunk": "@@ -86,20 +86,21 @@ case class InsertIntoHiveDirCommand(\n     val jobConf = new JobConf(hadoopConf)\n \n     val targetPath = new Path(storage.locationUri.get)\n-    val writeToPath =\n+    val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n+    val (writeToPath : Path , fs : FileSystem) =\n       if (isLocal) {\n         val localFileSystem = FileSystem.getLocal(jobConf)\n-        localFileSystem.makeQualified(targetPath)\n+        (localFileSystem.makeQualified(targetPath), localFileSystem)\n       } else {\n-        val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n-        val dfs = qualifiedPath.getFileSystem(jobConf)\n-        if (!dfs.exists(qualifiedPath)) {\n-          dfs.mkdirs(qualifiedPath.getParent)\n-        }\n-        qualifiedPath\n+        val dfs = qualifiedPath.getFileSystem(hadoopConf)\n+        (qualifiedPath, dfs)\n       }\n+    if (!fs.exists(writeToPath)) {\n+      fs.mkdirs(writeToPath)\n+    }\n \n-    val tmpPath = getExternalTmpPath(sparkSession, hadoopConf, writeToPath)\n+    // The temporary path must be a HDFS path but a local path."
  }],
  "prId": 23841
}, {
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "nit: \r\n```\r\n    val (writeToPath, fs) = if (isLocal) {\r\n      val localFileSystem = FileSystem.getLocal(jobConf)\r\n      (localFileSystem.makeQualified(targetPath), localFileSystem)\r\n    } else {\r\n      val dfs = qualifiedPath.getFileSystem(hadoopConf)\r\n      (qualifiedPath, dfs)\r\n    }\r\n```",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-02-26T05:56:31Z",
    "diffHunk": "@@ -86,20 +86,21 @@ case class InsertIntoHiveDirCommand(\n     val jobConf = new JobConf(hadoopConf)\n \n     val targetPath = new Path(storage.locationUri.get)\n-    val writeToPath =\n+    val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n+    val (writeToPath: Path, fs: FileSystem) =",
    "line": 6
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "OK, I have adjust it.",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-02-26T08:18:53Z",
    "diffHunk": "@@ -86,20 +86,21 @@ case class InsertIntoHiveDirCommand(\n     val jobConf = new JobConf(hadoopConf)\n \n     val targetPath = new Path(storage.locationUri.get)\n-    val writeToPath =\n+    val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n+    val (writeToPath: Path, fs: FileSystem) =",
    "line": 6
  }],
  "prId": 23841
}, {
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "In case of inserts from non-hive tables, we still need to use a non-local path?",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-02-26T05:57:59Z",
    "diffHunk": "@@ -86,20 +86,21 @@ case class InsertIntoHiveDirCommand(\n     val jobConf = new JobConf(hadoopConf)\n \n     val targetPath = new Path(storage.locationUri.get)\n-    val writeToPath =\n+    val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n+    val (writeToPath: Path, fs: FileSystem) =\n       if (isLocal) {\n         val localFileSystem = FileSystem.getLocal(jobConf)\n-        localFileSystem.makeQualified(targetPath)\n+        (localFileSystem.makeQualified(targetPath), localFileSystem)\n       } else {\n-        val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n-        val dfs = qualifiedPath.getFileSystem(jobConf)\n-        if (!dfs.exists(qualifiedPath)) {\n-          dfs.mkdirs(qualifiedPath.getParent)\n-        }\n-        qualifiedPath\n+        val dfs = qualifiedPath.getFileSystem(hadoopConf)\n+        (qualifiedPath, dfs)\n       }\n+    if (!fs.exists(writeToPath)) {\n+      fs.mkdirs(writeToPath)\n+    }\n \n-    val tmpPath = getExternalTmpPath(sparkSession, hadoopConf, writeToPath)\n+    // The temporary path must be a HDFS path, not a local path.\n+    val tmpPath = getExternalTmpPath(sparkSession, hadoopConf, qualifiedPath)",
    "line": 27
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "If target path is local, we still need to use a non-local path.",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-02-26T08:19:43Z",
    "diffHunk": "@@ -86,20 +86,21 @@ case class InsertIntoHiveDirCommand(\n     val jobConf = new JobConf(hadoopConf)\n \n     val targetPath = new Path(storage.locationUri.get)\n-    val writeToPath =\n+    val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n+    val (writeToPath: Path, fs: FileSystem) =\n       if (isLocal) {\n         val localFileSystem = FileSystem.getLocal(jobConf)\n-        localFileSystem.makeQualified(targetPath)\n+        (localFileSystem.makeQualified(targetPath), localFileSystem)\n       } else {\n-        val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n-        val dfs = qualifiedPath.getFileSystem(jobConf)\n-        if (!dfs.exists(qualifiedPath)) {\n-          dfs.mkdirs(qualifiedPath.getParent)\n-        }\n-        qualifiedPath\n+        val dfs = qualifiedPath.getFileSystem(hadoopConf)\n+        (qualifiedPath, dfs)\n       }\n+    if (!fs.exists(writeToPath)) {\n+      fs.mkdirs(writeToPath)\n+    }\n \n-    val tmpPath = getExternalTmpPath(sparkSession, hadoopConf, writeToPath)\n+    // The temporary path must be a HDFS path, not a local path.\n+    val tmpPath = getExternalTmpPath(sparkSession, hadoopConf, qualifiedPath)",
    "line": 27
  }],
  "prId": 23841
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "What is the reason we change this line from jobConf to hadoopConf?",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-04-29T20:23:29Z",
    "diffHunk": "@@ -87,17 +87,17 @@ case class InsertIntoHiveDirCommand(\n \n     val targetPath = new Path(storage.locationUri.get)\n     val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n-    val writeToPath =\n+    val (writeToPath: Path, fs: FileSystem) =\n       if (isLocal) {\n         val localFileSystem = FileSystem.getLocal(jobConf)\n-        localFileSystem.makeQualified(targetPath)\n+        (localFileSystem.makeQualified(targetPath), localFileSystem)\n       } else {\n-        val dfs = qualifiedPath.getFileSystem(jobConf)\n-        if (!dfs.exists(qualifiedPath)) {\n-          dfs.mkdirs(qualifiedPath.getParent)\n-        }\n-        qualifiedPath\n+        val dfs = qualifiedPath.getFileSystem(hadoopConf)",
    "line": 18
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "cc @beliefer @srowen ",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-04-29T20:24:28Z",
    "diffHunk": "@@ -87,17 +87,17 @@ case class InsertIntoHiveDirCommand(\n \n     val targetPath = new Path(storage.locationUri.get)\n     val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n-    val writeToPath =\n+    val (writeToPath: Path, fs: FileSystem) =\n       if (isLocal) {\n         val localFileSystem = FileSystem.getLocal(jobConf)\n-        localFileSystem.makeQualified(targetPath)\n+        (localFileSystem.makeQualified(targetPath), localFileSystem)\n       } else {\n-        val dfs = qualifiedPath.getFileSystem(jobConf)\n-        if (!dfs.exists(qualifiedPath)) {\n-          dfs.mkdirs(qualifiedPath.getParent)\n-        }\n-        qualifiedPath\n+        val dfs = qualifiedPath.getFileSystem(hadoopConf)",
    "line": 18
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Good question; I believe it was for consistency? the previous line used `hadoopConf`. `jobConf` inherits the same properties so I'd imagine it won't matter in this context. Does it cause an issue that you know of?",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-04-29T20:33:34Z",
    "diffHunk": "@@ -87,17 +87,17 @@ case class InsertIntoHiveDirCommand(\n \n     val targetPath = new Path(storage.locationUri.get)\n     val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n-    val writeToPath =\n+    val (writeToPath: Path, fs: FileSystem) =\n       if (isLocal) {\n         val localFileSystem = FileSystem.getLocal(jobConf)\n-        localFileSystem.makeQualified(targetPath)\n+        (localFileSystem.makeQualified(targetPath), localFileSystem)\n       } else {\n-        val dfs = qualifiedPath.getFileSystem(jobConf)\n-        if (!dfs.exists(qualifiedPath)) {\n-          dfs.mkdirs(qualifiedPath.getParent)\n-        }\n-        qualifiedPath\n+        val dfs = qualifiedPath.getFileSystem(hadoopConf)",
    "line": 18
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "It sounds like it is safe to remove the variable jobConf and replace all the usage in this file by hadoopConf",
    "commit": "0913a45ac701b99fc1c8d8e556646a45f30c330f",
    "createdAt": "2019-04-29T20:39:44Z",
    "diffHunk": "@@ -87,17 +87,17 @@ case class InsertIntoHiveDirCommand(\n \n     val targetPath = new Path(storage.locationUri.get)\n     val qualifiedPath = FileUtils.makeQualified(targetPath, hadoopConf)\n-    val writeToPath =\n+    val (writeToPath: Path, fs: FileSystem) =\n       if (isLocal) {\n         val localFileSystem = FileSystem.getLocal(jobConf)\n-        localFileSystem.makeQualified(targetPath)\n+        (localFileSystem.makeQualified(targetPath), localFileSystem)\n       } else {\n-        val dfs = qualifiedPath.getFileSystem(jobConf)\n-        if (!dfs.exists(qualifiedPath)) {\n-          dfs.mkdirs(qualifiedPath.getParent)\n-        }\n-        qualifiedPath\n+        val dfs = qualifiedPath.getFileSystem(hadoopConf)",
    "line": 18
  }],
  "prId": 23841
}]