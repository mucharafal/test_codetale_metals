[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "split this line\n",
    "commit": "c2ce29607ce61ecedd2aabe41835fa0a1750af57",
    "createdAt": "2016-03-28T18:18:31Z",
    "diffHunk": "@@ -317,12 +317,11 @@ private[orc] case class OrcTableScan(\n       classOf[OrcInputFormat]\n         .asInstanceOf[Class[_ <: MapRedInputFormat[NullWritable, Writable]]]\n \n-    val rdd = sqlContext.sparkContext.hadoopRDD(\n+    val rdd = new HadoopRDD(sqlContext.sparkContext,\n       conf.asInstanceOf[JobConf],\n       inputFormatClass,\n       classOf[NullWritable],\n-      classOf[Writable]\n-    ).asInstanceOf[HadoopRDD[NullWritable, Writable]]\n+      classOf[Writable], sqlContext.sparkContext.defaultMinPartitions)"
  }],
  "prId": 11978
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "can you add a comment to say we're creating a HadoopRDD here directly to bypass closure cleaning\n",
    "commit": "c2ce29607ce61ecedd2aabe41835fa0a1750af57",
    "createdAt": "2016-03-28T18:18:49Z",
    "diffHunk": "@@ -317,12 +317,11 @@ private[orc] case class OrcTableScan(\n       classOf[OrcInputFormat]\n         .asInstanceOf[Class[_ <: MapRedInputFormat[NullWritable, Writable]]]\n \n-    val rdd = sqlContext.sparkContext.hadoopRDD(\n+    val rdd = new HadoopRDD(sqlContext.sparkContext,"
  }],
  "prId": 11978
}]