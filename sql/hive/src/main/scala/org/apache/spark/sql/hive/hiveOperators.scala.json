[{
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Typo in here\n",
    "commit": "9d2c6ca513580749ca2fd4cd60076fe83bc166aa",
    "createdAt": "2014-04-22T07:46:30Z",
    "diffHunk": "@@ -123,40 +159,88 @@ case class HiveTableScan(\n    * @return Partitions that are involved in the query plan.\n    */\n   private[hive] def prunePartitions(partitions: Seq[HivePartition]) = {\n+    /** mutable row implementation to avoid creating row instance at\n+     *  each iteration inside the while loop.\n+     */\n+    val row = new GenericMutableRow(attributes.length)\n     boundPruningPred match {\n       case None => partitions\n       case Some(shouldKeep) => partitions.filter { part =>\n-        val dataTypes = relation.partitionKeys.map(_.dataType)\n-        val castedValues = for ((value, dataType) <- part.getValues.zip(dataTypes)) yield {\n-          castFromString(value, dataType)\n+        val castedValues = mutable.ArrayBuffer[Any]()\n+        var i = 0\n+        var len = relation.partitionKeys.length\n+        val iter: Iterator[String] = part.getValues.iterator\n+        while (i < len) {\n+          castedValues += castFromString(iter.next,relation.partitionKeys(i).dataType)\n+          i += 1\n         }\n-\n         // Only partitioned values are needed here, since the predicate has already been bound to\n         // partition key attribute references.\n-        val row = new GenericRow(castedValues.toArray)\n+        i = 0\n+        len = castedValues.length\n+        // castedValues represents columns in the row.\n+        while (i < len) {\n+          castedValues(i) match {\n+            case n: String if n.toLowerCase == \"null\" => row.setNullAt(i)\n+            case n: Boolean => row.setBoolean(i,n)\n+            case n: Byte => row.setByte(i,n)\n+            case n: Double => row.setDouble(i,n)\n+            case n: Float => row.setFloat(i,n)\n+            case n: Int => row.setInt(i,n)\n+            case n: Long => row.setLong(i,n)\n+            case n: String  => row.setString(i,n)\n+            case n: Short  => row.setShort(i,n)\n+            case other => row.update(i,other)\n+          }\n+          i += 1\n+        }\n         shouldKeep.eval(row).asInstanceOf[Boolean]\n       }\n     }\n   }\n \n   def execute() = {\n-    inputRdd.map { row =>\n-      val values = row match {\n-        case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n-          attributeFunctions.map(_(deserializedRow, partitionKeys))\n-        case deserializedRow: AnyRef =>\n-          attributeFunctions.map(_(deserializedRow, Array.empty))\n+    /**\n+     *  mutableRow is GenericMutableRow type and only created once.\n+     *  mutableRow is upadted at each iteration inside the while loop. \n+     */ \n+    val mutableRow = new GenericMutableRow(attributes.length) \n+    var i = 0\n+    \n+    var res = inputRdd.context.runJob(inputRdd,(iter: Iterator[_]) => {\n+      /** rddBuffer keeps track of all the transformed rows.\n+       *  needed later to create finalRdd \n+       */ \n+      val rddBuffer = mutable.ArrayBuffer[Row]()\n+      while (iter.hasNext) {\n+        val row = iter.next()\n+        val values = row match {\n+          case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n+            attributeFunctions.map(_(deserializedRow, partitionKeys))\n+          case deserializedRow: AnyRef =>\n+            attributeFunctions.map(_(deserializedRow, Array.empty))\n+        }\n+        i = 0\n+        val len = values.length\n+        while ( i < len ) {\n+          values(i) match {\n+            case n: String if n.toLowerCase == \"null\" => mutableRow.setNullAt(i)\n+            case varchar: org.apache.hadoop.hive.common.`type`.HiveVarchar => \n+              mutableRow.update(i,varchar.getValue)\n+            case decimal: org.apache.hadoop.hive.common.`type`.HiveDecimal =>\n+              mutableRow.update(i,BigDecimal(decimal.bigDecimalValue))\n+            case other => mutableRow.update(i,other)\n+          }\n+          i += 1\n+        }\n+        rddBuffer +=  mutableRow\n       }\n-      buildRow(values.map {\n-        case n: String if n.toLowerCase == \"null\" => null\n-        case varchar: org.apache.hadoop.hive.common.`type`.HiveVarchar => varchar.getValue\n-        case decimal: org.apache.hadoop.hive.common.`type`.HiveDecimal =>\n-          BigDecimal(decimal.bigDecimalValue)\n-        case other => other\n-      })\n-    }\n+      rddBuffer\n+    })\n+    /** finalRdd ... equivelant to Rdd generated from inputRdd.map(...) */"
  }],
  "prId": 456
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Please make sure to follow the [code style](https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide) throughout, e.g. there are some problems here:\n- Comments written in a non-Javadoc like syntax (with text on the `/**` line and weird indent)\n- Extra spaces inside parens and after +=\n- No space after comma\n",
    "commit": "9d2c6ca513580749ca2fd4cd60076fe83bc166aa",
    "createdAt": "2014-04-22T07:48:20Z",
    "diffHunk": "@@ -123,40 +159,88 @@ case class HiveTableScan(\n    * @return Partitions that are involved in the query plan.\n    */\n   private[hive] def prunePartitions(partitions: Seq[HivePartition]) = {\n+    /** mutable row implementation to avoid creating row instance at\n+     *  each iteration inside the while loop.\n+     */\n+    val row = new GenericMutableRow(attributes.length)\n     boundPruningPred match {\n       case None => partitions\n       case Some(shouldKeep) => partitions.filter { part =>\n-        val dataTypes = relation.partitionKeys.map(_.dataType)\n-        val castedValues = for ((value, dataType) <- part.getValues.zip(dataTypes)) yield {\n-          castFromString(value, dataType)\n+        val castedValues = mutable.ArrayBuffer[Any]()\n+        var i = 0\n+        var len = relation.partitionKeys.length\n+        val iter: Iterator[String] = part.getValues.iterator\n+        while (i < len) {\n+          castedValues += castFromString(iter.next,relation.partitionKeys(i).dataType)\n+          i += 1\n         }\n-\n         // Only partitioned values are needed here, since the predicate has already been bound to\n         // partition key attribute references.\n-        val row = new GenericRow(castedValues.toArray)\n+        i = 0\n+        len = castedValues.length\n+        // castedValues represents columns in the row.\n+        while (i < len) {\n+          castedValues(i) match {\n+            case n: String if n.toLowerCase == \"null\" => row.setNullAt(i)\n+            case n: Boolean => row.setBoolean(i,n)\n+            case n: Byte => row.setByte(i,n)\n+            case n: Double => row.setDouble(i,n)\n+            case n: Float => row.setFloat(i,n)\n+            case n: Int => row.setInt(i,n)\n+            case n: Long => row.setLong(i,n)\n+            case n: String  => row.setString(i,n)\n+            case n: Short  => row.setShort(i,n)\n+            case other => row.update(i,other)\n+          }\n+          i += 1\n+        }\n         shouldKeep.eval(row).asInstanceOf[Boolean]\n       }\n     }\n   }\n \n   def execute() = {\n-    inputRdd.map { row =>\n-      val values = row match {\n-        case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n-          attributeFunctions.map(_(deserializedRow, partitionKeys))\n-        case deserializedRow: AnyRef =>\n-          attributeFunctions.map(_(deserializedRow, Array.empty))\n+    /**\n+     *  mutableRow is GenericMutableRow type and only created once.\n+     *  mutableRow is upadted at each iteration inside the while loop. \n+     */ \n+    val mutableRow = new GenericMutableRow(attributes.length) \n+    var i = 0\n+    \n+    var res = inputRdd.context.runJob(inputRdd,(iter: Iterator[_]) => {\n+      /** rddBuffer keeps track of all the transformed rows.\n+       *  needed later to create finalRdd \n+       */ \n+      val rddBuffer = mutable.ArrayBuffer[Row]()\n+      while (iter.hasNext) {\n+        val row = iter.next()\n+        val values = row match {\n+          case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n+            attributeFunctions.map(_(deserializedRow, partitionKeys))\n+          case deserializedRow: AnyRef =>\n+            attributeFunctions.map(_(deserializedRow, Array.empty))\n+        }\n+        i = 0\n+        val len = values.length\n+        while ( i < len ) {\n+          values(i) match {\n+            case n: String if n.toLowerCase == \"null\" => mutableRow.setNullAt(i)\n+            case varchar: org.apache.hadoop.hive.common.`type`.HiveVarchar => \n+              mutableRow.update(i,varchar.getValue)\n+            case decimal: org.apache.hadoop.hive.common.`type`.HiveDecimal =>\n+              mutableRow.update(i,BigDecimal(decimal.bigDecimalValue))\n+            case other => mutableRow.update(i,other)\n+          }\n+          i += 1\n+        }\n+        rddBuffer +=  mutableRow"
  }, {
    "author": {
      "login": "esoroush"
    },
    "body": "Thanks Matei for the comments. I will address it today whenever I find a time. \n My major problem is the following. I appreciate the suggestions. \n\ndef execute() = {\n    /**\n     \\*  mutableRow is GenericMutableRow type and only created once.\n     \\*  mutableRow is upadted at each iteration inside the while loop. \n     */\n    val mutableRow = new GenericMutableRow(attributes.length)\n\n```\nval res = inputRdd.context.runJob(inputRdd,(iter: Iterator[_]) => {\n  /** rddBuffer keeps track of all the transformed rows.\n   *  needed later to create finalRdd \n   */\n  val rddBuffer = mutable.ArrayBuffer[Row]()\n  while (iter.hasNext) {\n    val row = iter.next()\n    val values = row match {\n      case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n        attributeFunctions.map(_(deserializedRow, partitionKeys))\n      case deserializedRow: AnyRef =>\n        attributeFunctions.map(_(deserializedRow, Array.empty))\n    }\n    var i = 0\n    val len = values.length\n    while ( i < len ) {\n      values(i) match {\n        case n: String if n.toLowerCase == \"null\" => mutableRow.setNullAt(i)\n        case varchar: org.apache.hadoop.hive.common.`type`.HiveVarchar =>\n          mutableRow.update(i,varchar.getValue)\n        case decimal: org.apache.hadoop.hive.common.`type`.HiveDecimal =>\n          mutableRow.update(i,BigDecimal(decimal.bigDecimalValue))\n        case other => mutableRow.update(i,other)\n      }\n      i += 1\n    }\n    rddBuffer += new GenericRow(mutableRow.toArray)\n  }\n  rddBuffer\n})\n/** finalRdd ... equivalent to Rdd generated from inputRdd.map(...) */\nval concatArray = Array.concat[mutable.ArrayBuffer[Row]](res)\ninputRdd.context.makeRDD(concatArray)\n```\n\n  }\n\nIn the execute() method of hiveOperators.scala, I take an iterator over all the partitions in the inputRDD and pass it to the runJob() method as argument. The return from runJob() method is an Array[ArrayBuffer[Row]]. I need to generate the output rdd from that and the only way I can think of is concatenating all the ArrayBuffer[Row]s and generating one Array[Row] and then generating a new finalRDD from that. I don't think this is the right way to do that ( by doing concatenation, we are kind of localizing the already partitioned data and then again partition it by creating a new RDD ) ... Does anyone have a suggestion? \n\nThis is the main reason that I cannot pass the regression tests ... \n",
    "commit": "9d2c6ca513580749ca2fd4cd60076fe83bc166aa",
    "createdAt": "2014-04-22T16:52:52Z",
    "diffHunk": "@@ -123,40 +159,88 @@ case class HiveTableScan(\n    * @return Partitions that are involved in the query plan.\n    */\n   private[hive] def prunePartitions(partitions: Seq[HivePartition]) = {\n+    /** mutable row implementation to avoid creating row instance at\n+     *  each iteration inside the while loop.\n+     */\n+    val row = new GenericMutableRow(attributes.length)\n     boundPruningPred match {\n       case None => partitions\n       case Some(shouldKeep) => partitions.filter { part =>\n-        val dataTypes = relation.partitionKeys.map(_.dataType)\n-        val castedValues = for ((value, dataType) <- part.getValues.zip(dataTypes)) yield {\n-          castFromString(value, dataType)\n+        val castedValues = mutable.ArrayBuffer[Any]()\n+        var i = 0\n+        var len = relation.partitionKeys.length\n+        val iter: Iterator[String] = part.getValues.iterator\n+        while (i < len) {\n+          castedValues += castFromString(iter.next,relation.partitionKeys(i).dataType)\n+          i += 1\n         }\n-\n         // Only partitioned values are needed here, since the predicate has already been bound to\n         // partition key attribute references.\n-        val row = new GenericRow(castedValues.toArray)\n+        i = 0\n+        len = castedValues.length\n+        // castedValues represents columns in the row.\n+        while (i < len) {\n+          castedValues(i) match {\n+            case n: String if n.toLowerCase == \"null\" => row.setNullAt(i)\n+            case n: Boolean => row.setBoolean(i,n)\n+            case n: Byte => row.setByte(i,n)\n+            case n: Double => row.setDouble(i,n)\n+            case n: Float => row.setFloat(i,n)\n+            case n: Int => row.setInt(i,n)\n+            case n: Long => row.setLong(i,n)\n+            case n: String  => row.setString(i,n)\n+            case n: Short  => row.setShort(i,n)\n+            case other => row.update(i,other)\n+          }\n+          i += 1\n+        }\n         shouldKeep.eval(row).asInstanceOf[Boolean]\n       }\n     }\n   }\n \n   def execute() = {\n-    inputRdd.map { row =>\n-      val values = row match {\n-        case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n-          attributeFunctions.map(_(deserializedRow, partitionKeys))\n-        case deserializedRow: AnyRef =>\n-          attributeFunctions.map(_(deserializedRow, Array.empty))\n+    /**\n+     *  mutableRow is GenericMutableRow type and only created once.\n+     *  mutableRow is upadted at each iteration inside the while loop. \n+     */ \n+    val mutableRow = new GenericMutableRow(attributes.length) \n+    var i = 0\n+    \n+    var res = inputRdd.context.runJob(inputRdd,(iter: Iterator[_]) => {\n+      /** rddBuffer keeps track of all the transformed rows.\n+       *  needed later to create finalRdd \n+       */ \n+      val rddBuffer = mutable.ArrayBuffer[Row]()\n+      while (iter.hasNext) {\n+        val row = iter.next()\n+        val values = row match {\n+          case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n+            attributeFunctions.map(_(deserializedRow, partitionKeys))\n+          case deserializedRow: AnyRef =>\n+            attributeFunctions.map(_(deserializedRow, Array.empty))\n+        }\n+        i = 0\n+        val len = values.length\n+        while ( i < len ) {\n+          values(i) match {\n+            case n: String if n.toLowerCase == \"null\" => mutableRow.setNullAt(i)\n+            case varchar: org.apache.hadoop.hive.common.`type`.HiveVarchar => \n+              mutableRow.update(i,varchar.getValue)\n+            case decimal: org.apache.hadoop.hive.common.`type`.HiveDecimal =>\n+              mutableRow.update(i,BigDecimal(decimal.bigDecimalValue))\n+            case other => mutableRow.update(i,other)\n+          }\n+          i += 1\n+        }\n+        rddBuffer +=  mutableRow"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Why not just use a mapPartitions and work on one partition at a time? You don't want to collect all the data back to the driver.\n",
    "commit": "9d2c6ca513580749ca2fd4cd60076fe83bc166aa",
    "createdAt": "2014-04-22T17:09:47Z",
    "diffHunk": "@@ -123,40 +159,88 @@ case class HiveTableScan(\n    * @return Partitions that are involved in the query plan.\n    */\n   private[hive] def prunePartitions(partitions: Seq[HivePartition]) = {\n+    /** mutable row implementation to avoid creating row instance at\n+     *  each iteration inside the while loop.\n+     */\n+    val row = new GenericMutableRow(attributes.length)\n     boundPruningPred match {\n       case None => partitions\n       case Some(shouldKeep) => partitions.filter { part =>\n-        val dataTypes = relation.partitionKeys.map(_.dataType)\n-        val castedValues = for ((value, dataType) <- part.getValues.zip(dataTypes)) yield {\n-          castFromString(value, dataType)\n+        val castedValues = mutable.ArrayBuffer[Any]()\n+        var i = 0\n+        var len = relation.partitionKeys.length\n+        val iter: Iterator[String] = part.getValues.iterator\n+        while (i < len) {\n+          castedValues += castFromString(iter.next,relation.partitionKeys(i).dataType)\n+          i += 1\n         }\n-\n         // Only partitioned values are needed here, since the predicate has already been bound to\n         // partition key attribute references.\n-        val row = new GenericRow(castedValues.toArray)\n+        i = 0\n+        len = castedValues.length\n+        // castedValues represents columns in the row.\n+        while (i < len) {\n+          castedValues(i) match {\n+            case n: String if n.toLowerCase == \"null\" => row.setNullAt(i)\n+            case n: Boolean => row.setBoolean(i,n)\n+            case n: Byte => row.setByte(i,n)\n+            case n: Double => row.setDouble(i,n)\n+            case n: Float => row.setFloat(i,n)\n+            case n: Int => row.setInt(i,n)\n+            case n: Long => row.setLong(i,n)\n+            case n: String  => row.setString(i,n)\n+            case n: Short  => row.setShort(i,n)\n+            case other => row.update(i,other)\n+          }\n+          i += 1\n+        }\n         shouldKeep.eval(row).asInstanceOf[Boolean]\n       }\n     }\n   }\n \n   def execute() = {\n-    inputRdd.map { row =>\n-      val values = row match {\n-        case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n-          attributeFunctions.map(_(deserializedRow, partitionKeys))\n-        case deserializedRow: AnyRef =>\n-          attributeFunctions.map(_(deserializedRow, Array.empty))\n+    /**\n+     *  mutableRow is GenericMutableRow type and only created once.\n+     *  mutableRow is upadted at each iteration inside the while loop. \n+     */ \n+    val mutableRow = new GenericMutableRow(attributes.length) \n+    var i = 0\n+    \n+    var res = inputRdd.context.runJob(inputRdd,(iter: Iterator[_]) => {\n+      /** rddBuffer keeps track of all the transformed rows.\n+       *  needed later to create finalRdd \n+       */ \n+      val rddBuffer = mutable.ArrayBuffer[Row]()\n+      while (iter.hasNext) {\n+        val row = iter.next()\n+        val values = row match {\n+          case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n+            attributeFunctions.map(_(deserializedRow, partitionKeys))\n+          case deserializedRow: AnyRef =>\n+            attributeFunctions.map(_(deserializedRow, Array.empty))\n+        }\n+        i = 0\n+        val len = values.length\n+        while ( i < len ) {\n+          values(i) match {\n+            case n: String if n.toLowerCase == \"null\" => mutableRow.setNullAt(i)\n+            case varchar: org.apache.hadoop.hive.common.`type`.HiveVarchar => \n+              mutableRow.update(i,varchar.getValue)\n+            case decimal: org.apache.hadoop.hive.common.`type`.HiveDecimal =>\n+              mutableRow.update(i,BigDecimal(decimal.bigDecimalValue))\n+            case other => mutableRow.update(i,other)\n+          }\n+          i += 1\n+        }\n+        rddBuffer +=  mutableRow"
  }, {
    "author": {
      "login": "esoroush"
    },
    "body": "Thanks Reynold, \nit worked ... I am running unit tests right now ... \n",
    "commit": "9d2c6ca513580749ca2fd4cd60076fe83bc166aa",
    "createdAt": "2014-04-22T17:52:47Z",
    "diffHunk": "@@ -123,40 +159,88 @@ case class HiveTableScan(\n    * @return Partitions that are involved in the query plan.\n    */\n   private[hive] def prunePartitions(partitions: Seq[HivePartition]) = {\n+    /** mutable row implementation to avoid creating row instance at\n+     *  each iteration inside the while loop.\n+     */\n+    val row = new GenericMutableRow(attributes.length)\n     boundPruningPred match {\n       case None => partitions\n       case Some(shouldKeep) => partitions.filter { part =>\n-        val dataTypes = relation.partitionKeys.map(_.dataType)\n-        val castedValues = for ((value, dataType) <- part.getValues.zip(dataTypes)) yield {\n-          castFromString(value, dataType)\n+        val castedValues = mutable.ArrayBuffer[Any]()\n+        var i = 0\n+        var len = relation.partitionKeys.length\n+        val iter: Iterator[String] = part.getValues.iterator\n+        while (i < len) {\n+          castedValues += castFromString(iter.next,relation.partitionKeys(i).dataType)\n+          i += 1\n         }\n-\n         // Only partitioned values are needed here, since the predicate has already been bound to\n         // partition key attribute references.\n-        val row = new GenericRow(castedValues.toArray)\n+        i = 0\n+        len = castedValues.length\n+        // castedValues represents columns in the row.\n+        while (i < len) {\n+          castedValues(i) match {\n+            case n: String if n.toLowerCase == \"null\" => row.setNullAt(i)\n+            case n: Boolean => row.setBoolean(i,n)\n+            case n: Byte => row.setByte(i,n)\n+            case n: Double => row.setDouble(i,n)\n+            case n: Float => row.setFloat(i,n)\n+            case n: Int => row.setInt(i,n)\n+            case n: Long => row.setLong(i,n)\n+            case n: String  => row.setString(i,n)\n+            case n: Short  => row.setShort(i,n)\n+            case other => row.update(i,other)\n+          }\n+          i += 1\n+        }\n         shouldKeep.eval(row).asInstanceOf[Boolean]\n       }\n     }\n   }\n \n   def execute() = {\n-    inputRdd.map { row =>\n-      val values = row match {\n-        case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n-          attributeFunctions.map(_(deserializedRow, partitionKeys))\n-        case deserializedRow: AnyRef =>\n-          attributeFunctions.map(_(deserializedRow, Array.empty))\n+    /**\n+     *  mutableRow is GenericMutableRow type and only created once.\n+     *  mutableRow is upadted at each iteration inside the while loop. \n+     */ \n+    val mutableRow = new GenericMutableRow(attributes.length) \n+    var i = 0\n+    \n+    var res = inputRdd.context.runJob(inputRdd,(iter: Iterator[_]) => {\n+      /** rddBuffer keeps track of all the transformed rows.\n+       *  needed later to create finalRdd \n+       */ \n+      val rddBuffer = mutable.ArrayBuffer[Row]()\n+      while (iter.hasNext) {\n+        val row = iter.next()\n+        val values = row match {\n+          case Array(deserializedRow: AnyRef, partitionKeys: Array[String]) =>\n+            attributeFunctions.map(_(deserializedRow, partitionKeys))\n+          case deserializedRow: AnyRef =>\n+            attributeFunctions.map(_(deserializedRow, Array.empty))\n+        }\n+        i = 0\n+        val len = values.length\n+        while ( i < len ) {\n+          values(i) match {\n+            case n: String if n.toLowerCase == \"null\" => mutableRow.setNullAt(i)\n+            case varchar: org.apache.hadoop.hive.common.`type`.HiveVarchar => \n+              mutableRow.update(i,varchar.getValue)\n+            case decimal: org.apache.hadoop.hive.common.`type`.HiveDecimal =>\n+              mutableRow.update(i,BigDecimal(decimal.bigDecimalValue))\n+            case other => mutableRow.update(i,other)\n+          }\n+          i += 1\n+        }\n+        rddBuffer +=  mutableRow"
  }],
  "prId": 456
}]