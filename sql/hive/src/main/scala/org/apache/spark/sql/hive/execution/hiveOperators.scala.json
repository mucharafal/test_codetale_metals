[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "actually for describe can we only split up to 3 columns?\n\n``` scala\nscala> \"a b c d e\".split(\"\\\\s+\", 3)\nres2: Array[String] = Array(a, b, c d e)\n```\n",
    "commit": "fd2534c79e8e84cbc6d66dd25bf09cc29fd473f2",
    "createdAt": "2014-06-18T19:45:48Z",
    "diffHunk": "@@ -445,7 +445,19 @@ case class NativeCommand(\n     if (sideEffectResult.size == 0) {\n       context.emptyResult\n     } else {\n-      val rows = sideEffectResult.map(r => new GenericRow(Array[Any](r)))\n+      // TODO: Need a better way to handle the result of a native command.\n+      // We may want to consider to use JsonMetaDataFormatter in Hive.\n+      val isDescribe = sql.trim.startsWith(\"describe\")\n+      val rows = if (isDescribe) {\n+        // TODO: If we upgrade Hive to 0.13, we need to check the results of\n+        // context.sessionState.isHiveServerQuery() to determine how to split the result.\n+        // This method is introduced by https://issues.apache.org/jira/browse/HIVE-4545.\n+        // Right now, we split every string by any number of consecutive spaces.\n+        sideEffectResult.map(\n+          r => r.split(\"\\\\s+\")).map(r => new GenericRow(r.asInstanceOf[Array[Any]]))"
  }],
  "prId": 1118
}, {
  "comments": [{
    "author": {
      "login": "concretevitamin"
    },
    "body": "Instead of introducing a special case here, can we put this piece of logic in a separate `DescribeCommand`? A while ago the introduction of `SetCommand` / `ExplainCommand` / `CacheCommand` serves partly to reduce special-casing in random places -- pinging @liancheng on this too.\n",
    "commit": "fd2534c79e8e84cbc6d66dd25bf09cc29fd473f2",
    "createdAt": "2014-06-18T20:19:30Z",
    "diffHunk": "@@ -445,7 +445,19 @@ case class NativeCommand(\n     if (sideEffectResult.size == 0) {\n       context.emptyResult\n     } else {\n-      val rows = sideEffectResult.map(r => new GenericRow(Array[Any](r)))\n+      // TODO: Need a better way to handle the result of a native command.\n+      // We may want to consider to use JsonMetaDataFormatter in Hive."
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "That sounds good. Let's merge this first and submit another PR for that. (Reason is this should make it into 1.0.1)\n",
    "commit": "fd2534c79e8e84cbc6d66dd25bf09cc29fd473f2",
    "createdAt": "2014-06-18T20:24:58Z",
    "diffHunk": "@@ -445,7 +445,19 @@ case class NativeCommand(\n     if (sideEffectResult.size == 0) {\n       context.emptyResult\n     } else {\n-      val rows = sideEffectResult.map(r => new GenericRow(Array[Any](r)))\n+      // TODO: Need a better way to handle the result of a native command.\n+      // We may want to consider to use JsonMetaDataFormatter in Hive."
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Yeah, it sounds good.\n",
    "commit": "fd2534c79e8e84cbc6d66dd25bf09cc29fd473f2",
    "createdAt": "2014-06-18T20:29:12Z",
    "diffHunk": "@@ -445,7 +445,19 @@ case class NativeCommand(\n     if (sideEffectResult.size == 0) {\n       context.emptyResult\n     } else {\n-      val rows = sideEffectResult.map(r => new GenericRow(Array[Any](r)))\n+      // TODO: Need a better way to handle the result of a native command.\n+      // We may want to consider to use JsonMetaDataFormatter in Hive."
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Ah, my bad, when saying \"just refer to `NativeCommand`\", I actually meant to add a `DescribeCommand` following `NativeCommand` in `hiveOperations.scala`.\n\nActually, as briefly mentioned at the end of section [PR Overview](https://github.com/apache/spark/pull/1071#issue-35616519) of PR #1071 description, we should specialize all native commands in the same way, and use `NativeCommand` as a default handler for those commands that haven't been specialized yet.\n",
    "commit": "fd2534c79e8e84cbc6d66dd25bf09cc29fd473f2",
    "createdAt": "2014-06-19T01:56:07Z",
    "diffHunk": "@@ -445,7 +445,19 @@ case class NativeCommand(\n     if (sideEffectResult.size == 0) {\n       context.emptyResult\n     } else {\n-      val rows = sideEffectResult.map(r => new GenericRow(Array[Any](r)))\n+      // TODO: Need a better way to handle the result of a native command.\n+      // We may want to consider to use JsonMetaDataFormatter in Hive."
  }],
  "prId": 1118
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "api should go after MetaStoreUtils since api is  a package\n",
    "commit": "fd2534c79e8e84cbc6d66dd25bf09cc29fd473f2",
    "createdAt": "2014-06-19T06:06:28Z",
    "diffHunk": "@@ -19,8 +19,10 @@ package org.apache.spark.sql.hive.execution\n \n import org.apache.hadoop.hive.common.`type`.{HiveDecimal, HiveVarchar}\n import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.metastore.api.FieldSchema"
  }],
  "prId": 1118
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this one also should go after the next line\n",
    "commit": "fd2534c79e8e84cbc6d66dd25bf09cc29fd473f2",
    "createdAt": "2014-06-19T06:06:41Z",
    "diffHunk": "@@ -19,8 +19,10 @@ package org.apache.spark.sql.hive.execution\n \n import org.apache.hadoop.hive.common.`type`.{HiveDecimal, HiveVarchar}\n import org.apache.hadoop.hive.conf.HiveConf\n+import org.apache.hadoop.hive.metastore.api.FieldSchema\n import org.apache.hadoop.hive.metastore.MetaStoreUtils\n import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils"
  }],
  "prId": 1118
}]