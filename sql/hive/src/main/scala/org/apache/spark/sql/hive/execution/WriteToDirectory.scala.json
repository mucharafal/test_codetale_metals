[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "+1\n\nI think my biggest feedback is that this seems like a lot of new code, given we already have the ability to write data using arbitrary SerDes to a file system.  Can we acomplish the same thing with only small changes to the parser and maybe a small logical / physical node?  Or am i missing something?\n",
    "commit": "9cc8474fbe172fc28085e06c2c4e37ee967b038b",
    "createdAt": "2015-04-03T01:16:51Z",
    "diffHunk": "@@ -0,0 +1,177 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.common.FileUtils\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde2.Serializer\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption\n+import org.apache.hadoop.hive.serde2.objectinspector.{ObjectInspectorUtils, StructObjectInspector}\n+import org.apache.hadoop.mapred.{FileOutputCommitter, FileOutputFormat, JobConf}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryNode}\n+import org.apache.spark.sql.hive.HiveShim._\n+import org.apache.spark.sql.hive.{HiveContext, HiveInspectors, HiveShim, SparkHiveWriterContainer, ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.{SerializableWritable, SparkContext, TaskContext}\n+\n+import scala.collection.JavaConversions._\n+\n+/**\n+ * :: DeveloperApi ::\n+ */\n+@DeveloperApi\n+case class WriteToDirectory(\n+   path: String,\n+   child: SparkPlan,\n+   isLocal: Boolean,\n+   desc: TableDesc) extends UnaryNode with HiveInspectors {\n+\n+  @transient val hiveContext = sqlContext.asInstanceOf[HiveContext]\n+  @transient private lazy val context = new Context(hiveContext.hiveconf)\n+  @transient lazy val outputClass = newSerializer(desc).getSerializedClass\n+\n+  def output = child.output\n+  \n+  private def newSerializer(tableDesc: TableDesc): Serializer = {\n+    val serializer = tableDesc.getDeserializerClass.newInstance().asInstanceOf[Serializer]\n+    serializer.initialize(null, tableDesc.getProperties)\n+    serializer\n+  }\n+\n+  // maybe we can make it as a common method, share with `InsertIntoHiveTable`"
  }, {
    "author": {
      "login": "scwf"
    },
    "body": "Yes in InsertIntoHiveTable we can write data using arbitrary SerDes to a file system. But we can not reuse InsertIntoHiveTable to share the same physical plan since \n1 InsertIntoHiveTable and  WriteToDirectory have different input\n2 InsertIntoHiveTable has complex logical to handle dynamic partitioning\n\nSo i think we can extract a common interface(SaveAsHiveFile) to reuse the code of writing data to file system.\n",
    "commit": "9cc8474fbe172fc28085e06c2c4e37ee967b038b",
    "createdAt": "2015-04-21T03:03:49Z",
    "diffHunk": "@@ -0,0 +1,177 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.common.FileUtils\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde2.Serializer\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption\n+import org.apache.hadoop.hive.serde2.objectinspector.{ObjectInspectorUtils, StructObjectInspector}\n+import org.apache.hadoop.mapred.{FileOutputCommitter, FileOutputFormat, JobConf}\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryNode}\n+import org.apache.spark.sql.hive.HiveShim._\n+import org.apache.spark.sql.hive.{HiveContext, HiveInspectors, HiveShim, SparkHiveWriterContainer, ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.{SerializableWritable, SparkContext, TaskContext}\n+\n+import scala.collection.JavaConversions._\n+\n+/**\n+ * :: DeveloperApi ::\n+ */\n+@DeveloperApi\n+case class WriteToDirectory(\n+   path: String,\n+   child: SparkPlan,\n+   isLocal: Boolean,\n+   desc: TableDesc) extends UnaryNode with HiveInspectors {\n+\n+  @transient val hiveContext = sqlContext.asInstanceOf[HiveContext]\n+  @transient private lazy val context = new Context(hiveContext.hiveconf)\n+  @transient lazy val outputClass = newSerializer(desc).getSerializedClass\n+\n+  def output = child.output\n+  \n+  private def newSerializer(tableDesc: TableDesc): Serializer = {\n+    val serializer = tableDesc.getDeserializerClass.newInstance().asInstanceOf[Serializer]\n+    serializer.initialize(null, tableDesc.getProperties)\n+    serializer\n+  }\n+\n+  // maybe we can make it as a common method, share with `InsertIntoHiveTable`"
  }],
  "prId": 4380
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "We should probably make this class extend RunnableCommand and get rid of the extra logical plan node.  That way we get feature like avoiding a spark job when calling `collect()` for free.\n",
    "commit": "9cc8474fbe172fc28085e06c2c4e37ee967b038b",
    "createdAt": "2015-07-22T18:54:44Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.common.FileUtils\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.Context\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.mapred.JobConf\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryNode}\n+import org.apache.spark.sql.hive.HiveShim.{ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.sql.hive._\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableJobConf\n+\n+/**\n+ * :: DeveloperApi ::\n+ */\n+@DeveloperApi\n+case class WriteToDirectory(\n+   path: String,\n+   child: SparkPlan,\n+   isLocal: Boolean,\n+   desc: TableDesc) extends UnaryNode with SaveAsHiveFile {"
  }],
  "prId": 4380
}]