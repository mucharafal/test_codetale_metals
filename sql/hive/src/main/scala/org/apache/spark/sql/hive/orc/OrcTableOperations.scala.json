[{
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Please reorder imports according to Spark coding conventions.\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T05:32:43Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._"
  }],
  "prId": 6135
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Make this `private[orc]`.\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T05:34:25Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "I think we don't really need `Seq[Attribute]` here, do we? A `StructType` should be enough.\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T05:51:20Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],"
  }],
  "prId": 6135
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Make this private.\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T05:34:33Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],\n+    @transient relation: OrcRelation,\n+    filters: Array[Filter],\n+    inputPaths: Array[String]) extends Logging {\n+  @transient val sqlContext = relation.sqlContext"
  }],
  "prId": 6135
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Make this private.\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T05:35:36Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],\n+    @transient relation: OrcRelation,\n+    filters: Array[Filter],\n+    inputPaths: Array[String]) extends Logging {\n+  @transient val sqlContext = relation.sqlContext\n+  val path = relation.paths(0)"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Taking only the first path disables HDFS style globbing.\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T05:42:32Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],\n+    @transient relation: OrcRelation,\n+    filters: Array[Filter],\n+    inputPaths: Array[String]) extends Logging {\n+  @transient val sqlContext = relation.sqlContext\n+  val path = relation.paths(0)"
  }],
  "prId": 6135
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Several issues about this method:\n1. Make it private.\n2. Don't omit `: Unit =`\n3. Wrapping and indentations are off\n\nPlease use either\n\n``` scala\n  private def addColumnIds(\n      output: Seq[Attribute], relation: OrcRelation, conf: Configuration): Unit = {\n    // ...\n  }\n```\n\nor\n\n``` scala\n  private def addColumnIds(\n      output: Seq[Attribute],\n      relation: OrcRelation,\n      conf: Configuration): Unit = {\n    // ...\n  }\n```\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T05:39:38Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],\n+    @transient relation: OrcRelation,\n+    filters: Array[Filter],\n+    inputPaths: Array[String]) extends Logging {\n+  @transient val sqlContext = relation.sqlContext\n+  val path = relation.paths(0)\n+\n+  def addColumnIds(output: Seq[Attribute],\n+                   relation: OrcRelation, conf: Configuration) {"
  }],
  "prId": 6135
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: We tend to use block syntax when writing multi-line closure:\n\n``` scala\n    val ids = output.map { a =>\n      relation.dataSchema.toAttributes.indexWhere(_.name == a.name): Integer\n    }.filter(_ >= 0)\n```\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T05:40:55Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],\n+    @transient relation: OrcRelation,\n+    filters: Array[Filter],\n+    inputPaths: Array[String]) extends Logging {\n+  @transient val sqlContext = relation.sqlContext\n+  val path = relation.paths(0)\n+\n+  def addColumnIds(output: Seq[Attribute],\n+                   relation: OrcRelation, conf: Configuration) {\n+    val ids =\n+      output.map(a =>\n+        relation.dataSchema.toAttributes.indexWhere(_.name == a.name): Integer)\n+        .filter(_ >= 0)"
  }],
  "prId": 6135
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Actually how about\n\n``` scala\n    val ids = output.map(a => relation.dataSchema.fieldIndex(a.name): Integer)\n    val (sortedIds, sortedNames) = ids.zip(attributes.map(_.name)).sorted.unzip\n    HiveShim.appendReadColumns(conf, sortedIds, sortedNames)\n```\n\nEspecially, if a column name is not found in `dataSchema`, instead of `filter(_ >= 0)`, we should report an error, which is what `StructType.fieldIndex` does.\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T05:50:08Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],\n+    @transient relation: OrcRelation,\n+    filters: Array[Filter],\n+    inputPaths: Array[String]) extends Logging {\n+  @transient val sqlContext = relation.sqlContext\n+  val path = relation.paths(0)\n+\n+  def addColumnIds(output: Seq[Attribute],\n+                   relation: OrcRelation, conf: Configuration) {\n+    val ids =\n+      output.map(a =>\n+        relation.dataSchema.toAttributes.indexWhere(_.name == a.name): Integer)\n+        .filter(_ >= 0)\n+    val names = attributes.map(_.name)\n+    val sorted = ids.zip(names).sorted\n+    HiveShim.appendReadColumns(conf, sorted.map(_._1), sorted.map(_._2))\n+  }"
  }],
  "prId": 6135
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "The following style is more preferrable:\n\n``` scala\n      OrcFilters.createFilter(filters).foreach { f =>\n        conf.set(SARG_PUSHDOWN, toKryo(f))\n        conf.setBoolean(ConfVars.HIVEOPTINDEXFILTER.name(), true)\n      }\n```\n\n`INDEX_FILTER` duplicates `ConfVars.HIVEOPTINDEXFILTER.name()`, so I removed it.\n\nWhere does the `SARG_PUSHDOWN` property get checked? Couldn't find it elsewhere in this PR. Is it checked in Hive ORC code path?\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T06:22:47Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],\n+    @transient relation: OrcRelation,\n+    filters: Array[Filter],\n+    inputPaths: Array[String]) extends Logging {\n+  @transient val sqlContext = relation.sqlContext\n+  val path = relation.paths(0)\n+\n+  def addColumnIds(output: Seq[Attribute],\n+                   relation: OrcRelation, conf: Configuration) {\n+    val ids =\n+      output.map(a =>\n+        relation.dataSchema.toAttributes.indexWhere(_.name == a.name): Integer)\n+        .filter(_ >= 0)\n+    val names = attributes.map(_.name)\n+    val sorted = ids.zip(names).sorted\n+    HiveShim.appendReadColumns(conf, sorted.map(_._1), sorted.map(_._2))\n+  }\n+\n+  def buildFilter(job: Job, filters: Array[Filter]): Unit = {\n+    if (ORC_FILTER_PUSHDOWN_ENABLED) {\n+      val conf: Configuration = job.getConfiguration\n+      val recordFilter = OrcFilters.createFilter(filters)\n+      if (recordFilter.isDefined) {\n+        conf.set(SARG_PUSHDOWN, toKryo(recordFilter.get))\n+        conf.setBoolean(INDEX_FILTER, true)\n+      }"
  }],
  "prId": 6135
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Please also wrap the first argument.\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T08:00:42Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],\n+    @transient relation: OrcRelation,\n+    filters: Array[Filter],\n+    inputPaths: Array[String]) extends Logging {\n+  @transient val sqlContext = relation.sqlContext\n+  val path = relation.paths(0)\n+\n+  def addColumnIds(output: Seq[Attribute],\n+                   relation: OrcRelation, conf: Configuration) {\n+    val ids =\n+      output.map(a =>\n+        relation.dataSchema.toAttributes.indexWhere(_.name == a.name): Integer)\n+        .filter(_ >= 0)\n+    val names = attributes.map(_.name)\n+    val sorted = ids.zip(names).sorted\n+    HiveShim.appendReadColumns(conf, sorted.map(_._1), sorted.map(_._2))\n+  }\n+\n+  def buildFilter(job: Job, filters: Array[Filter]): Unit = {\n+    if (ORC_FILTER_PUSHDOWN_ENABLED) {\n+      val conf: Configuration = job.getConfiguration\n+      val recordFilter = OrcFilters.createFilter(filters)\n+      if (recordFilter.isDefined) {\n+        conf.set(SARG_PUSHDOWN, toKryo(recordFilter.get))\n+        conf.setBoolean(INDEX_FILTER, true)\n+      }\n+    }\n+  }\n+\n+  // Transform all given raw `Writable`s into `Row`s.\n+  def fillObject(conf: Configuration,"
  }],
  "prId": 6135
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Let's move this row object into the following `.mapPartitions` call to avoid serializing it.\n",
    "commit": "4dbea6ee3feafd549938aebf7bba4181b5a097ae",
    "createdAt": "2015-05-14T17:35:16Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.util._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.ql.io.orc._\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.hadoop.mapreduce.Job\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.hive.HiveShim\n+import org.apache.spark.sql.sources.Filter\n+import org.apache.spark.{Logging, SerializableWritable}\n+import scala.collection.JavaConversions._\n+\n+case class OrcTableScan(attributes: Seq[Attribute],\n+    @transient relation: OrcRelation,\n+    filters: Array[Filter],\n+    inputPaths: Array[String]) extends Logging {\n+  @transient val sqlContext = relation.sqlContext\n+  val path = relation.paths(0)\n+\n+  def addColumnIds(output: Seq[Attribute],\n+                   relation: OrcRelation, conf: Configuration) {\n+    val ids =\n+      output.map(a =>\n+        relation.dataSchema.toAttributes.indexWhere(_.name == a.name): Integer)\n+        .filter(_ >= 0)\n+    val names = attributes.map(_.name)\n+    val sorted = ids.zip(names).sorted\n+    HiveShim.appendReadColumns(conf, sorted.map(_._1), sorted.map(_._2))\n+  }\n+\n+  def buildFilter(job: Job, filters: Array[Filter]): Unit = {\n+    if (ORC_FILTER_PUSHDOWN_ENABLED) {\n+      val conf: Configuration = job.getConfiguration\n+      val recordFilter = OrcFilters.createFilter(filters)\n+      if (recordFilter.isDefined) {\n+        conf.set(SARG_PUSHDOWN, toKryo(recordFilter.get))\n+        conf.setBoolean(INDEX_FILTER, true)\n+      }\n+    }\n+  }\n+\n+  // Transform all given raw `Writable`s into `Row`s.\n+  def fillObject(conf: Configuration,\n+      iterator: Iterator[org.apache.hadoop.io.Writable],\n+      nonPartitionKeyAttrs: Seq[(Attribute, Int)],\n+      mutableRow: MutableRow): Iterator[Row] = {\n+    val deserializer = new OrcSerde\n+    val soi = OrcFileOperator.getObjectInspector(path, Some(conf))\n+    val (fieldRefs, fieldOrdinals) = nonPartitionKeyAttrs.map {\n+      case (attr, ordinal) =>\n+        soi.getStructFieldRef(attr.name.toLowerCase) -> ordinal\n+    }.unzip\n+    val unwrappers = HadoopTypeConverter.unwrappers(fieldRefs)\n+    // Map each tuple to a row object\n+    iterator.map { value =>\n+      val raw = deserializer.deserialize(value)\n+      logDebug(\"Raw data: \" + raw)\n+      var i = 0\n+      while (i < fieldRefs.length) {\n+        val fieldValue = soi.getStructFieldData(raw, fieldRefs(i))\n+        if (fieldValue == null) {\n+          mutableRow.setNullAt(fieldOrdinals(i))\n+        } else {\n+          unwrappers(i)(fieldValue, mutableRow, fieldOrdinals(i))\n+        }\n+        i += 1\n+      }\n+      mutableRow: Row\n+    }\n+  }\n+\n+  def execute(): RDD[Row] = {\n+    val sc = sqlContext.sparkContext\n+    val job = new Job(sc.hadoopConfiguration)\n+    val conf: Configuration = job.getConfiguration\n+\n+    buildFilter(job, filters)\n+    addColumnIds(attributes, relation, conf)\n+    FileInputFormat.setInputPaths(job, inputPaths.map(new Path(_)): _*)\n+\n+    val inputClass = classOf[OrcInputFormat].asInstanceOf[\n+      Class[_ <: org.apache.hadoop.mapred.InputFormat[NullWritable, Writable]]]\n+\n+    val rdd = sc.hadoopRDD(conf.asInstanceOf[JobConf],\n+      inputClass, classOf[NullWritable], classOf[Writable]).map(_._2)\n+    val mutableRow = new SpecificMutableRow(attributes.map(_.dataType))"
  }],
  "prId": 6135
}]