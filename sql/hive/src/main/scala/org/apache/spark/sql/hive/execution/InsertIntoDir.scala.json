[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Maybe it would be nicer if the additional closure is removed. This was discussed in (http://apache-spark-developers-list.1001551.n3.nabble.com/Question-about-Scala-style-explicit-typing-within-transformation-functions-and-anonymous-val-td17173.html)\n\n``` scala\nval (cols, types) = child.output.foldLeft((\"\", \"\")) { case (r, a) =>\n  r(0) = r(0) + a.name + \",\"\n  r(1) = r(1) + a.dataType.typeName + \":\"\n  r\n}\n```\n",
    "commit": "056eac64a646c77d375bd5ae863388ab4cbeee1b",
    "createdAt": "2016-05-12T04:28:36Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import antlr.SemanticException\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.ql.io.{HiveIgnoreKeyTextOutputFormat, RCFileOutputFormat}\n+import org.apache.hadoop.hive.ql.io.orc.{OrcOutputFormat, OrcSerde}\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde.serdeConstants\n+import org.apache.hadoop.hive.serde2.`lazy`.LazySimpleSerDe\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.mapred.{FileOutputFormat, JobConf, SequenceFileOutputFormat, TextInputFormat}\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.hive._\n+import org.apache.spark.sql.hive.HiveShim.{ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.util.{SerializableJobConf, Utils}\n+\n+case class InsertIntoDir(\n+    path: String,\n+    isLocal: Boolean,\n+    fileFormat: String,\n+    rowFormat: CatalogStorageFormat,\n+    child: SparkPlan) extends SaveAsHiveFile {\n+\n+  @transient private val sessionState = sqlContext.sessionState.asInstanceOf[HiveSessionState]\n+  def output: Seq[Attribute] = Seq.empty\n+\n+  protected[sql] lazy val sideEffectResult: Seq[InternalRow] = {\n+    val hadoopConf = sessionState.newHadoopConf()\n+\n+    val properties = new Properties()\n+\n+    val Array(cols, types) = child.output.foldLeft(Array(\"\", \"\"))((r, a) => {\n+      r(0) = r(0) + a.name + \",\"\n+      r(1) = r(1) + a.dataType.typeName + \":\"\n+      r\n+    })"
  }, {
    "author": {
      "login": "Parth-Brahmbhatt"
    },
    "body": "Fixed.\n",
    "commit": "056eac64a646c77d375bd5ae863388ab4cbeee1b",
    "createdAt": "2016-05-12T17:13:35Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import antlr.SemanticException\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.ql.io.{HiveIgnoreKeyTextOutputFormat, RCFileOutputFormat}\n+import org.apache.hadoop.hive.ql.io.orc.{OrcOutputFormat, OrcSerde}\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde.serdeConstants\n+import org.apache.hadoop.hive.serde2.`lazy`.LazySimpleSerDe\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.mapred.{FileOutputFormat, JobConf, SequenceFileOutputFormat, TextInputFormat}\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.hive._\n+import org.apache.spark.sql.hive.HiveShim.{ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.util.{SerializableJobConf, Utils}\n+\n+case class InsertIntoDir(\n+    path: String,\n+    isLocal: Boolean,\n+    fileFormat: String,\n+    rowFormat: CatalogStorageFormat,\n+    child: SparkPlan) extends SaveAsHiveFile {\n+\n+  @transient private val sessionState = sqlContext.sessionState.asInstanceOf[HiveSessionState]\n+  def output: Seq[Attribute] = Seq.empty\n+\n+  protected[sql] lazy val sideEffectResult: Seq[InternalRow] = {\n+    val hadoopConf = sessionState.newHadoopConf()\n+\n+    val properties = new Properties()\n+\n+    val Array(cols, types) = child.output.foldLeft(Array(\"\", \"\"))((r, a) => {\n+      r(0) = r(0) + a.name + \",\"\n+      r(1) = r(1) + a.dataType.typeName + \":\"\n+      r\n+    })"
  }],
  "prId": 13067
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I think this command can be removed.\n",
    "commit": "056eac64a646c77d375bd5ae863388ab4cbeee1b",
    "createdAt": "2016-05-12T04:29:52Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import antlr.SemanticException\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.ql.io.{HiveIgnoreKeyTextOutputFormat, RCFileOutputFormat}\n+import org.apache.hadoop.hive.ql.io.orc.{OrcOutputFormat, OrcSerde}\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde.serdeConstants\n+import org.apache.hadoop.hive.serde2.`lazy`.LazySimpleSerDe\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.mapred.{FileOutputFormat, JobConf, SequenceFileOutputFormat, TextInputFormat}\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.hive._\n+import org.apache.spark.sql.hive.HiveShim.{ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.util.{SerializableJobConf, Utils}\n+\n+case class InsertIntoDir(\n+    path: String,\n+    isLocal: Boolean,\n+    fileFormat: String,\n+    rowFormat: CatalogStorageFormat,\n+    child: SparkPlan) extends SaveAsHiveFile {\n+\n+  @transient private val sessionState = sqlContext.sessionState.asInstanceOf[HiveSessionState]\n+  def output: Seq[Attribute] = Seq.empty\n+\n+  protected[sql] lazy val sideEffectResult: Seq[InternalRow] = {\n+    val hadoopConf = sessionState.newHadoopConf()\n+\n+    val properties = new Properties()\n+\n+    val Array(cols, types) = child.output.foldLeft(Array(\"\", \"\"))((r, a) => {\n+      r(0) = r(0) + a.name + \",\"\n+      r(1) = r(1) + a.dataType.typeName + \":\"\n+      r\n+    })\n+\n+    properties.put(\"columns\", cols.dropRight(1))\n+    properties.put(\"columns.types\", types.dropRight(1))\n+\n+    val fileFormatMap = Map(\n+      \"orc\" -> (classOf[OrcOutputFormat], classOf[OrcSerde]),\n+      \"parquet\" -> (classOf[MapredParquetOutputFormat], classOf[ParquetHiveSerDe]),\n+//      \"rcfile\" -> (classOf[RCFileOutputFormat], classOf[LazySimpleSerDe]),"
  }, {
    "author": {
      "login": "Parth-Brahmbhatt"
    },
    "body": "I just removed the comment , it should be supported. \n",
    "commit": "056eac64a646c77d375bd5ae863388ab4cbeee1b",
    "createdAt": "2016-05-12T17:14:00Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import antlr.SemanticException\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.ql.io.{HiveIgnoreKeyTextOutputFormat, RCFileOutputFormat}\n+import org.apache.hadoop.hive.ql.io.orc.{OrcOutputFormat, OrcSerde}\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde.serdeConstants\n+import org.apache.hadoop.hive.serde2.`lazy`.LazySimpleSerDe\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.mapred.{FileOutputFormat, JobConf, SequenceFileOutputFormat, TextInputFormat}\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.hive._\n+import org.apache.spark.sql.hive.HiveShim.{ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.util.{SerializableJobConf, Utils}\n+\n+case class InsertIntoDir(\n+    path: String,\n+    isLocal: Boolean,\n+    fileFormat: String,\n+    rowFormat: CatalogStorageFormat,\n+    child: SparkPlan) extends SaveAsHiveFile {\n+\n+  @transient private val sessionState = sqlContext.sessionState.asInstanceOf[HiveSessionState]\n+  def output: Seq[Attribute] = Seq.empty\n+\n+  protected[sql] lazy val sideEffectResult: Seq[InternalRow] = {\n+    val hadoopConf = sessionState.newHadoopConf()\n+\n+    val properties = new Properties()\n+\n+    val Array(cols, types) = child.output.foldLeft(Array(\"\", \"\"))((r, a) => {\n+      r(0) = r(0) + a.name + \",\"\n+      r(1) = r(1) + a.dataType.typeName + \":\"\n+      r\n+    })\n+\n+    properties.put(\"columns\", cols.dropRight(1))\n+    properties.put(\"columns.types\", types.dropRight(1))\n+\n+    val fileFormatMap = Map(\n+      \"orc\" -> (classOf[OrcOutputFormat], classOf[OrcSerde]),\n+      \"parquet\" -> (classOf[MapredParquetOutputFormat], classOf[ParquetHiveSerDe]),\n+//      \"rcfile\" -> (classOf[RCFileOutputFormat], classOf[LazySimpleSerDe]),"
  }],
  "prId": 13067
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Indentation. I think this should be double-spaced.\n",
    "commit": "056eac64a646c77d375bd5ae863388ab4cbeee1b",
    "createdAt": "2016-05-12T04:30:34Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import antlr.SemanticException\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.ql.io.{HiveIgnoreKeyTextOutputFormat, RCFileOutputFormat}\n+import org.apache.hadoop.hive.ql.io.orc.{OrcOutputFormat, OrcSerde}\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde.serdeConstants\n+import org.apache.hadoop.hive.serde2.`lazy`.LazySimpleSerDe\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.mapred.{FileOutputFormat, JobConf, SequenceFileOutputFormat, TextInputFormat}\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.hive._\n+import org.apache.spark.sql.hive.HiveShim.{ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.util.{SerializableJobConf, Utils}\n+\n+case class InsertIntoDir(\n+    path: String,\n+    isLocal: Boolean,\n+    fileFormat: String,\n+    rowFormat: CatalogStorageFormat,\n+    child: SparkPlan) extends SaveAsHiveFile {\n+\n+  @transient private val sessionState = sqlContext.sessionState.asInstanceOf[HiveSessionState]\n+  def output: Seq[Attribute] = Seq.empty\n+\n+  protected[sql] lazy val sideEffectResult: Seq[InternalRow] = {\n+    val hadoopConf = sessionState.newHadoopConf()\n+\n+    val properties = new Properties()\n+\n+    val Array(cols, types) = child.output.foldLeft(Array(\"\", \"\"))((r, a) => {\n+      r(0) = r(0) + a.name + \",\"\n+      r(1) = r(1) + a.dataType.typeName + \":\"\n+      r\n+    })\n+\n+    properties.put(\"columns\", cols.dropRight(1))\n+    properties.put(\"columns.types\", types.dropRight(1))\n+\n+    val fileFormatMap = Map(\n+      \"orc\" -> (classOf[OrcOutputFormat], classOf[OrcSerde]),\n+      \"parquet\" -> (classOf[MapredParquetOutputFormat], classOf[ParquetHiveSerDe]),\n+//      \"rcfile\" -> (classOf[RCFileOutputFormat], classOf[LazySimpleSerDe]),\n+      \"textfile\" -> (classOf[HiveIgnoreKeyTextOutputFormat[Text, Text]], classOf[LazySimpleSerDe]),\n+      \"sequencefile\" -> (classOf[SequenceFileOutputFormat[Any, Any]], classOf[LazySimpleSerDe])\n+    )\n+\n+    val (ouputFormatClass, serdeClass) = fileFormatMap.getOrElse(fileFormat.toLowerCase,\n+      throw new SemanticException(s\"Unrecognized file format in STORED AS clause: $fileFormat,\" +\n+        s\" expected one of ${fileFormatMap.keys.mkString(\",\")}\"))\n+\n+    properties.put(serdeConstants.SERIALIZATION_LIB, serdeClass.getName)\n+    import scala.collection.JavaConverters._\n+    properties.putAll(rowFormat.serdeProperties.asJava)\n+\n+    // if user specified a serde in the ROW FORMAT, use that.\n+    rowFormat.serde.map(properties.put(serdeConstants.SERIALIZATION_LIB, _))\n+\n+    val tableDesc = new TableDesc(\n+      classOf[TextInputFormat],\n+      ouputFormatClass,\n+      properties\n+    )\n+\n+    val isCompressed =\n+      sessionState.conf.getConfString(\"hive.exec.compress.output\", \"false\").toBoolean\n+\n+    val targetPath = new Path(path)\n+\n+    val fileSinkConf = new FileSinkDesc(targetPath.toString, tableDesc, isCompressed)\n+\n+    val jobConf = new JobConf(hadoopConf)\n+    jobConf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n+\n+    val jobConfSer = new SerializableJobConf(jobConf)\n+\n+    val writerContainer = new SparkHiveWriterContainer(\n+        jobConf,\n+        fileSinkConf,\n+        child.output)\n+\n+    if( !isLocal ) {\n+        FileSystem.get(jobConf).delete(targetPath, true)"
  }, {
    "author": {
      "login": "Parth-Brahmbhatt"
    },
    "body": "Fixed.\n",
    "commit": "056eac64a646c77d375bd5ae863388ab4cbeee1b",
    "createdAt": "2016-05-12T17:14:07Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import antlr.SemanticException\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.ql.io.{HiveIgnoreKeyTextOutputFormat, RCFileOutputFormat}\n+import org.apache.hadoop.hive.ql.io.orc.{OrcOutputFormat, OrcSerde}\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde.serdeConstants\n+import org.apache.hadoop.hive.serde2.`lazy`.LazySimpleSerDe\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.mapred.{FileOutputFormat, JobConf, SequenceFileOutputFormat, TextInputFormat}\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.hive._\n+import org.apache.spark.sql.hive.HiveShim.{ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.util.{SerializableJobConf, Utils}\n+\n+case class InsertIntoDir(\n+    path: String,\n+    isLocal: Boolean,\n+    fileFormat: String,\n+    rowFormat: CatalogStorageFormat,\n+    child: SparkPlan) extends SaveAsHiveFile {\n+\n+  @transient private val sessionState = sqlContext.sessionState.asInstanceOf[HiveSessionState]\n+  def output: Seq[Attribute] = Seq.empty\n+\n+  protected[sql] lazy val sideEffectResult: Seq[InternalRow] = {\n+    val hadoopConf = sessionState.newHadoopConf()\n+\n+    val properties = new Properties()\n+\n+    val Array(cols, types) = child.output.foldLeft(Array(\"\", \"\"))((r, a) => {\n+      r(0) = r(0) + a.name + \",\"\n+      r(1) = r(1) + a.dataType.typeName + \":\"\n+      r\n+    })\n+\n+    properties.put(\"columns\", cols.dropRight(1))\n+    properties.put(\"columns.types\", types.dropRight(1))\n+\n+    val fileFormatMap = Map(\n+      \"orc\" -> (classOf[OrcOutputFormat], classOf[OrcSerde]),\n+      \"parquet\" -> (classOf[MapredParquetOutputFormat], classOf[ParquetHiveSerDe]),\n+//      \"rcfile\" -> (classOf[RCFileOutputFormat], classOf[LazySimpleSerDe]),\n+      \"textfile\" -> (classOf[HiveIgnoreKeyTextOutputFormat[Text, Text]], classOf[LazySimpleSerDe]),\n+      \"sequencefile\" -> (classOf[SequenceFileOutputFormat[Any, Any]], classOf[LazySimpleSerDe])\n+    )\n+\n+    val (ouputFormatClass, serdeClass) = fileFormatMap.getOrElse(fileFormat.toLowerCase,\n+      throw new SemanticException(s\"Unrecognized file format in STORED AS clause: $fileFormat,\" +\n+        s\" expected one of ${fileFormatMap.keys.mkString(\",\")}\"))\n+\n+    properties.put(serdeConstants.SERIALIZATION_LIB, serdeClass.getName)\n+    import scala.collection.JavaConverters._\n+    properties.putAll(rowFormat.serdeProperties.asJava)\n+\n+    // if user specified a serde in the ROW FORMAT, use that.\n+    rowFormat.serde.map(properties.put(serdeConstants.SERIALIZATION_LIB, _))\n+\n+    val tableDesc = new TableDesc(\n+      classOf[TextInputFormat],\n+      ouputFormatClass,\n+      properties\n+    )\n+\n+    val isCompressed =\n+      sessionState.conf.getConfString(\"hive.exec.compress.output\", \"false\").toBoolean\n+\n+    val targetPath = new Path(path)\n+\n+    val fileSinkConf = new FileSinkDesc(targetPath.toString, tableDesc, isCompressed)\n+\n+    val jobConf = new JobConf(hadoopConf)\n+    jobConf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n+\n+    val jobConfSer = new SerializableJobConf(jobConf)\n+\n+    val writerContainer = new SparkHiveWriterContainer(\n+        jobConf,\n+        fileSinkConf,\n+        child.output)\n+\n+    if( !isLocal ) {\n+        FileSystem.get(jobConf).delete(targetPath, true)"
  }],
  "prId": 13067
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Use `HiveSerDe.sourceToSerDe`.\n",
    "commit": "056eac64a646c77d375bd5ae863388ab4cbeee1b",
    "createdAt": "2016-05-23T06:04:00Z",
    "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import antlr.SemanticException\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.ql.io.{HiveIgnoreKeyTextOutputFormat, RCFileOutputFormat}\n+import org.apache.hadoop.hive.ql.io.orc.{OrcOutputFormat, OrcSerde}\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde.serdeConstants\n+import org.apache.hadoop.hive.serde2.SerDe\n+import org.apache.hadoop.hive.serde2.`lazy`.LazySimpleSerDe\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.hive._\n+import org.apache.spark.sql.hive.HiveShim.{ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.util.{SerializableJobConf, Utils}\n+\n+case class InsertIntoDir(\n+    path: String,\n+    isLocal: Boolean,\n+    fileFormat: String,\n+    rowFormat: CatalogStorageFormat,\n+    child: SparkPlan) extends SaveAsHiveFile {\n+\n+  @transient private val sessionState = sqlContext.sessionState.asInstanceOf[HiveSessionState]\n+  def output: Seq[Attribute] = Seq.empty\n+\n+  protected[sql] lazy val sideEffectResult: Seq[InternalRow] = {\n+    val hadoopConf = sessionState.newHadoopConf()\n+\n+    val properties = new Properties()\n+\n+    val Array(cols, types) = child.output.foldLeft(Array(\"\", \"\")) { case (r, a) =>\n+      r(0) = r(0) + a.name + \",\"\n+      r(1) = r(1) + a.dataType.typeName + \":\"\n+      r\n+    }\n+\n+    properties.put(\"columns\", cols.dropRight(1))\n+    properties.put(\"columns.types\", types.dropRight(1))\n+\n+    val fileFormatMap = Map[String, (Class[_ <: OutputFormat[_, _]], Class[_ <: SerDe])]("
  }, {
    "author": {
      "login": "Parth-Brahmbhatt"
    },
    "body": "Done\n",
    "commit": "056eac64a646c77d375bd5ae863388ab4cbeee1b",
    "createdAt": "2016-05-23T23:15:27Z",
    "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.execution\n+\n+import java.io.File\n+import java.util.Properties\n+\n+import scala.language.existentials\n+\n+import antlr.SemanticException\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.hive.ql.io.{HiveIgnoreKeyTextOutputFormat, RCFileOutputFormat}\n+import org.apache.hadoop.hive.ql.io.orc.{OrcOutputFormat, OrcSerde}\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n+import org.apache.hadoop.hive.ql.plan.TableDesc\n+import org.apache.hadoop.hive.serde.serdeConstants\n+import org.apache.hadoop.hive.serde2.SerDe\n+import org.apache.hadoop.hive.serde2.`lazy`.LazySimpleSerDe\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.hive._\n+import org.apache.spark.sql.hive.HiveShim.{ShimFileSinkDesc => FileSinkDesc}\n+import org.apache.spark.util.{SerializableJobConf, Utils}\n+\n+case class InsertIntoDir(\n+    path: String,\n+    isLocal: Boolean,\n+    fileFormat: String,\n+    rowFormat: CatalogStorageFormat,\n+    child: SparkPlan) extends SaveAsHiveFile {\n+\n+  @transient private val sessionState = sqlContext.sessionState.asInstanceOf[HiveSessionState]\n+  def output: Seq[Attribute] = Seq.empty\n+\n+  protected[sql] lazy val sideEffectResult: Seq[InternalRow] = {\n+    val hadoopConf = sessionState.newHadoopConf()\n+\n+    val properties = new Properties()\n+\n+    val Array(cols, types) = child.output.foldLeft(Array(\"\", \"\")) { case (r, a) =>\n+      r(0) = r(0) + a.name + \",\"\n+      r(1) = r(1) + a.dataType.typeName + \":\"\n+      r\n+    }\n+\n+    properties.put(\"columns\", cols.dropRight(1))\n+    properties.put(\"columns.types\", types.dropRight(1))\n+\n+    val fileFormatMap = Map[String, (Class[_ <: OutputFormat[_, _]], Class[_ <: SerDe])]("
  }],
  "prId": 13067
}]