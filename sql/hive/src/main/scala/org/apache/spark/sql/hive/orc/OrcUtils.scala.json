[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "we'd better return a function to avoid per-row pattern matche. cc @HyukjinKwon who fixed similar problems many times.",
    "commit": "ed43eb7fb47a1c65875bbf97a8e108abfb115925",
    "createdAt": "2017-08-22T07:03:57Z",
    "diffHunk": "@@ -0,0 +1,288 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.io.IOException\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.io._\n+import org.apache.orc.{OrcFile, TypeDescription}\n+import org.apache.orc.mapred.{OrcList, OrcMap, OrcStruct, OrcTimestamp}\n+import org.apache.orc.storage.common.`type`.HiveDecimal\n+import org.apache.orc.storage.serde2.io.{DateWritable, HiveDecimalWritable}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.SpecificInternalRow\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+object OrcUtils {\n+  /**\n+   * Read ORC file schema. This method is used in `inferSchema`.\n+   */\n+  private[orc] def readSchema(file: Path, conf: Configuration): Option[TypeDescription] = {\n+    try {\n+      val options = OrcFile.readerOptions(conf).filesystem(FileSystem.get(conf))\n+      val reader = OrcFile.createReader(file, options)\n+      val schema = reader.getSchema\n+      if (schema.getFieldNames.isEmpty) {\n+        None\n+      } else {\n+        Some(schema)\n+      }\n+    } catch {\n+      case _: IOException => None\n+    }\n+  }\n+\n+  /**\n+   * Return ORC schema with schema field name correction and the total number of rows.\n+   */\n+  private[orc] def getSchemaAndNumberOfRows(\n+      dataSchema: StructType,\n+      filePath: String,\n+      conf: Configuration) = {\n+    val hdfsPath = new Path(filePath)\n+    val fs = hdfsPath.getFileSystem(conf)\n+    val reader = OrcFile.createReader(hdfsPath, OrcFile.readerOptions(conf).filesystem(fs))\n+    val rawSchema = reader.getSchema\n+    val orcSchema = if (!rawSchema.getFieldNames.isEmpty &&\n+        rawSchema.getFieldNames.asScala.forall(_.startsWith(\"_col\"))) {\n+      var schemaString = rawSchema.toString\n+      dataSchema.zipWithIndex.foreach { case (field: StructField, index: Int) =>\n+        schemaString = schemaString.replace(s\"_col$index:\", s\"${field.name}:\")\n+      }\n+      TypeDescription.fromString(schemaString)\n+    } else {\n+      rawSchema\n+    }\n+    (orcSchema, reader.getNumberOfRows)\n+  }\n+\n+  /**\n+   * Return a ORC schema string for ORCStruct.\n+   */\n+  private[orc] def getSchemaString(schema: StructType): String = {\n+    schema.fields.map(f => s\"${f.name}:${f.dataType.catalogString}\").mkString(\"struct<\", \",\", \">\")\n+  }\n+\n+  private[orc] def getTypeDescription(dataType: DataType) = dataType match {\n+    case st: StructType => TypeDescription.fromString(getSchemaString(st))\n+    case _ => TypeDescription.fromString(dataType.catalogString)\n+  }\n+\n+  /**\n+   * Return a Orc value object for the given Spark schema.\n+   */\n+  private[orc] def createOrcValue(dataType: DataType) =\n+    OrcStruct.createValue(getTypeDescription(dataType))\n+\n+  /**\n+   * Convert Apache ORC OrcStruct to Apache Spark InternalRow.\n+   * If internalRow is not None, fill into it. Otherwise, create a SpecificInternalRow and use it.\n+   */\n+  private[orc] def convertOrcStructToInternalRow(\n+      orcStruct: OrcStruct,\n+      schema: StructType,\n+      internalRow: Option[InternalRow] = None): InternalRow = {\n+    val mutableRow = internalRow.getOrElse(new SpecificInternalRow(schema.map(_.dataType)))\n+\n+    for (schemaIndex <- 0 until schema.length) {\n+      val writable = orcStruct.getFieldValue(schema(schemaIndex).name)\n+      if (writable == null) {\n+        mutableRow.setNullAt(schemaIndex)\n+      } else {\n+        mutableRow(schemaIndex) = getCatalystValue(writable, schema(schemaIndex).dataType)\n+      }\n+    }\n+\n+    mutableRow\n+  }\n+\n+  /**\n+   * Convert Apache Spark InternalRow to Apache ORC OrcStruct.\n+   */\n+  private[orc] def convertInternalRowToOrcStruct(\n+      row: InternalRow,\n+      schema: StructType,\n+      struct: Option[OrcStruct] = None): OrcStruct = {\n+    val orcStruct = struct.getOrElse(createOrcValue(schema).asInstanceOf[OrcStruct])\n+\n+    for (schemaIndex <- 0 until schema.length) {\n+      val fieldType = schema(schemaIndex).dataType\n+      val fieldValue = if (row.isNullAt(schemaIndex)) {\n+        null\n+      } else {\n+        getWritable(row.get(schemaIndex, fieldType), fieldType)\n+      }\n+      orcStruct.setFieldValue(schemaIndex, fieldValue)\n+    }\n+    orcStruct\n+  }\n+\n+  /**\n+   * Return WritableComparable from Spark catalyst values.\n+   */\n+  private[orc] def getWritable(value: Object, dataType: DataType): WritableComparable[_] = {\n+    if (value == null) {\n+      null\n+    } else {\n+      dataType match {\n+        case NullType => null\n+\n+        case BooleanType => new BooleanWritable(value.asInstanceOf[Boolean])\n+\n+        case ByteType => new ByteWritable(value.asInstanceOf[Byte])\n+        case ShortType => new ShortWritable(value.asInstanceOf[Short])\n+        case IntegerType => new IntWritable(value.asInstanceOf[Int])\n+        case LongType => new LongWritable(value.asInstanceOf[Long])\n+\n+        case FloatType => new FloatWritable(value.asInstanceOf[Float])\n+        case DoubleType => new DoubleWritable(value.asInstanceOf[Double])\n+\n+        case StringType => new Text(value.asInstanceOf[UTF8String].getBytes)\n+\n+        case BinaryType => new BytesWritable(value.asInstanceOf[Array[Byte]])\n+\n+        case DateType => new DateWritable(DateTimeUtils.toJavaDate(value.asInstanceOf[Int]))\n+\n+        case TimestampType =>\n+          val us = value.asInstanceOf[Long]\n+          var seconds = us / DateTimeUtils.MICROS_PER_SECOND\n+          var micros = us % DateTimeUtils.MICROS_PER_SECOND\n+          if (micros < 0) {\n+            micros += DateTimeUtils.MICROS_PER_SECOND\n+            seconds -= 1\n+          }\n+          val t = new OrcTimestamp(seconds * 1000)\n+          t.setNanos(micros.toInt * 1000)\n+          t\n+\n+        case _: DecimalType =>\n+          new HiveDecimalWritable(HiveDecimal.create(value.asInstanceOf[Decimal].toJavaBigDecimal))\n+\n+        case st: StructType =>\n+          convertInternalRowToOrcStruct(value.asInstanceOf[InternalRow], st)\n+\n+        case ArrayType(et, _) =>\n+          val data = value.asInstanceOf[ArrayData]\n+          val list = createOrcValue(dataType)\n+          for (i <- 0 until data.numElements()) {\n+            list.asInstanceOf[OrcList[WritableComparable[_]]]\n+              .add(getWritable(data.get(i, et), et))\n+          }\n+          list\n+\n+        case MapType(keyType, valueType, _) =>\n+          val data = value.asInstanceOf[MapData]\n+          val map = createOrcValue(dataType)\n+            .asInstanceOf[OrcMap[WritableComparable[_], WritableComparable[_]]]\n+          data.foreach(keyType, valueType, { case (k, v) =>\n+            map.put(\n+              getWritable(k.asInstanceOf[Object], keyType),\n+              getWritable(v.asInstanceOf[Object], valueType))\n+          })\n+          map\n+\n+        case udt: UserDefinedType[_] =>\n+          val udtRow = new SpecificInternalRow(Seq(udt.sqlType))\n+          udtRow(0) = value\n+          convertInternalRowToOrcStruct(udtRow,\n+            StructType(Seq(StructField(\"tmp\", udt.sqlType)))).getFieldValue(0)\n+\n+        case _ =>\n+          throw new UnsupportedOperationException(s\"$dataType is not supported yet.\")\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Return Spark Catalyst value from WritableComparable object.\n+   */\n+  private[orc] def getCatalystValue(value: WritableComparable[_], dataType: DataType): Any = {"
  }],
  "prId": 18953
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Hi, @cloud-fan .\r\nI updated the PR to return functions. Could you review again?",
    "commit": "ed43eb7fb47a1c65875bbf97a8e108abfb115925",
    "createdAt": "2017-08-23T05:03:29Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.orc\n+\n+import java.io.IOException\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.io._\n+import org.apache.orc.{OrcFile, TypeDescription}\n+import org.apache.orc.mapred.{OrcList, OrcMap, OrcStruct, OrcTimestamp}\n+import org.apache.orc.storage.common.`type`.HiveDecimal\n+import org.apache.orc.storage.serde2.io.{DateWritable, HiveDecimalWritable}\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.SpecificInternalRow\n+import org.apache.spark.sql.catalyst.util._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+object OrcUtils {\n+  /**\n+   * Read ORC file schema. This method is used in `inferSchema`.\n+   */\n+  private[orc] def readSchema(file: Path, conf: Configuration): Option[TypeDescription] = {\n+    try {\n+      val options = OrcFile.readerOptions(conf).filesystem(FileSystem.get(conf))\n+      val reader = OrcFile.createReader(file, options)\n+      val schema = reader.getSchema\n+      if (schema.getFieldNames.isEmpty) {\n+        None\n+      } else {\n+        Some(schema)\n+      }\n+    } catch {\n+      case _: IOException => None\n+    }\n+  }\n+\n+  /**\n+   * Return ORC schema with schema field name correction and the total number of rows.\n+   */\n+  private[orc] def getSchemaAndNumberOfRows(\n+      dataSchema: StructType,\n+      filePath: String,\n+      conf: Configuration) = {\n+    val hdfsPath = new Path(filePath)\n+    val fs = hdfsPath.getFileSystem(conf)\n+    val reader = OrcFile.createReader(hdfsPath, OrcFile.readerOptions(conf).filesystem(fs))\n+    val rawSchema = reader.getSchema\n+    val orcSchema = if (!rawSchema.getFieldNames.isEmpty &&\n+        rawSchema.getFieldNames.asScala.forall(_.startsWith(\"_col\"))) {\n+      var schemaString = rawSchema.toString\n+      dataSchema.zipWithIndex.foreach { case (field: StructField, index: Int) =>\n+        schemaString = schemaString.replace(s\"_col$index:\", s\"${field.name}:\")\n+      }\n+      TypeDescription.fromString(schemaString)\n+    } else {\n+      rawSchema\n+    }\n+    (orcSchema, reader.getNumberOfRows)\n+  }\n+\n+  /**\n+   * Return a ORC schema string for ORCStruct.\n+   */\n+  private[orc] def getSchemaString(schema: StructType): String = {\n+    schema.fields.map(f => s\"${f.name}:${f.dataType.catalogString}\").mkString(\"struct<\", \",\", \">\")\n+  }\n+\n+  private[orc] def getTypeDescription(dataType: DataType) = dataType match {\n+    case st: StructType => TypeDescription.fromString(getSchemaString(st))\n+    case _ => TypeDescription.fromString(dataType.catalogString)\n+  }\n+\n+  /**\n+   * Return a Orc value object for the given Spark schema.\n+   */\n+  private[orc] def createOrcValue(dataType: DataType) =\n+    OrcStruct.createValue(getTypeDescription(dataType))\n+\n+  /**\n+   * Convert Apache ORC OrcStruct to Apache Spark InternalRow.\n+   * If internalRow is not None, fill into it. Otherwise, create a SpecificInternalRow and use it.\n+   */\n+  private[orc] def convertOrcStructToInternalRow(\n+      orcStruct: OrcStruct,\n+      schema: StructType,\n+      valueWrappers: Option[Seq[Any => Any]] = None,\n+      internalRow: Option[InternalRow] = None): InternalRow = {\n+    val mutableRow = internalRow.getOrElse(new SpecificInternalRow(schema.map(_.dataType)))\n+    val wrappers = valueWrappers.getOrElse(schema.fields.map(_.dataType).map(getValueWrapper).toSeq)\n+    for (schemaIndex <- 0 until schema.length) {\n+      val writable = orcStruct.getFieldValue(schema(schemaIndex).name)\n+      if (writable == null) {\n+        mutableRow.setNullAt(schemaIndex)\n+      } else {\n+        mutableRow(schemaIndex) = wrappers(schemaIndex)(writable)\n+      }\n+    }\n+\n+    mutableRow\n+  }\n+\n+  private def withNullSafe(f: Any => Any): Any => Any = {\n+    input => if (input == null) null else f(input)\n+  }\n+\n+\n+  /**\n+   * Builds a catalyst-value return function ahead of time according to DataType\n+   * to avoid pattern matching and branching costs per row.\n+   */\n+  private[orc] def getValueWrapper(dataType: DataType): Any => Any = dataType match {\n+    case NullType => _ => null\n+\n+    case BooleanType => withNullSafe(o => o.asInstanceOf[BooleanWritable].get)\n+\n+    case ByteType => withNullSafe(o => o.asInstanceOf[ByteWritable].get)\n+    case ShortType => withNullSafe(o => o.asInstanceOf[ShortWritable].get)\n+    case IntegerType => withNullSafe(o => o.asInstanceOf[IntWritable].get)\n+    case LongType => withNullSafe(o => o.asInstanceOf[LongWritable].get)\n+\n+    case FloatType => withNullSafe(o => o.asInstanceOf[FloatWritable].get)\n+    case DoubleType => withNullSafe(o => o.asInstanceOf[DoubleWritable].get)\n+\n+    case StringType => withNullSafe(o => UTF8String.fromBytes(o.asInstanceOf[Text].copyBytes))\n+\n+    case BinaryType =>\n+      withNullSafe { o =>\n+        val binary = o.asInstanceOf[BytesWritable]\n+        val bytes = new Array[Byte](binary.getLength)\n+        System.arraycopy(binary.getBytes, 0, bytes, 0, binary.getLength)\n+        bytes\n+      }\n+\n+    case DateType =>\n+      withNullSafe(o => DateTimeUtils.fromJavaDate(o.asInstanceOf[DateWritable].get))\n+    case TimestampType =>\n+      withNullSafe(o => DateTimeUtils.fromJavaTimestamp(o.asInstanceOf[OrcTimestamp]))\n+\n+    case DecimalType.Fixed(precision, scale) =>\n+      withNullSafe { o =>\n+        val decimal = o.asInstanceOf[HiveDecimalWritable].getHiveDecimal()\n+        val v = Decimal(decimal.bigDecimalValue, decimal.precision(), decimal.scale())\n+        v.changePrecision(precision, scale)\n+        v\n+      }\n+\n+    case _: StructType =>\n+      withNullSafe { o =>\n+        val structValue = convertOrcStructToInternalRow(\n+          o.asInstanceOf[OrcStruct],\n+          dataType.asInstanceOf[StructType])\n+        structValue\n+      }\n+\n+    case ArrayType(elementType, _) =>\n+      withNullSafe { o =>\n+        val wrapper = getValueWrapper(elementType)\n+        val data = new scala.collection.mutable.ArrayBuffer[Any]\n+        o.asInstanceOf[OrcList[WritableComparable[_]]].asScala.foreach { x =>\n+          data += wrapper(x)\n+        }\n+        new GenericArrayData(data.toArray)\n+      }\n+\n+    case MapType(keyType, valueType, _) =>\n+      withNullSafe { o =>\n+        val keyWrapper = getValueWrapper(keyType)\n+        val valueWrapper = getValueWrapper(valueType)\n+        val map = new java.util.TreeMap[Any, Any]\n+        o.asInstanceOf[OrcMap[WritableComparable[_], WritableComparable[_]]]\n+          .entrySet().asScala.foreach { entry =>\n+          map.put(keyWrapper(entry.getKey), valueWrapper(entry.getValue))\n+        }\n+        ArrayBasedMapData(map.asScala)\n+      }\n+\n+    case udt: UserDefinedType[_] =>\n+      withNullSafe { o =>\n+        getValueWrapper(udt.sqlType)(o)\n+      }\n+\n+    case _ =>\n+      throw new UnsupportedOperationException(s\"$dataType is not supported yet.\")\n+  }\n+\n+  /**\n+   * Convert Apache Spark InternalRow to Apache ORC OrcStruct.\n+   */\n+  private[orc] def convertInternalRowToOrcStruct(\n+      row: InternalRow,\n+      schema: StructType,\n+      valueWrappers: Option[Seq[Any => Any]] = None,\n+      struct: Option[OrcStruct] = None): OrcStruct = {\n+    val wrappers =\n+      valueWrappers.getOrElse(schema.fields.map(_.dataType).map(getWritableWrapper).toSeq)\n+    val orcStruct = struct.getOrElse(createOrcValue(schema).asInstanceOf[OrcStruct])\n+\n+    for (schemaIndex <- 0 until schema.length) {\n+      val fieldType = schema(schemaIndex).dataType\n+      if (row.isNullAt(schemaIndex)) {\n+        orcStruct.setFieldValue(schemaIndex, null)\n+      } else {\n+        val field = row.get(schemaIndex, fieldType)\n+        val fieldValue = wrappers(schemaIndex)(field).asInstanceOf[WritableComparable[_]]\n+        orcStruct.setFieldValue(schemaIndex, fieldValue)\n+      }\n+    }\n+    orcStruct\n+  }\n+\n+  /**\n+   * Builds a WritableComparable-return function ahead of time according to DataType\n+   * to avoid pattern matching and branching costs per row.\n+   */\n+  private[orc] def getWritableWrapper(dataType: DataType): Any => Any = dataType match {",
    "line": 214
  }],
  "prId": 18953
}]