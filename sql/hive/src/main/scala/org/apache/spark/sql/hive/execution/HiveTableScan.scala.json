[{
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "I would keep it. It is important to set needed columns in conf. So, RCFile and ORC can know what columns should be skipped. Also, seems `hiveConf.set(serdeConstants.LIST_COLUMN_TYPES, columnTypeNames)` and `hiveConf.set(serdeConstants.LIST_COLUMNS, columnInternalNames)` will be used to push down filters.\n",
    "commit": "888968f0f0259c54a2376ab787731cd2fb22f58d",
    "createdAt": "2014-07-17T16:27:31Z",
    "diffHunk": "@@ -67,95 +61,12 @@ case class HiveTableScan(\n   }\n \n   @transient\n-  private[this] val hadoopReader = new HadoopTableReader(relation.tableDesc, context)\n-\n-  /**\n-   * The hive object inspector for this table, which can be used to extract values from the\n-   * serialized row representation.\n-   */\n-  @transient\n-  private[this] lazy val objectInspector =\n-    relation.tableDesc.getDeserializer.getObjectInspector.asInstanceOf[StructObjectInspector]\n-\n-  /**\n-   * Functions that extract the requested attributes from the hive output.  Partitioned values are\n-   * casted from string to its declared data type.\n-   */\n-  @transient\n-  protected lazy val attributeFunctions: Seq[(Any, Array[String]) => Any] = {\n-    attributes.map { a =>\n-      val ordinal = relation.partitionKeys.indexOf(a)\n-      if (ordinal >= 0) {\n-        val dataType = relation.partitionKeys(ordinal).dataType\n-        (_: Any, partitionKeys: Array[String]) => {\n-          castFromString(partitionKeys(ordinal), dataType)\n-        }\n-      } else {\n-        val ref = objectInspector.getAllStructFieldRefs\n-          .find(_.getFieldName == a.name)\n-          .getOrElse(sys.error(s\"Can't find attribute $a\"))\n-        val fieldObjectInspector = ref.getFieldObjectInspector\n-\n-        val unwrapHiveData = fieldObjectInspector match {\n-          case _: HiveVarcharObjectInspector =>\n-            (value: Any) => value.asInstanceOf[HiveVarchar].getValue\n-          case _: HiveDecimalObjectInspector =>\n-            (value: Any) => BigDecimal(value.asInstanceOf[HiveDecimal].bigDecimalValue())\n-          case _ =>\n-            identity[Any] _\n-        }\n-\n-        (row: Any, _: Array[String]) => {\n-          val data = objectInspector.getStructFieldData(row, ref)\n-          val hiveData = unwrapData(data, fieldObjectInspector)\n-          if (hiveData != null) unwrapHiveData(hiveData) else null\n-        }\n-      }\n-    }\n-  }\n+  private[this] val hadoopReader = new HadoopTableReader(attributes, relation, context)\n \n   private[this] def castFromString(value: String, dataType: DataType) = {\n     Cast(Literal(value), dataType).eval(null)\n   }\n \n-  private def addColumnMetadataToConf(hiveConf: HiveConf) {"
  }, {
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Oh, yes, I didn't realize that. I will revert it.\n",
    "commit": "888968f0f0259c54a2376ab787731cd2fb22f58d",
    "createdAt": "2014-07-18T00:40:54Z",
    "diffHunk": "@@ -67,95 +61,12 @@ case class HiveTableScan(\n   }\n \n   @transient\n-  private[this] val hadoopReader = new HadoopTableReader(relation.tableDesc, context)\n-\n-  /**\n-   * The hive object inspector for this table, which can be used to extract values from the\n-   * serialized row representation.\n-   */\n-  @transient\n-  private[this] lazy val objectInspector =\n-    relation.tableDesc.getDeserializer.getObjectInspector.asInstanceOf[StructObjectInspector]\n-\n-  /**\n-   * Functions that extract the requested attributes from the hive output.  Partitioned values are\n-   * casted from string to its declared data type.\n-   */\n-  @transient\n-  protected lazy val attributeFunctions: Seq[(Any, Array[String]) => Any] = {\n-    attributes.map { a =>\n-      val ordinal = relation.partitionKeys.indexOf(a)\n-      if (ordinal >= 0) {\n-        val dataType = relation.partitionKeys(ordinal).dataType\n-        (_: Any, partitionKeys: Array[String]) => {\n-          castFromString(partitionKeys(ordinal), dataType)\n-        }\n-      } else {\n-        val ref = objectInspector.getAllStructFieldRefs\n-          .find(_.getFieldName == a.name)\n-          .getOrElse(sys.error(s\"Can't find attribute $a\"))\n-        val fieldObjectInspector = ref.getFieldObjectInspector\n-\n-        val unwrapHiveData = fieldObjectInspector match {\n-          case _: HiveVarcharObjectInspector =>\n-            (value: Any) => value.asInstanceOf[HiveVarchar].getValue\n-          case _: HiveDecimalObjectInspector =>\n-            (value: Any) => BigDecimal(value.asInstanceOf[HiveDecimal].bigDecimalValue())\n-          case _ =>\n-            identity[Any] _\n-        }\n-\n-        (row: Any, _: Array[String]) => {\n-          val data = objectInspector.getStructFieldData(row, ref)\n-          val hiveData = unwrapData(data, fieldObjectInspector)\n-          if (hiveData != null) unwrapHiveData(hiveData) else null\n-        }\n-      }\n-    }\n-  }\n+  private[this] val hadoopReader = new HadoopTableReader(attributes, relation, context)\n \n   private[this] def castFromString(value: String, dataType: DataType) = {\n     Cast(Literal(value), dataType).eval(null)\n   }\n \n-  private def addColumnMetadataToConf(hiveConf: HiveConf) {"
  }],
  "prId": 1439
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "A `HiveTableScan` will be only created by `SQLContext#pruneFilterProject` and `pruneFilterProject` will make sure there is no duplicated attributes. Let's add a comment to explain it.\n",
    "commit": "888968f0f0259c54a2376ab787731cd2fb22f58d",
    "createdAt": "2014-07-26T21:02:16Z",
    "diffHunk": "@@ -114,6 +77,7 @@ case class HiveTableScan(\n     val columnInternalNames = neededColumnIDs.map(HiveConf.getColumnInternalName(_)).mkString(\",\")\n \n     if (attributes.size == relation.output.size) {\n+      // TODO what if duplicated attributes queried?"
  }],
  "prId": 1439
}]