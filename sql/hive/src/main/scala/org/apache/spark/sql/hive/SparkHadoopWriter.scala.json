[{
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Move it in the block calculating `newWriter`?\n",
    "commit": "e69ce883ee9d337a81d4aae3a63943937f771e84",
    "createdAt": "2014-09-17T23:17:31Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.IOException\n+import java.text.NumberFormat\n+import java.util.Date\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n+import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n+\n+/**\n+ * Internal helper class that saves an RDD using a Hive OutputFormat.\n+ * It is based on [[SparkHadoopWriter]].\n+ */\n+private[hive] class SparkHiveWriterContainer(\n+    @transient jobConf: JobConf,\n+    fileSinkConf: FileSinkDesc)\n+  extends Logging\n+  with SparkHadoopMapRedUtil\n+  with Serializable {\n+\n+  private val now = new Date()\n+  protected val conf = new SerializableWritable(jobConf)\n+\n+  private var jobID = 0\n+  private var splitID = 0\n+  private var attemptID = 0\n+  private var jID: SerializableWritable[JobID] = null\n+  private var taID: SerializableWritable[TaskAttemptID] = null\n+\n+  @transient private var writer: FileSinkOperator.RecordWriter = null\n+  @transient private lazy val committer = conf.value.getOutputCommitter\n+  @transient private lazy val jobContext = newJobContext(conf.value, jID.value)\n+  @transient private lazy val taskContext = newTaskAttemptContext(conf.value, taID.value)\n+  @transient private lazy val outputFormat =\n+    conf.value.getOutputFormat.asInstanceOf[HiveOutputFormat[AnyRef,Writable]]\n+\n+  def driverSideSetup() {\n+    setIDs(0, 0, 0)\n+    setConfParams()\n+    committer.setupJob(jobContext)\n+  }\n+\n+  def executorSideSetup(jobId: Int, splitId: Int, attemptId: Int) {\n+    setIDs(jobId, splitId, attemptId)\n+    setConfParams()\n+    committer.setupTask(taskContext)\n+  }\n+\n+  /**\n+   * Create a `HiveRecordWriter`. A relative dynamic partition path can be used to create a writer\n+   * for writing data to a dynamic partition.\n+   */\n+  def open() {\n+    writer = HiveFileFormatUtils.getHiveRecordWriter(\n+      conf.value,\n+      fileSinkConf.getTableInfo,\n+      conf.value.getOutputValueClass.asInstanceOf[Class[Writable]],\n+      fileSinkConf,\n+      FileOutputFormat.getTaskOutputPath(conf.value, getOutputName),\n+      Reporter.NULL)\n+  }\n+\n+  protected def getOutputName: String = {\n+    val numberFormat = NumberFormat.getInstance()\n+    numberFormat.setMinimumIntegerDigits(5)\n+    numberFormat.setGroupingUsed(false)\n+    val extension = Utilities.getFileExtension(conf.value, fileSinkConf.getCompressed, outputFormat)\n+    \"part-\" + numberFormat.format(splitID) + extension\n+  }\n+\n+  def getLocalFileWriter(row: Row): FileSinkOperator.RecordWriter = writer\n+\n+  def close() {\n+    // Seems the boolean value passed into close does not matter.\n+    writer.close(false)\n+  }\n+\n+  def commit() {\n+    if (committer.needsTaskCommit(taskContext)) {\n+      try {\n+        committer.commitTask(taskContext)\n+        logInfo (taID + \": Committed\")\n+      } catch {\n+        case e: IOException =>\n+          logError(\"Error committing the output of task: \" + taID.value, e)\n+          committer.abortTask(taskContext)\n+          throw e\n+      }\n+    } else {\n+      logInfo(\"No need to commit output of task: \" + taID.value)\n+    }\n+  }\n+\n+  def commitJob() {\n+    committer.commitJob(jobContext)\n+  }\n+\n+  // ********* Private Functions *********\n+\n+  private def setIDs(jobId: Int, splitId: Int, attemptId: Int) {\n+    jobID = jobId\n+    splitID = splitId\n+    attemptID = attemptId\n+\n+    jID = new SerializableWritable[JobID](SparkHadoopWriter.createJobID(now, jobId))\n+    taID = new SerializableWritable[TaskAttemptID](\n+      new TaskAttemptID(new TaskID(jID.value, true, splitID), attemptID))\n+  }\n+\n+  private def setConfParams() {\n+    conf.value.set(\"mapred.job.id\", jID.value.toString)\n+    conf.value.set(\"mapred.tip.id\", taID.value.getTaskID.toString)\n+    conf.value.set(\"mapred.task.id\", taID.value.toString)\n+    conf.value.setBoolean(\"mapred.task.is.map\", true)\n+    conf.value.setInt(\"mapred.task.partition\", splitID)\n+  }\n+}\n+\n+private[hive] object SparkHiveWriterContainer {\n+  def createPathFromString(path: String, conf: JobConf): Path = {\n+    if (path == null) {\n+      throw new IllegalArgumentException(\"Output path is null\")\n+    }\n+    val outputPath = new Path(path)\n+    val fs = outputPath.getFileSystem(conf)\n+    if (outputPath == null || fs == null) {\n+      throw new IllegalArgumentException(\"Incorrectly formatted output path\")\n+    }\n+    outputPath.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+  }\n+}\n+\n+private[spark] class SparkHiveDynamicPartitionWriterContainer(\n+    @transient jobConf: JobConf,\n+    fileSinkConf: FileSinkDesc,\n+    dynamicPartColNames: Array[String])\n+  extends SparkHiveWriterContainer(jobConf, fileSinkConf) {\n+\n+  private val defaultPartName = jobConf.get(\n+    ConfVars.DEFAULTPARTITIONNAME.varname, ConfVars.DEFAULTPARTITIONNAME.defaultVal)\n+\n+  @transient private var writers: mutable.HashMap[String, FileSinkOperator.RecordWriter] = _\n+\n+  override def open(): Unit = {\n+    writers = mutable.HashMap.empty[String, FileSinkOperator.RecordWriter]\n+  }\n+\n+  override def close(): Unit = {\n+    writers.values.foreach(_.close(false))\n+  }\n+\n+  override def getLocalFileWriter(row: Row): FileSinkOperator.RecordWriter = {\n+    val dynamicPartPath = dynamicPartColNames\n+      .zip(row.takeRight(dynamicPartColNames.length))\n+      .map { case (col, rawVal) =>\n+        val string = String.valueOf(rawVal)\n+        s\"/$col=${if (rawVal == null || string.isEmpty) defaultPartName else string}\"\n+      }\n+      .mkString\n+\n+    val path = {"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Good point.\n",
    "commit": "e69ce883ee9d337a81d4aae3a63943937f771e84",
    "createdAt": "2014-09-18T00:46:00Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.IOException\n+import java.text.NumberFormat\n+import java.util.Date\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n+import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n+\n+/**\n+ * Internal helper class that saves an RDD using a Hive OutputFormat.\n+ * It is based on [[SparkHadoopWriter]].\n+ */\n+private[hive] class SparkHiveWriterContainer(\n+    @transient jobConf: JobConf,\n+    fileSinkConf: FileSinkDesc)\n+  extends Logging\n+  with SparkHadoopMapRedUtil\n+  with Serializable {\n+\n+  private val now = new Date()\n+  protected val conf = new SerializableWritable(jobConf)\n+\n+  private var jobID = 0\n+  private var splitID = 0\n+  private var attemptID = 0\n+  private var jID: SerializableWritable[JobID] = null\n+  private var taID: SerializableWritable[TaskAttemptID] = null\n+\n+  @transient private var writer: FileSinkOperator.RecordWriter = null\n+  @transient private lazy val committer = conf.value.getOutputCommitter\n+  @transient private lazy val jobContext = newJobContext(conf.value, jID.value)\n+  @transient private lazy val taskContext = newTaskAttemptContext(conf.value, taID.value)\n+  @transient private lazy val outputFormat =\n+    conf.value.getOutputFormat.asInstanceOf[HiveOutputFormat[AnyRef,Writable]]\n+\n+  def driverSideSetup() {\n+    setIDs(0, 0, 0)\n+    setConfParams()\n+    committer.setupJob(jobContext)\n+  }\n+\n+  def executorSideSetup(jobId: Int, splitId: Int, attemptId: Int) {\n+    setIDs(jobId, splitId, attemptId)\n+    setConfParams()\n+    committer.setupTask(taskContext)\n+  }\n+\n+  /**\n+   * Create a `HiveRecordWriter`. A relative dynamic partition path can be used to create a writer\n+   * for writing data to a dynamic partition.\n+   */\n+  def open() {\n+    writer = HiveFileFormatUtils.getHiveRecordWriter(\n+      conf.value,\n+      fileSinkConf.getTableInfo,\n+      conf.value.getOutputValueClass.asInstanceOf[Class[Writable]],\n+      fileSinkConf,\n+      FileOutputFormat.getTaskOutputPath(conf.value, getOutputName),\n+      Reporter.NULL)\n+  }\n+\n+  protected def getOutputName: String = {\n+    val numberFormat = NumberFormat.getInstance()\n+    numberFormat.setMinimumIntegerDigits(5)\n+    numberFormat.setGroupingUsed(false)\n+    val extension = Utilities.getFileExtension(conf.value, fileSinkConf.getCompressed, outputFormat)\n+    \"part-\" + numberFormat.format(splitID) + extension\n+  }\n+\n+  def getLocalFileWriter(row: Row): FileSinkOperator.RecordWriter = writer\n+\n+  def close() {\n+    // Seems the boolean value passed into close does not matter.\n+    writer.close(false)\n+  }\n+\n+  def commit() {\n+    if (committer.needsTaskCommit(taskContext)) {\n+      try {\n+        committer.commitTask(taskContext)\n+        logInfo (taID + \": Committed\")\n+      } catch {\n+        case e: IOException =>\n+          logError(\"Error committing the output of task: \" + taID.value, e)\n+          committer.abortTask(taskContext)\n+          throw e\n+      }\n+    } else {\n+      logInfo(\"No need to commit output of task: \" + taID.value)\n+    }\n+  }\n+\n+  def commitJob() {\n+    committer.commitJob(jobContext)\n+  }\n+\n+  // ********* Private Functions *********\n+\n+  private def setIDs(jobId: Int, splitId: Int, attemptId: Int) {\n+    jobID = jobId\n+    splitID = splitId\n+    attemptID = attemptId\n+\n+    jID = new SerializableWritable[JobID](SparkHadoopWriter.createJobID(now, jobId))\n+    taID = new SerializableWritable[TaskAttemptID](\n+      new TaskAttemptID(new TaskID(jID.value, true, splitID), attemptID))\n+  }\n+\n+  private def setConfParams() {\n+    conf.value.set(\"mapred.job.id\", jID.value.toString)\n+    conf.value.set(\"mapred.tip.id\", taID.value.getTaskID.toString)\n+    conf.value.set(\"mapred.task.id\", taID.value.toString)\n+    conf.value.setBoolean(\"mapred.task.is.map\", true)\n+    conf.value.setInt(\"mapred.task.partition\", splitID)\n+  }\n+}\n+\n+private[hive] object SparkHiveWriterContainer {\n+  def createPathFromString(path: String, conf: JobConf): Path = {\n+    if (path == null) {\n+      throw new IllegalArgumentException(\"Output path is null\")\n+    }\n+    val outputPath = new Path(path)\n+    val fs = outputPath.getFileSystem(conf)\n+    if (outputPath == null || fs == null) {\n+      throw new IllegalArgumentException(\"Incorrectly formatted output path\")\n+    }\n+    outputPath.makeQualified(fs.getUri, fs.getWorkingDirectory)\n+  }\n+}\n+\n+private[spark] class SparkHiveDynamicPartitionWriterContainer(\n+    @transient jobConf: JobConf,\n+    fileSinkConf: FileSinkDesc,\n+    dynamicPartColNames: Array[String])\n+  extends SparkHiveWriterContainer(jobConf, fileSinkConf) {\n+\n+  private val defaultPartName = jobConf.get(\n+    ConfVars.DEFAULTPARTITIONNAME.varname, ConfVars.DEFAULTPARTITIONNAME.defaultVal)\n+\n+  @transient private var writers: mutable.HashMap[String, FileSinkOperator.RecordWriter] = _\n+\n+  override def open(): Unit = {\n+    writers = mutable.HashMap.empty[String, FileSinkOperator.RecordWriter]\n+  }\n+\n+  override def close(): Unit = {\n+    writers.values.foreach(_.close(false))\n+  }\n+\n+  override def getLocalFileWriter(row: Row): FileSinkOperator.RecordWriter = {\n+    val dynamicPartPath = dynamicPartColNames\n+      .zip(row.takeRight(dynamicPartColNames.length))\n+      .map { case (col, rawVal) =>\n+        val string = String.valueOf(rawVal)\n+        s\"/$col=${if (rawVal == null || string.isEmpty) defaultPartName else string}\"\n+      }\n+      .mkString\n+\n+    val path = {"
  }],
  "prId": 2226
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Seems `open` is not a good name at here. Maybe rename it?\n",
    "commit": "e69ce883ee9d337a81d4aae3a63943937f771e84",
    "createdAt": "2014-09-17T23:19:58Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.IOException\n+import java.text.NumberFormat\n+import java.util.Date\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n+import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n+\n+/**\n+ * Internal helper class that saves an RDD using a Hive OutputFormat.\n+ * It is based on [[SparkHadoopWriter]].\n+ */\n+private[hive] class SparkHiveWriterContainer(\n+    @transient jobConf: JobConf,\n+    fileSinkConf: FileSinkDesc)\n+  extends Logging\n+  with SparkHadoopMapRedUtil\n+  with Serializable {\n+\n+  private val now = new Date()\n+  protected val conf = new SerializableWritable(jobConf)\n+\n+  private var jobID = 0\n+  private var splitID = 0\n+  private var attemptID = 0\n+  private var jID: SerializableWritable[JobID] = null\n+  private var taID: SerializableWritable[TaskAttemptID] = null\n+\n+  @transient private var writer: FileSinkOperator.RecordWriter = null\n+  @transient private lazy val committer = conf.value.getOutputCommitter\n+  @transient private lazy val jobContext = newJobContext(conf.value, jID.value)\n+  @transient private lazy val taskContext = newTaskAttemptContext(conf.value, taID.value)\n+  @transient private lazy val outputFormat =\n+    conf.value.getOutputFormat.asInstanceOf[HiveOutputFormat[AnyRef,Writable]]\n+\n+  def driverSideSetup() {\n+    setIDs(0, 0, 0)\n+    setConfParams()\n+    committer.setupJob(jobContext)\n+  }\n+\n+  def executorSideSetup(jobId: Int, splitId: Int, attemptId: Int) {\n+    setIDs(jobId, splitId, attemptId)\n+    setConfParams()\n+    committer.setupTask(taskContext)\n+  }\n+\n+  /**\n+   * Create a `HiveRecordWriter`. A relative dynamic partition path can be used to create a writer\n+   * for writing data to a dynamic partition.\n+   */\n+  def open() {"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Maybe `init()`? Also, I forgot to update the comments.\n",
    "commit": "e69ce883ee9d337a81d4aae3a63943937f771e84",
    "createdAt": "2014-09-18T00:48:03Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.IOException\n+import java.text.NumberFormat\n+import java.util.Date\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n+import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n+\n+/**\n+ * Internal helper class that saves an RDD using a Hive OutputFormat.\n+ * It is based on [[SparkHadoopWriter]].\n+ */\n+private[hive] class SparkHiveWriterContainer(\n+    @transient jobConf: JobConf,\n+    fileSinkConf: FileSinkDesc)\n+  extends Logging\n+  with SparkHadoopMapRedUtil\n+  with Serializable {\n+\n+  private val now = new Date()\n+  protected val conf = new SerializableWritable(jobConf)\n+\n+  private var jobID = 0\n+  private var splitID = 0\n+  private var attemptID = 0\n+  private var jID: SerializableWritable[JobID] = null\n+  private var taID: SerializableWritable[TaskAttemptID] = null\n+\n+  @transient private var writer: FileSinkOperator.RecordWriter = null\n+  @transient private lazy val committer = conf.value.getOutputCommitter\n+  @transient private lazy val jobContext = newJobContext(conf.value, jID.value)\n+  @transient private lazy val taskContext = newTaskAttemptContext(conf.value, taID.value)\n+  @transient private lazy val outputFormat =\n+    conf.value.getOutputFormat.asInstanceOf[HiveOutputFormat[AnyRef,Writable]]\n+\n+  def driverSideSetup() {\n+    setIDs(0, 0, 0)\n+    setConfParams()\n+    committer.setupJob(jobContext)\n+  }\n+\n+  def executorSideSetup(jobId: Int, splitId: Int, attemptId: Int) {\n+    setIDs(jobId, splitId, attemptId)\n+    setConfParams()\n+    committer.setupTask(taskContext)\n+  }\n+\n+  /**\n+   * Create a `HiveRecordWriter`. A relative dynamic partition path can be used to create a writer\n+   * for writing data to a dynamic partition.\n+   */\n+  def open() {"
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Is it always called after `executorSideSetup`? If so, can we rename it to something like `setupWriter` (or `initWriter`) and call it at the end of `executorSideSetup` instead of call it in `writeToFile`?\n",
    "commit": "e69ce883ee9d337a81d4aae3a63943937f771e84",
    "createdAt": "2014-09-18T01:00:40Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.IOException\n+import java.text.NumberFormat\n+import java.util.Date\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n+import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n+\n+/**\n+ * Internal helper class that saves an RDD using a Hive OutputFormat.\n+ * It is based on [[SparkHadoopWriter]].\n+ */\n+private[hive] class SparkHiveWriterContainer(\n+    @transient jobConf: JobConf,\n+    fileSinkConf: FileSinkDesc)\n+  extends Logging\n+  with SparkHadoopMapRedUtil\n+  with Serializable {\n+\n+  private val now = new Date()\n+  protected val conf = new SerializableWritable(jobConf)\n+\n+  private var jobID = 0\n+  private var splitID = 0\n+  private var attemptID = 0\n+  private var jID: SerializableWritable[JobID] = null\n+  private var taID: SerializableWritable[TaskAttemptID] = null\n+\n+  @transient private var writer: FileSinkOperator.RecordWriter = null\n+  @transient private lazy val committer = conf.value.getOutputCommitter\n+  @transient private lazy val jobContext = newJobContext(conf.value, jID.value)\n+  @transient private lazy val taskContext = newTaskAttemptContext(conf.value, taID.value)\n+  @transient private lazy val outputFormat =\n+    conf.value.getOutputFormat.asInstanceOf[HiveOutputFormat[AnyRef,Writable]]\n+\n+  def driverSideSetup() {\n+    setIDs(0, 0, 0)\n+    setConfParams()\n+    committer.setupJob(jobContext)\n+  }\n+\n+  def executorSideSetup(jobId: Int, splitId: Int, attemptId: Int) {\n+    setIDs(jobId, splitId, attemptId)\n+    setConfParams()\n+    committer.setupTask(taskContext)\n+  }\n+\n+  /**\n+   * Create a `HiveRecordWriter`. A relative dynamic partition path can be used to create a writer\n+   * for writing data to a dynamic partition.\n+   */\n+  def open() {"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "Yea, I also realized this. Renamed this to `initWriters` and merged it into `executorSideSetup`. Also merged the `commit()` call into `close()`.\n",
    "commit": "e69ce883ee9d337a81d4aae3a63943937f771e84",
    "createdAt": "2014-09-18T01:14:23Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.IOException\n+import java.text.NumberFormat\n+import java.util.Date\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n+import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n+\n+/**\n+ * Internal helper class that saves an RDD using a Hive OutputFormat.\n+ * It is based on [[SparkHadoopWriter]].\n+ */\n+private[hive] class SparkHiveWriterContainer(\n+    @transient jobConf: JobConf,\n+    fileSinkConf: FileSinkDesc)\n+  extends Logging\n+  with SparkHadoopMapRedUtil\n+  with Serializable {\n+\n+  private val now = new Date()\n+  protected val conf = new SerializableWritable(jobConf)\n+\n+  private var jobID = 0\n+  private var splitID = 0\n+  private var attemptID = 0\n+  private var jID: SerializableWritable[JobID] = null\n+  private var taID: SerializableWritable[TaskAttemptID] = null\n+\n+  @transient private var writer: FileSinkOperator.RecordWriter = null\n+  @transient private lazy val committer = conf.value.getOutputCommitter\n+  @transient private lazy val jobContext = newJobContext(conf.value, jID.value)\n+  @transient private lazy val taskContext = newTaskAttemptContext(conf.value, taID.value)\n+  @transient private lazy val outputFormat =\n+    conf.value.getOutputFormat.asInstanceOf[HiveOutputFormat[AnyRef,Writable]]\n+\n+  def driverSideSetup() {\n+    setIDs(0, 0, 0)\n+    setConfParams()\n+    committer.setupJob(jobContext)\n+  }\n+\n+  def executorSideSetup(jobId: Int, splitId: Int, attemptId: Int) {\n+    setIDs(jobId, splitId, attemptId)\n+    setConfParams()\n+    committer.setupTask(taskContext)\n+  }\n+\n+  /**\n+   * Create a `HiveRecordWriter`. A relative dynamic partition path can be used to create a writer\n+   * for writing data to a dynamic partition.\n+   */\n+  def open() {"
  }],
  "prId": 2226
}, {
  "comments": [{
    "author": {
      "login": "yhuai"
    },
    "body": "Shall we rename this file?\n",
    "commit": "e69ce883ee9d337a81d4aae3a63943937f771e84",
    "createdAt": "2014-09-17T23:21:05Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.IOException\n+import java.text.NumberFormat\n+import java.util.Date\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n+import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n+\n+/**\n+ * Internal helper class that saves an RDD using a Hive OutputFormat.\n+ * It is based on [[SparkHadoopWriter]].\n+ */\n+private[hive] class SparkHiveWriterContainer("
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "How about `hiveWriterContainers.scala`?\n",
    "commit": "e69ce883ee9d337a81d4aae3a63943937f771e84",
    "createdAt": "2014-09-18T00:46:47Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.IOException\n+import java.text.NumberFormat\n+import java.util.Date\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n+import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n+\n+/**\n+ * Internal helper class that saves an RDD using a Hive OutputFormat.\n+ * It is based on [[SparkHadoopWriter]].\n+ */\n+private[hive] class SparkHiveWriterContainer("
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "Sounds good.\n\nOn Wed, Sep 17, 2014 at 8:46 PM, Cheng Lian notifications@github.com\nwrote:\n\n> In\n> sql/hive/src/main/scala/org/apache/spark/sql/hive/SparkHadoopWriter.scala:\n> \n> > +import org.apache.hadoop.fs.Path\n> > +import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n> > +import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n> > +import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n> > +import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n> > +import org.apache.hadoop.io.Writable\n> > +import org.apache.hadoop.mapred._\n> > +\n> > +import org.apache.spark.sql.Row\n> > +import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n> > +\n> > +/**\n> > - \\* Internal helper class that saves an RDD using a Hive OutputFormat.\n> > - \\* It is based on [[SparkHadoopWriter]].\n> > - */\n> >   +private[hive] class SparkHiveWriterContainer(\n> \n> How about hiveWriterContainers.scala?\n> \n> â€”\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/2226/files#r17703479.\n",
    "commit": "e69ce883ee9d337a81d4aae3a63943937f771e84",
    "createdAt": "2014-09-18T00:56:14Z",
    "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import java.io.IOException\n+import java.text.NumberFormat\n+import java.util.Date\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars\n+import org.apache.hadoop.hive.ql.exec.{FileSinkOperator, Utilities}\n+import org.apache.hadoop.hive.ql.io.{HiveFileFormatUtils, HiveOutputFormat}\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc\n+import org.apache.hadoop.io.Writable\n+import org.apache.hadoop.mapred._\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.{Logging, SerializableWritable, SparkHadoopWriter}\n+\n+/**\n+ * Internal helper class that saves an RDD using a Hive OutputFormat.\n+ * It is based on [[SparkHadoopWriter]].\n+ */\n+private[hive] class SparkHiveWriterContainer("
  }],
  "prId": 2226
}]