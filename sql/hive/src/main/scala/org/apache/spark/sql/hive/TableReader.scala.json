[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "space after =\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:14:39Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "also the indent id off\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:14:54Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "indent 2 spaces from line above\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:15:24Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Space after `:` and indent 2 spaces from case.  declare the return type and spaces after ) and before {\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:16:08Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "matches?\n\nalso remove the ;\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:16:27Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matchs = fs.globStatus(pathPattern);"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "no space after (\nspace after =>\nspace before +=\nno inner ()\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:16:59Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matchs = fs.globStatus(pathPattern);\n+              matchs.map( fileStatus =>(existPathSet+= fileStatus.getPath.toString))"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "I'd use longer variable names, what is `tpath`?\nAlso: space after , and :\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:17:38Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matchs = fs.globStatus(pathPattern);\n+              matchs.map( fileStatus =>(existPathSet+= fileStatus.getPath.toString))\n+            }\n+            // convert  /demo/data/year/month/day  to  /demo/data/**/**/**/\n+            def getPathPatternByPath(parNum:Int,tpath:Path):String = {"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "extra space before =\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:17:50Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matchs = fs.globStatus(pathPattern);\n+              matchs.map( fileStatus =>(existPathSet+= fileStatus.getPath.toString))\n+            }\n+            // convert  /demo/data/year/month/day  to  /demo/data/**/**/**/\n+            def getPathPatternByPath(parNum:Int,tpath:Path):String = {\n+              var path  = tpath"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "space before and after +=\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:18:11Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matchs = fs.globStatus(pathPattern);\n+              matchs.map( fileStatus =>(existPathSet+= fileStatus.getPath.toString))\n+            }\n+            // convert  /demo/data/year/month/day  to  /demo/data/**/**/**/\n+            def getPathPatternByPath(parNum:Int,tpath:Path):String = {\n+              var path  = tpath\n+              for (i <- (1 to parNum)) { path = path.getParent }\n+              val tails = (1 to parNum).map(_ => \"*\").mkString(\"/\",\"/\",\"/\")\n+              path.toString + tails\n+            }\n+\n+            val partPath = HiveShim.getDataLocationPath(partition)\n+            val partNum = Utilities.getPartitionDesc(partition).getPartSpec.size();\n+            var pathPatternStr = getPathPatternByPath(partNum,partPath)\n+            if(!pathPatternSet.contains(pathPatternStr)){\n+              pathPatternSet+=pathPatternStr"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "space after if\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:18:15Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matchs = fs.globStatus(pathPattern);\n+              matchs.map( fileStatus =>(existPathSet+= fileStatus.getPath.toString))\n+            }\n+            // convert  /demo/data/year/month/day  to  /demo/data/**/**/**/\n+            def getPathPatternByPath(parNum:Int,tpath:Path):String = {\n+              var path  = tpath\n+              for (i <- (1 to parNum)) { path = path.getParent }\n+              val tails = (1 to parNum).map(_ => \"*\").mkString(\"/\",\"/\",\"/\")\n+              path.toString + tails\n+            }\n+\n+            val partPath = HiveShim.getDataLocationPath(partition)\n+            val partNum = Utilities.getPartitionDesc(partition).getPartSpec.size();\n+            var pathPatternStr = getPathPatternByPath(partNum,partPath)\n+            if(!pathPatternSet.contains(pathPatternStr)){"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "and before {\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:18:24Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matchs = fs.globStatus(pathPattern);\n+              matchs.map( fileStatus =>(existPathSet+= fileStatus.getPath.toString))\n+            }\n+            // convert  /demo/data/year/month/day  to  /demo/data/**/**/**/\n+            def getPathPatternByPath(parNum:Int,tpath:Path):String = {\n+              var path  = tpath\n+              for (i <- (1 to parNum)) { path = path.getParent }\n+              val tails = (1 to parNum).map(_ => \"*\").mkString(\"/\",\"/\",\"/\")\n+              path.toString + tails\n+            }\n+\n+            val partPath = HiveShim.getDataLocationPath(partition)\n+            val partNum = Utilities.getPartitionDesc(partition).getPartSpec.size();\n+            var pathPatternStr = getPathPatternByPath(partNum,partPath)\n+            if(!pathPatternSet.contains(pathPatternStr)){"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "indent is off\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:26:08Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matchs = fs.globStatus(pathPattern);\n+              matchs.map( fileStatus =>(existPathSet+= fileStatus.getPath.toString))\n+            }\n+            // convert  /demo/data/year/month/day  to  /demo/data/**/**/**/\n+            def getPathPatternByPath(parNum:Int,tpath:Path):String = {\n+              var path  = tpath\n+              for (i <- (1 to parNum)) { path = path.getParent }\n+              val tails = (1 to parNum).map(_ => \"*\").mkString(\"/\",\"/\",\"/\")\n+              path.toString + tails\n+            }\n+\n+            val partPath = HiveShim.getDataLocationPath(partition)\n+            val partNum = Utilities.getPartitionDesc(partition).getPartSpec.size();\n+            var pathPatternStr = getPathPatternByPath(partNum,partPath)\n+            if(!pathPatternSet.contains(pathPatternStr)){\n+              pathPatternSet+=pathPatternStr\n+              updateExistPathSetByPathPattern(pathPatternStr)\n+            }\n+              existPathSet.contains(partPath.toString)"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "This is very long.  I'd break the chaining here and have another variable.\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-03-18T03:26:29Z",
    "diffHunk": "@@ -142,7 +142,39 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+\n+      var existPathSet =collection.mutable.Set[String]()\n+      var pathPatternSet = collection.mutable.Set[String]()\n+\n+     val hivePartitionRDDs = partitionToDeserializer.filter {\n+            case (partition, partDeserializer) =>\n+\n+            def updateExistPathSetByPathPattern(pathPatternStr:String ){\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matchs = fs.globStatus(pathPattern);\n+              matchs.map( fileStatus =>(existPathSet+= fileStatus.getPath.toString))\n+            }\n+            // convert  /demo/data/year/month/day  to  /demo/data/**/**/**/\n+            def getPathPatternByPath(parNum:Int,tpath:Path):String = {\n+              var path  = tpath\n+              for (i <- (1 to parNum)) { path = path.getParent }\n+              val tails = (1 to parNum).map(_ => \"*\").mkString(\"/\",\"/\",\"/\")\n+              path.toString + tails\n+            }\n+\n+            val partPath = HiveShim.getDataLocationPath(partition)\n+            val partNum = Utilities.getPartitionDesc(partition).getPartSpec.size();\n+            var pathPatternStr = getPathPatternByPath(partNum,partPath)\n+            if(!pathPatternSet.contains(pathPatternStr)){\n+              pathPatternSet+=pathPatternStr\n+              updateExistPathSetByPathPattern(pathPatternStr)\n+            }\n+              existPathSet.contains(partPath.toString)\n+\n+       }\n+      .map { case (partition, partDeserializer) =>"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "Can you move this into SQLConf?\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-04-03T00:57:52Z",
    "diffHunk": "@@ -142,7 +142,46 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+        \n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+    def verifyPartitionPath(\n+        partitionToDeserializer: Map[HivePartition, Class[_ <: Deserializer]]):\n+        Map[HivePartition, Class[_ <: Deserializer]] = {\n+      if (!sc.getConf(\"spark.sql.hive.verifyPartitionPath\", \"true\").toBoolean) {"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "Use `foreach`?\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-04-06T01:25:06Z",
    "diffHunk": "@@ -142,7 +142,46 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+        \n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+    def verifyPartitionPath(\n+        partitionToDeserializer: Map[HivePartition, Class[_ <: Deserializer]]):\n+        Map[HivePartition, Class[_ <: Deserializer]] = {\n+      if (!sc.getConf(\"spark.sql.hive.verifyPartitionPath\", \"true\").toBoolean) {\n+        partitionToDeserializer\n+      } else {\n+        var existPathSet = collection.mutable.Set[String]()\n+        var pathPatternSet = collection.mutable.Set[String]()\n+        partitionToDeserializer.filter {\n+          case (partition, partDeserializer) =>\n+            def updateExistPathSetByPathPattern(pathPatternStr: String) {\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matches = fs.globStatus(pathPattern)\n+              matches.map(fileStatus => existPathSet += fileStatus.getPath.toString)"
  }],
  "prId": 5059
}, {
  "comments": [{
    "author": {
      "login": "chenghao-intel"
    },
    "body": "to `/demo/data/*/*/*/`?\n",
    "commit": "5bfcbfdbdce31d640f9bdf6e874b7f916e89e7ce",
    "createdAt": "2015-04-06T01:32:10Z",
    "diffHunk": "@@ -142,7 +142,46 @@ class HadoopTableReader(\n       partitionToDeserializer: Map[HivePartition,\n       Class[_ <: Deserializer]],\n       filterOpt: Option[PathFilter]): RDD[Row] = {\n-    val hivePartitionRDDs = partitionToDeserializer.map { case (partition, partDeserializer) =>\n+        \n+    // SPARK-5068:get FileStatus and do the filtering locally when the path is not exists\n+    def verifyPartitionPath(\n+        partitionToDeserializer: Map[HivePartition, Class[_ <: Deserializer]]):\n+        Map[HivePartition, Class[_ <: Deserializer]] = {\n+      if (!sc.getConf(\"spark.sql.hive.verifyPartitionPath\", \"true\").toBoolean) {\n+        partitionToDeserializer\n+      } else {\n+        var existPathSet = collection.mutable.Set[String]()\n+        var pathPatternSet = collection.mutable.Set[String]()\n+        partitionToDeserializer.filter {\n+          case (partition, partDeserializer) =>\n+            def updateExistPathSetByPathPattern(pathPatternStr: String) {\n+              val pathPattern = new Path(pathPatternStr)\n+              val fs = pathPattern.getFileSystem(sc.hiveconf)\n+              val matches = fs.globStatus(pathPattern)\n+              matches.map(fileStatus => existPathSet += fileStatus.getPath.toString)\n+            }\n+            // convert  /demo/data/year/month/day  to  /demo/data/**/**/**/"
  }],
  "prId": 5059
}]