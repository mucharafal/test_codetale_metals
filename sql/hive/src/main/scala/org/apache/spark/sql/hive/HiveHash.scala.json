[{
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "What is this? Can we assume that the `sqlType` of an `UserDefinedType` is an Array?\n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-11T16:14:11Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {\n+  def this(arguments: Seq[Expression]) = this(arguments, 42)\n+\n+  override def dataType: DataType = IntegerType\n+\n+  override def prettyName: String = \"hive-hash\"\n+\n+  override protected def hasherClassName: String = classOf[HiveHash].getName\n+\n+  override protected def computeHash(value: Any, dataType: DataType, seed: Int): Int = {\n+    HiveHashFunction.hash(value, dataType, seed).toInt\n+  }\n+}\n+\n+object HiveHashFunction extends InterpretedHashFunction {\n+  override protected def hashInt(i: Int, seed: Long): Long = {\n+    HiveHasher.hashInt(i, seed.toInt)\n+  }\n+\n+  override protected def hashLong(l: Long, seed: Long): Long = {\n+    HiveHasher.hashLong(l, seed.toInt)\n+  }\n+\n+  override protected def hashUnsafeBytes(base: AnyRef, offset: Long, len: Int, seed: Long): Long = {\n+    HiveHasher.hashUnsafeBytes(base, offset, len, seed.toInt)\n+  }\n+\n+  override def hash(value: Any, dataType: DataType, seed: Long): Long = {\n+    value match {\n+      case s: UTF8String =>\n+        val bytes = s.getBytes\n+        var result: Int = 0\n+        var i = 0\n+        while (i < bytes.length) {\n+          result = (result * 31) + bytes(i).toInt\n+          i += 1\n+        }\n+        result\n+\n+\n+      case array: ArrayData =>\n+        val elementType = dataType match {\n+          case udt: UserDefinedType[_] => udt.sqlType.asInstanceOf[ArrayType].elementType"
  }, {
    "author": {
      "login": "tejasapatil"
    },
    "body": "I mimic'ed exactly what happens in case of `Murmur3Hash`. See  https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/misc.scala#L388\n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-11T17:36:20Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {\n+  def this(arguments: Seq[Expression]) = this(arguments, 42)\n+\n+  override def dataType: DataType = IntegerType\n+\n+  override def prettyName: String = \"hive-hash\"\n+\n+  override protected def hasherClassName: String = classOf[HiveHash].getName\n+\n+  override protected def computeHash(value: Any, dataType: DataType, seed: Int): Int = {\n+    HiveHashFunction.hash(value, dataType, seed).toInt\n+  }\n+}\n+\n+object HiveHashFunction extends InterpretedHashFunction {\n+  override protected def hashInt(i: Int, seed: Long): Long = {\n+    HiveHasher.hashInt(i, seed.toInt)\n+  }\n+\n+  override protected def hashLong(l: Long, seed: Long): Long = {\n+    HiveHasher.hashLong(l, seed.toInt)\n+  }\n+\n+  override protected def hashUnsafeBytes(base: AnyRef, offset: Long, len: Int, seed: Long): Long = {\n+    HiveHasher.hashUnsafeBytes(base, offset, len, seed.toInt)\n+  }\n+\n+  override def hash(value: Any, dataType: DataType, seed: Long): Long = {\n+    value match {\n+      case s: UTF8String =>\n+        val bytes = s.getBytes\n+        var result: Int = 0\n+        var i = 0\n+        while (i < bytes.length) {\n+          result = (result * 31) + bytes(i).toInt\n+          i += 1\n+        }\n+        result\n+\n+\n+      case array: ArrayData =>\n+        val elementType = dataType match {\n+          case udt: UserDefinedType[_] => udt.sqlType.asInstanceOf[ArrayType].elementType"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "@cloud-fan I think you wrote the initial version. Could you you tell us what is happening here?\n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-11T17:43:06Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {\n+  def this(arguments: Seq[Expression]) = this(arguments, 42)\n+\n+  override def dataType: DataType = IntegerType\n+\n+  override def prettyName: String = \"hive-hash\"\n+\n+  override protected def hasherClassName: String = classOf[HiveHash].getName\n+\n+  override protected def computeHash(value: Any, dataType: DataType, seed: Int): Int = {\n+    HiveHashFunction.hash(value, dataType, seed).toInt\n+  }\n+}\n+\n+object HiveHashFunction extends InterpretedHashFunction {\n+  override protected def hashInt(i: Int, seed: Long): Long = {\n+    HiveHasher.hashInt(i, seed.toInt)\n+  }\n+\n+  override protected def hashLong(l: Long, seed: Long): Long = {\n+    HiveHasher.hashLong(l, seed.toInt)\n+  }\n+\n+  override protected def hashUnsafeBytes(base: AnyRef, offset: Long, len: Int, seed: Long): Long = {\n+    HiveHasher.hashUnsafeBytes(base, offset, len, seed.toInt)\n+  }\n+\n+  override def hash(value: Any, dataType: DataType, seed: Long): Long = {\n+    value match {\n+      case s: UTF8String =>\n+        val bytes = s.getBytes\n+        var result: Int = 0\n+        var i = 0\n+        while (i < bytes.length) {\n+          result = (result * 31) + bytes(i).toInt\n+          i += 1\n+        }\n+        result\n+\n+\n+      case array: ArrayData =>\n+        val elementType = dataType match {\n+          case udt: UserDefinedType[_] => udt.sqlType.asInstanceOf[ArrayType].elementType"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "the caller of `hash` guarantees the value matches the data type. So in this branch, if the value is `ArrayData`, the data type must be `ArrayType` or UDT of `ArrayType`\n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-12T08:28:05Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {\n+  def this(arguments: Seq[Expression]) = this(arguments, 42)\n+\n+  override def dataType: DataType = IntegerType\n+\n+  override def prettyName: String = \"hive-hash\"\n+\n+  override protected def hasherClassName: String = classOf[HiveHash].getName\n+\n+  override protected def computeHash(value: Any, dataType: DataType, seed: Int): Int = {\n+    HiveHashFunction.hash(value, dataType, seed).toInt\n+  }\n+}\n+\n+object HiveHashFunction extends InterpretedHashFunction {\n+  override protected def hashInt(i: Int, seed: Long): Long = {\n+    HiveHasher.hashInt(i, seed.toInt)\n+  }\n+\n+  override protected def hashLong(l: Long, seed: Long): Long = {\n+    HiveHasher.hashLong(l, seed.toInt)\n+  }\n+\n+  override protected def hashUnsafeBytes(base: AnyRef, offset: Long, len: Int, seed: Long): Long = {\n+    HiveHasher.hashUnsafeBytes(base, offset, len, seed.toInt)\n+  }\n+\n+  override def hash(value: Any, dataType: DataType, seed: Long): Long = {\n+    value match {\n+      case s: UTF8String =>\n+        val bytes = s.getBytes\n+        var result: Int = 0\n+        var i = 0\n+        while (i < bytes.length) {\n+          result = (result * 31) + bytes(i).toInt\n+          i += 1\n+        }\n+        result\n+\n+\n+      case array: ArrayData =>\n+        val elementType = dataType match {\n+          case udt: UserDefinedType[_] => udt.sqlType.asInstanceOf[ArrayType].elementType"
  }],
  "prId": 15047
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Same question\n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-11T16:14:22Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {\n+  def this(arguments: Seq[Expression]) = this(arguments, 42)\n+\n+  override def dataType: DataType = IntegerType\n+\n+  override def prettyName: String = \"hive-hash\"\n+\n+  override protected def hasherClassName: String = classOf[HiveHash].getName\n+\n+  override protected def computeHash(value: Any, dataType: DataType, seed: Int): Int = {\n+    HiveHashFunction.hash(value, dataType, seed).toInt\n+  }\n+}\n+\n+object HiveHashFunction extends InterpretedHashFunction {\n+  override protected def hashInt(i: Int, seed: Long): Long = {\n+    HiveHasher.hashInt(i, seed.toInt)\n+  }\n+\n+  override protected def hashLong(l: Long, seed: Long): Long = {\n+    HiveHasher.hashLong(l, seed.toInt)\n+  }\n+\n+  override protected def hashUnsafeBytes(base: AnyRef, offset: Long, len: Int, seed: Long): Long = {\n+    HiveHasher.hashUnsafeBytes(base, offset, len, seed.toInt)\n+  }\n+\n+  override def hash(value: Any, dataType: DataType, seed: Long): Long = {\n+    value match {\n+      case s: UTF8String =>\n+        val bytes = s.getBytes\n+        var result: Int = 0\n+        var i = 0\n+        while (i < bytes.length) {\n+          result = (result * 31) + bytes(i).toInt\n+          i += 1\n+        }\n+        result\n+\n+\n+      case array: ArrayData =>\n+        val elementType = dataType match {\n+          case udt: UserDefinedType[_] => udt.sqlType.asInstanceOf[ArrayType].elementType\n+          case ArrayType(et, _) => et\n+        }\n+        var result: Int = 0\n+        var i = 0\n+        while (i < array.numElements()) {\n+          result = (31 * result) + hash(array.get(i, elementType), elementType, 0).toInt\n+          i += 1\n+        }\n+        result\n+\n+      case map: MapData =>\n+        val (kt, vt) = dataType match {\n+          case udt: UserDefinedType[_] =>\n+            val mapType = udt.sqlType.asInstanceOf[MapType]"
  }],
  "prId": 15047
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Same question.\n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-11T16:17:02Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {\n+  def this(arguments: Seq[Expression]) = this(arguments, 42)\n+\n+  override def dataType: DataType = IntegerType\n+\n+  override def prettyName: String = \"hive-hash\"\n+\n+  override protected def hasherClassName: String = classOf[HiveHash].getName\n+\n+  override protected def computeHash(value: Any, dataType: DataType, seed: Int): Int = {\n+    HiveHashFunction.hash(value, dataType, seed).toInt\n+  }\n+}\n+\n+object HiveHashFunction extends InterpretedHashFunction {\n+  override protected def hashInt(i: Int, seed: Long): Long = {\n+    HiveHasher.hashInt(i, seed.toInt)\n+  }\n+\n+  override protected def hashLong(l: Long, seed: Long): Long = {\n+    HiveHasher.hashLong(l, seed.toInt)\n+  }\n+\n+  override protected def hashUnsafeBytes(base: AnyRef, offset: Long, len: Int, seed: Long): Long = {\n+    HiveHasher.hashUnsafeBytes(base, offset, len, seed.toInt)\n+  }\n+\n+  override def hash(value: Any, dataType: DataType, seed: Long): Long = {\n+    value match {\n+      case s: UTF8String =>\n+        val bytes = s.getBytes\n+        var result: Int = 0\n+        var i = 0\n+        while (i < bytes.length) {\n+          result = (result * 31) + bytes(i).toInt\n+          i += 1\n+        }\n+        result\n+\n+\n+      case array: ArrayData =>\n+        val elementType = dataType match {\n+          case udt: UserDefinedType[_] => udt.sqlType.asInstanceOf[ArrayType].elementType\n+          case ArrayType(et, _) => et\n+        }\n+        var result: Int = 0\n+        var i = 0\n+        while (i < array.numElements()) {\n+          result = (31 * result) + hash(array.get(i, elementType), elementType, 0).toInt\n+          i += 1\n+        }\n+        result\n+\n+      case map: MapData =>\n+        val (kt, vt) = dataType match {\n+          case udt: UserDefinedType[_] =>\n+            val mapType = udt.sqlType.asInstanceOf[MapType]\n+            mapType.keyType -> mapType.valueType\n+          case MapType(_kt, _vt, _) => _kt -> _vt\n+        }\n+        val keys = map.keyArray()\n+        val values = map.valueArray()\n+        var result: Int = 0\n+        var i = 0\n+        while (i < map.numElements()) {\n+          result += hash(keys.get(i, kt), kt, 0).toInt ^ hash(values.get(i, vt), vt, 0).toInt\n+          i += 1\n+        }\n+        result\n+\n+      case struct: InternalRow =>\n+        val types: Array[DataType] = dataType match {\n+          case udt: UserDefinedType[_] =>\n+            udt.sqlType.asInstanceOf[StructType].map(_.dataType).toArray"
  }],
  "prId": 15047
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "Should we skip the last `length % 4` bytes?\n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-11T16:19:17Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {\n+  def this(arguments: Seq[Expression]) = this(arguments, 42)\n+\n+  override def dataType: DataType = IntegerType\n+\n+  override def prettyName: String = \"hive-hash\"\n+\n+  override protected def hasherClassName: String = classOf[HiveHash].getName\n+\n+  override protected def computeHash(value: Any, dataType: DataType, seed: Int): Int = {\n+    HiveHashFunction.hash(value, dataType, seed).toInt\n+  }\n+}\n+\n+object HiveHashFunction extends InterpretedHashFunction {\n+  override protected def hashInt(i: Int, seed: Long): Long = {\n+    HiveHasher.hashInt(i, seed.toInt)\n+  }\n+\n+  override protected def hashLong(l: Long, seed: Long): Long = {\n+    HiveHasher.hashLong(l, seed.toInt)\n+  }\n+\n+  override protected def hashUnsafeBytes(base: AnyRef, offset: Long, len: Int, seed: Long): Long = {\n+    HiveHasher.hashUnsafeBytes(base, offset, len, seed.toInt)\n+  }\n+\n+  override def hash(value: Any, dataType: DataType, seed: Long): Long = {\n+    value match {\n+      case s: UTF8String =>\n+        val bytes = s.getBytes\n+        var result: Int = 0\n+        var i = 0\n+        while (i < bytes.length) {\n+          result = (result * 31) + bytes(i).toInt\n+          i += 1\n+        }\n+        result\n+\n+\n+      case array: ArrayData =>\n+        val elementType = dataType match {\n+          case udt: UserDefinedType[_] => udt.sqlType.asInstanceOf[ArrayType].elementType\n+          case ArrayType(et, _) => et\n+        }\n+        var result: Int = 0\n+        var i = 0\n+        while (i < array.numElements()) {\n+          result = (31 * result) + hash(array.get(i, elementType), elementType, 0).toInt\n+          i += 1\n+        }\n+        result\n+\n+      case map: MapData =>\n+        val (kt, vt) = dataType match {\n+          case udt: UserDefinedType[_] =>\n+            val mapType = udt.sqlType.asInstanceOf[MapType]\n+            mapType.keyType -> mapType.valueType\n+          case MapType(_kt, _vt, _) => _kt -> _vt\n+        }\n+        val keys = map.keyArray()\n+        val values = map.valueArray()\n+        var result: Int = 0\n+        var i = 0\n+        while (i < map.numElements()) {\n+          result += hash(keys.get(i, kt), kt, 0).toInt ^ hash(values.get(i, vt), vt, 0).toInt\n+          i += 1\n+        }\n+        result\n+\n+      case struct: InternalRow =>\n+        val types: Array[DataType] = dataType match {\n+          case udt: UserDefinedType[_] =>\n+            udt.sqlType.asInstanceOf[StructType].map(_.dataType).toArray\n+          case StructType(fields) => fields.map(_.dataType)\n+        }\n+\n+        var i = 0\n+        var result = 0\n+        val len = struct.numFields\n+        while (i < len) {\n+          result = (31 * result) + hash(struct.get(i, types(i)), types(i), seed).toInt\n+          i += 1\n+        }\n+        result\n+\n+      case _ => super.hash(value, dataType, seed)\n+    }\n+  }\n+}\n+\n+object HiveHasher {\n+  def hashInt(input: Int, seed: Int): Int = input\n+\n+  def hashLong(input: Long, seed: Int): Int = ((input >>> 32) ^ input).toInt\n+\n+  def hashUnsafeBytes(base: AnyRef, offset: Long, lengthInBytes: Int, seed: Int): Int = {\n+    assert(lengthInBytes >= 0, \"lengthInBytes cannot be negative\")\n+\n+    var result: Int = 0\n+    var i: Int = lengthInBytes - lengthInBytes % 4"
  }],
  "prId": 15047
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "This will also produce a code generated hash code. Does the current implementation produce a Hive compatible hash?\n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-11T16:23:35Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {"
  }, {
    "author": {
      "login": "tejasapatil"
    },
    "body": "Good catch. As far as I see, it will produce incorrect results for string and complex data types and I will have to codegen those : https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/misc.scala#L292\n\nPS: I have not worked on codegen stuff before. How does one write it, verify and test codegen ? On the surface it looks like writing code in a string but wondering if there are some cult ways which helps with this.\n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-11T17:49:37Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "It might be the easiest to isolate element hashing for `Maps`/`Arrays`/`Rows` in the super class. Code generation is basically string concatenation with some fancy tricks (look in `CodegenContext`). Ping me if you need a hand. \n",
    "commit": "238dbb80ead789229dd4022a320addedef648757",
    "createdAt": "2016-09-11T18:06:27Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.util.{ArrayData, MapData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.Platform\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * Simulates Hive's hashing function at\n+ * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode() in Hive\n+ *\n+ * We should use this hash function for both shuffle and bucket of Hive tables, so that\n+ * we can guarantee shuffle and bucketing have same data distribution\n+ *\n+ * TODO: Support Decimal and date related types\n+ */\n+@ExpressionDescription(\n+  usage = \"_FUNC_(a1, a2, ...) - Returns a hash value of the arguments.\")\n+case class HiveHash(children: Seq[Expression], seed: Int) extends HashExpression[Int] {"
  }],
  "prId": 15047
}]